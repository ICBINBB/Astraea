{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'Optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-84e97ed85a35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/bert/run_classifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/bert/optimization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAdamWeightDecayOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m   \u001b[0;34m\"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# from allennlp.predictors.predictor import Predictor\n",
    "# import allennlp_models.coref\n",
    "# predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "pass\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "# from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
    "# import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dict = dict()\n",
    "mask_dict[\"bert-base-cased\"] = \"[MASK]\"\n",
    "mask_dict[\"bert-base-uncased\"] = \"[MASK]\"\n",
    "mask_dict[\"distilbert-base-uncased\"] = \"[MASK]\"\n",
    "mask_dict[\"distilbert-base-cased\"] = \"[MASK]\"\n",
    "mask_dict[\"albert-base-v2\"] = \"[MASK]\"\n",
    "\n",
    "mask_dict[\"roberta-base\"] = \"<mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.05\n",
    "# THRESHOLD = 0.1\n",
    "# THRESHOLD = 0.15\n",
    "# THRESHOLD = 0.2\n",
    "# THRESHOLD = 0.25\n",
    "# THRESHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0817 21:08:47.169203 4678790592 configuration_utils.py:262] loading configuration file models/bert-base-uncased/config.json\n",
      "I0817 21:08:47.171446 4678790592 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0817 21:08:47.172374 4678790592 tokenization_utils_base.py:1167] Model name 'models/bert-base-uncased/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'models/bert-base-uncased/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0817 21:08:47.173218 4678790592 tokenization_utils_base.py:1197] Didn't find file models/bert-base-uncased/added_tokens.json. We won't load it.\n",
      "I0817 21:08:47.174522 4678790592 tokenization_utils_base.py:1197] Didn't find file models/bert-base-uncased/tokenizer.json. We won't load it.\n",
      "I0817 21:08:47.175484 4678790592 tokenization_utils_base.py:1252] loading file models/bert-base-uncased/vocab.txt\n",
      "I0817 21:08:47.175898 4678790592 tokenization_utils_base.py:1252] loading file None\n",
      "I0817 21:08:47.176667 4678790592 tokenization_utils_base.py:1252] loading file models/bert-base-uncased/special_tokens_map.json\n",
      "I0817 21:08:47.177181 4678790592 tokenization_utils_base.py:1252] loading file models/bert-base-uncased/tokenizer_config.json\n",
      "I0817 21:08:47.177855 4678790592 tokenization_utils_base.py:1252] loading file None\n",
      "/usr/local/lib/python3.7/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "I0817 21:08:47.218312 4678790592 configuration_utils.py:262] loading configuration file models/bert-base-uncased/config.json\n",
      "I0817 21:08:47.219378 4678790592 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0817 21:08:47.220671 4678790592 modeling_utils.py:665] loading weights file models/bert-base-uncased/pytorch_model.bin\n",
      "I0817 21:08:49.693572 4678790592 modeling_utils.py:765] All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "I0817 21:08:49.694508 4678790592 modeling_utils.py:774] All the weights of BertForMaskedLM were initialized from the model checkpoint at models/bert-base-uncased/.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# model_type = \"bert-base-cased\"\n",
    "model_type = \"bert-base-uncased\"\n",
    "# model_type = \"distilbert-base-uncased\"\n",
    "# model_type = \"distilbert-base-cased\"\n",
    "# model_type = \"albert-base-v2\"\n",
    "# model_type = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/\" + model_type +\"/\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"models/\" + model_type +\"/\")\n",
    "unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/unique_input1_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    unique_input1_set = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    unique_input1_error_set = pickle.load(handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/occupation_pair_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    occupation_pair_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    occupation1_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    occupation2_error = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    verb_error = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/Exploration/action_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    action_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation_pair_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    occupation_pair_count = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    occupation1_count = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    occupation2_count = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    verb_count = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/Exploration/action_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "    action_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "verb_list_p1 = ['walked', 'rode a bike', 'took a train', 'drove a car', \n",
    "                'took a taxi', 'got a lift', 'rode a horse', \n",
    "                'travelled by sea', 'took a plane', 'took a helicopter', \n",
    "                'flew a plane', 'jogged', 'cycled', 'ran', \n",
    "                'rode a motorcycle', 'rode a bicycle'] \n",
    "\n",
    "action1 = ['home', 'place of work', 'hometown', 'neighborhood', \n",
    "           'house', 'domicile', 'office', 'place of residence', \n",
    "           'holiday home', 'birthday party', 'musical concert', \n",
    "           'final examination']\n",
    "\n",
    "for verb in verb_list_p1:\n",
    "    verb_action[verb] = [action1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_type == \"roberta-base\":\n",
    "#     with open(\"models/\" + model_type +\"/vocab.json\", \"r\") as vocab_file:\n",
    "#         vocab_list = json.load(vocab_file)\n",
    "# else:\n",
    "vocab_file = open(\"models/\" + model_type +\"/vocab.txt\", \"r\")\n",
    "vocab_list = vocab_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_dict(D):\n",
    "    return {k: v for k, v in sorted(D.items(), key=lambda item: item[1], reverse=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate_dict(error_dict, count_dict):\n",
    "    error_rate_dict = {}\n",
    "    for key in error_dict:\n",
    "        error_rate_dict[key] = error_dict[key]/count_dict[key]\n",
    "    return get_sorted_dict(error_rate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dict(error_dict, count_dict):\n",
    "    error_rate_dict = get_error_rate_dict(error_dict, count_dict)\n",
    "    \n",
    "    probability_dict = {}\n",
    "    error_rate_sum = sum(error_rate_dict.values())\n",
    "    for error_rate in error_rate_dict:\n",
    "        probability_dict[error_rate] = error_rate_dict[error_rate]/error_rate_sum\n",
    "    \n",
    "    return probability_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_random_choice(error_dict, count_dict, probablilities_dict = None):\n",
    "    if probablilities_dict == None:\n",
    "        probability_dict = get_probability_dict(error_dict, count_dict)\n",
    "    else:\n",
    "        probability_dict = probablilities_dict\n",
    "    \n",
    "    return list(probability_dict.keys())[np.random.choice(len(list(probability_dict.keys())), p=list(probability_dict.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(sentence):\n",
    "    out = unmasker(sentence)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_score(output, tokens=['his', 'her']):\n",
    "    score_dict = {}\n",
    "    for out in output:\n",
    "        if(out['token_str'] == 'his'):\n",
    "            score_dict['his'] = out['score'] \n",
    "        if(out['token_str'] == 'her'):\n",
    "            score_dict['her'] = out['score'] \n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_violation(output1, output2, threshold = 0.2):\n",
    "\n",
    "    his_score1 = -1\n",
    "    his_score2 = -1\n",
    "\n",
    "    her_score1 = -1\n",
    "    her_score2 = -1\n",
    "\n",
    "    for out in output1:\n",
    "        if(out['token_str'] == 'his'):\n",
    "            his_score1 = out['score'] \n",
    "        if(out['token_str'] == 'her'):\n",
    "            her_score1 = out['score'] \n",
    "\n",
    "    for out in output2:\n",
    "        if(out['token_str'] == 'his'):\n",
    "            his_score2 = out['score'] \n",
    "        if(out['token_str'] == 'her'):\n",
    "            her_score2 = out['score']\n",
    "\n",
    "    if (abs(his_score1 - his_score2) > threshold and his_score1 > 0 and his_score2 > 0):\n",
    "        return True\n",
    "    elif (abs(her_score1 - her_score2) > threshold and her_score1 > 0 and her_score2 > 0):\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'farmer': 0.052881996868582526, 'sheriff': 0.05159493332214438, 'mechanic': 0.05105752444256722, 'lawyer': 0.05040773440522491, 'chief': 0.048107425331884245, 'driver': 0.04789932705436719, 'laborer': 0.047148017989083135, 'janitor': 0.046480087734237785, 'carpenter': 0.04613832238325074, 'developer': 0.046131181333508714, 'CEO': 0.04568200930473508, 'guard': 0.04562368357289473, 'physician': 0.04548028224638418, 'mover': 0.04531261300307954, 'engineer': 0.045124293319882595, 'manager': 0.04429160522040145, 'analyst': 0.04329668279869582, 'salesperson': 0.0407977693161785, 'construction worker': 0.03961185873072171, 'supervisor': 0.03925846826353765, 'technician': 0.03919986324417691, 'cook': 0.038474320114461044}\n",
      "\n",
      "{'farmer': 0.7669172932330827, 'sheriff': 0.7482517482517482, 'mechanic': 0.7404580152671756, 'lawyer': 0.7310344827586207, 'chief': 0.6976744186046512, 'driver': 0.6946564885496184, 'laborer': 0.6837606837606838, 'janitor': 0.674074074074074, 'carpenter': 0.6691176470588235, 'developer': 0.6690140845070423, 'CEO': 0.6625, 'guard': 0.6616541353383458, 'physician': 0.6595744680851063, 'mover': 0.6571428571428571, 'engineer': 0.6544117647058824, 'manager': 0.6423357664233577, 'analyst': 0.627906976744186, 'salesperson': 0.5916666666666667, 'construction worker': 0.574468085106383, 'supervisor': 0.5693430656934306, 'technician': 0.5684931506849316, 'cook': 0.5579710144927537}\n",
      "\n",
      "{'chief': 57, 'janitor': 54, 'farmer': 54, 'sheriff': 53, 'laborer': 51, 'lawyer': 50, 'developer': 50, 'driver': 49, 'supervisor': 49, 'mechanic': 48, 'CEO': 48, 'physician': 44, 'engineer': 44, 'carpenter': 43, 'guard': 43, 'mover': 40, 'analyst': 39, 'construction worker': 38, 'cook': 38, 'technician': 37, 'manager': 37, 'salesperson': 34}\n"
     ]
    }
   ],
   "source": [
    "oc1_probability = get_probability_dict(occupation1_error, occupation1_count)\n",
    "\n",
    "print(oc1_probability)\n",
    "print()\n",
    "\n",
    "error_rate_dict = get_error_rate_dict(occupation1_error, occupation1_count)\n",
    "print(error_rate_dict)\n",
    "print()\n",
    "\n",
    "output_dict = {}\n",
    "for i in range(1000):\n",
    "    oc1 = get_weighted_random_choice(occupation1_error, occupation1_count, probablilities_dict=oc1_probability)\n",
    "    update_dict(output_dict, oc1)\n",
    "print(get_sorted_dict(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_sentences(ITERS=3000):\n",
    "    err_count = 0\n",
    "\n",
    "    oc1_probability = get_probability_dict(occupation1_error, occupation1_count)\n",
    "    oc2_probability = get_probability_dict(occupation2_error, occupation2_count)\n",
    "\n",
    "    for i in range(ITERS):\n",
    "#         oc1 = random.choice(occupations_1)\n",
    "#         oc2 = random.choice(occupations_2)\n",
    "        oc1 = get_weighted_random_choice(occupation1_error, occupation1_count, probablilities_dict=oc1_probability)\n",
    "        oc2 = get_weighted_random_choice(occupation2_error, occupation2_count, probablilities_dict=oc2_probability)\n",
    "        verb = random.choice(verb_list_p1)\n",
    "        action = random.choice(random.choice(verb_action[verb]))\n",
    "        \n",
    "        input1 = \"The \" + oc1 + \" \" + verb + \" to \" + mask_dict[model_type] + \" \" + action \n",
    "        input2 = \"The \" + oc2 + \" \" + verb + \" to \" + mask_dict[model_type] + \" \" + action \n",
    "\n",
    "        pred1 = predict_mask(input1)\n",
    "        pred2 = predict_mask(input2)\n",
    "\n",
    "\n",
    "        if(i % 30 == 0):\n",
    "            print(\"Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "            print(\"Unique inputs: \" + str(len(unique_input1_set_exploitation)))\n",
    "            print(\"Iterations: \" + str(i))\n",
    "            print(\"------------------------------\")\n",
    "        \n",
    "        inp_tup = tuple(sorted([input1, input2]))\n",
    "        if inp_tup not in unique_input1_set:\n",
    "            unique_input1_set_exploitation.add(inp_tup)\n",
    "\n",
    "            update_dict(occupation_pair_count_exploitation, (oc1, oc2))\n",
    "            update_dict(occupation1_count_exploitation, oc1)\n",
    "            update_dict(occupation2_count_exploitation, oc2)\n",
    "            update_dict(verb_count_exploitation, verb)\n",
    "            update_dict(action_count_exploitation, action)\n",
    "\n",
    "\n",
    "\n",
    "        if get_violation(pred1,pred2, threshold=THRESHOLD):\n",
    "#             if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#                 if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0]) ):\n",
    "    #         if(True):\n",
    "                    err_count += 1\n",
    "        \n",
    "                    \n",
    "                    if inp_tup not in unique_input1_error_set:\n",
    "                        unique_input1_error_set_exploitation.add(inp_tup)\n",
    "\n",
    "#                         print(input1, input2)\n",
    "#                         print()\n",
    "#                         print(get_token_score(pred1))\n",
    "#                         print(get_token_score(pred2))\n",
    "#                         print(\"---------------------------\")\n",
    "\n",
    "                        update_dict(occupation_pair_error_exploitation, (oc1, oc2))\n",
    "                        update_dict(occupation1_error_exploitation, oc1)\n",
    "                        update_dict(occupation2_error_exploitation, oc2)\n",
    "                        update_dict(verb_error_exploitation, verb)\n",
    "                        update_dict(action_error_exploitation, action)\n",
    "\n",
    "\n",
    "\n",
    "    print(err_count)\n",
    "    print(err_count/ITERS)\n",
    "    print(\"Final Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "    print(\"Final Unique inputs: \" + str(len(unique_input1_set_exploitation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRESHOLD:  0.05\n",
      "Unique errors: 0\n",
      "Unique inputs: 0\n",
      "Iterations: 0\n",
      "------------------------------\n",
      "Unique errors: 21\n",
      "Unique inputs: 30\n",
      "Iterations: 30\n",
      "------------------------------\n",
      "Unique errors: 35\n",
      "Unique inputs: 59\n",
      "Iterations: 60\n",
      "------------------------------\n",
      "Unique errors: 53\n",
      "Unique inputs: 89\n",
      "Iterations: 90\n",
      "------------------------------\n",
      "Unique errors: 72\n",
      "Unique inputs: 119\n",
      "Iterations: 120\n",
      "------------------------------\n",
      "Unique errors: 88\n",
      "Unique inputs: 146\n",
      "Iterations: 150\n",
      "------------------------------\n",
      "Unique errors: 105\n",
      "Unique inputs: 175\n",
      "Iterations: 180\n",
      "------------------------------\n",
      "Unique errors: 117\n",
      "Unique inputs: 204\n",
      "Iterations: 210\n",
      "------------------------------\n",
      "Unique errors: 135\n",
      "Unique inputs: 233\n",
      "Iterations: 240\n",
      "------------------------------\n",
      "Unique errors: 158\n",
      "Unique inputs: 263\n",
      "Iterations: 270\n",
      "------------------------------\n",
      "Unique errors: 176\n",
      "Unique inputs: 293\n",
      "Iterations: 300\n",
      "------------------------------\n",
      "Unique errors: 197\n",
      "Unique inputs: 323\n",
      "Iterations: 330\n",
      "------------------------------\n",
      "Unique errors: 215\n",
      "Unique inputs: 353\n",
      "Iterations: 360\n",
      "------------------------------\n",
      "Unique errors: 232\n",
      "Unique inputs: 383\n",
      "Iterations: 390\n",
      "------------------------------\n",
      "Unique errors: 252\n",
      "Unique inputs: 412\n",
      "Iterations: 420\n",
      "------------------------------\n",
      "Unique errors: 269\n",
      "Unique inputs: 440\n",
      "Iterations: 450\n",
      "------------------------------\n",
      "Unique errors: 289\n",
      "Unique inputs: 470\n",
      "Iterations: 480\n",
      "------------------------------\n",
      "Unique errors: 309\n",
      "Unique inputs: 498\n",
      "Iterations: 510\n",
      "------------------------------\n",
      "Unique errors: 333\n",
      "Unique inputs: 528\n",
      "Iterations: 540\n",
      "------------------------------\n",
      "Unique errors: 353\n",
      "Unique inputs: 557\n",
      "Iterations: 570\n",
      "------------------------------\n",
      "Unique errors: 374\n",
      "Unique inputs: 586\n",
      "Iterations: 600\n",
      "------------------------------\n",
      "Unique errors: 395\n",
      "Unique inputs: 614\n",
      "Iterations: 630\n",
      "------------------------------\n",
      "Unique errors: 412\n",
      "Unique inputs: 642\n",
      "Iterations: 660\n",
      "------------------------------\n",
      "Unique errors: 432\n",
      "Unique inputs: 671\n",
      "Iterations: 690\n",
      "------------------------------\n",
      "Unique errors: 452\n",
      "Unique inputs: 699\n",
      "Iterations: 720\n",
      "------------------------------\n",
      "Unique errors: 469\n",
      "Unique inputs: 728\n",
      "Iterations: 750\n",
      "------------------------------\n",
      "Unique errors: 492\n",
      "Unique inputs: 758\n",
      "Iterations: 780\n",
      "------------------------------\n",
      "Unique errors: 508\n",
      "Unique inputs: 784\n",
      "Iterations: 810\n",
      "------------------------------\n",
      "Unique errors: 530\n",
      "Unique inputs: 813\n",
      "Iterations: 840\n",
      "------------------------------\n",
      "Unique errors: 550\n",
      "Unique inputs: 841\n",
      "Iterations: 870\n",
      "------------------------------\n",
      "Unique errors: 566\n",
      "Unique inputs: 869\n",
      "Iterations: 900\n",
      "------------------------------\n",
      "Unique errors: 580\n",
      "Unique inputs: 894\n",
      "Iterations: 930\n",
      "------------------------------\n",
      "Unique errors: 600\n",
      "Unique inputs: 924\n",
      "Iterations: 960\n",
      "------------------------------\n",
      "Unique errors: 619\n",
      "Unique inputs: 952\n",
      "Iterations: 990\n",
      "------------------------------\n",
      "Unique errors: 639\n",
      "Unique inputs: 980\n",
      "Iterations: 1020\n",
      "------------------------------\n",
      "Unique errors: 659\n",
      "Unique inputs: 1009\n",
      "Iterations: 1050\n",
      "------------------------------\n",
      "Unique errors: 679\n",
      "Unique inputs: 1038\n",
      "Iterations: 1080\n",
      "------------------------------\n",
      "Unique errors: 698\n",
      "Unique inputs: 1067\n",
      "Iterations: 1110\n",
      "------------------------------\n",
      "Unique errors: 722\n",
      "Unique inputs: 1096\n",
      "Iterations: 1140\n",
      "------------------------------\n",
      "Unique errors: 745\n",
      "Unique inputs: 1125\n",
      "Iterations: 1170\n",
      "------------------------------\n",
      "Unique errors: 762\n",
      "Unique inputs: 1154\n",
      "Iterations: 1200\n",
      "------------------------------\n",
      "Unique errors: 781\n",
      "Unique inputs: 1184\n",
      "Iterations: 1230\n",
      "------------------------------\n",
      "Unique errors: 800\n",
      "Unique inputs: 1213\n",
      "Iterations: 1260\n",
      "------------------------------\n",
      "Unique errors: 818\n",
      "Unique inputs: 1242\n",
      "Iterations: 1290\n",
      "------------------------------\n",
      "Unique errors: 840\n",
      "Unique inputs: 1271\n",
      "Iterations: 1320\n",
      "------------------------------\n",
      "Unique errors: 855\n",
      "Unique inputs: 1296\n",
      "Iterations: 1350\n",
      "------------------------------\n",
      "Unique errors: 875\n",
      "Unique inputs: 1323\n",
      "Iterations: 1380\n",
      "------------------------------\n",
      "Unique errors: 893\n",
      "Unique inputs: 1353\n",
      "Iterations: 1410\n",
      "------------------------------\n",
      "Unique errors: 910\n",
      "Unique inputs: 1382\n",
      "Iterations: 1440\n",
      "------------------------------\n",
      "Unique errors: 927\n",
      "Unique inputs: 1410\n",
      "Iterations: 1470\n",
      "------------------------------\n",
      "Unique errors: 951\n",
      "Unique inputs: 1439\n",
      "Iterations: 1500\n",
      "------------------------------\n",
      "Unique errors: 970\n",
      "Unique inputs: 1466\n",
      "Iterations: 1530\n",
      "------------------------------\n",
      "Unique errors: 990\n",
      "Unique inputs: 1493\n",
      "Iterations: 1560\n",
      "------------------------------\n",
      "Unique errors: 1008\n",
      "Unique inputs: 1522\n",
      "Iterations: 1590\n",
      "------------------------------\n",
      "Unique errors: 1030\n",
      "Unique inputs: 1552\n",
      "Iterations: 1620\n",
      "------------------------------\n",
      "Unique errors: 1046\n",
      "Unique inputs: 1576\n",
      "Iterations: 1650\n",
      "------------------------------\n",
      "Unique errors: 1069\n",
      "Unique inputs: 1606\n",
      "Iterations: 1680\n",
      "------------------------------\n",
      "Unique errors: 1090\n",
      "Unique inputs: 1631\n",
      "Iterations: 1710\n",
      "------------------------------\n",
      "Unique errors: 1105\n",
      "Unique inputs: 1659\n",
      "Iterations: 1740\n",
      "------------------------------\n",
      "Unique errors: 1125\n",
      "Unique inputs: 1687\n",
      "Iterations: 1770\n",
      "------------------------------\n",
      "Unique errors: 1143\n",
      "Unique inputs: 1714\n",
      "Iterations: 1800\n",
      "------------------------------\n",
      "Unique errors: 1160\n",
      "Unique inputs: 1743\n",
      "Iterations: 1830\n",
      "------------------------------\n",
      "Unique errors: 1179\n",
      "Unique inputs: 1769\n",
      "Iterations: 1860\n",
      "------------------------------\n",
      "Unique errors: 1199\n",
      "Unique inputs: 1798\n",
      "Iterations: 1890\n",
      "------------------------------\n",
      "Unique errors: 1214\n",
      "Unique inputs: 1825\n",
      "Iterations: 1920\n",
      "------------------------------\n",
      "Unique errors: 1238\n",
      "Unique inputs: 1855\n",
      "Iterations: 1950\n",
      "------------------------------\n",
      "Unique errors: 1259\n",
      "Unique inputs: 1884\n",
      "Iterations: 1980\n",
      "------------------------------\n",
      "Unique errors: 1277\n",
      "Unique inputs: 1912\n",
      "Iterations: 2010\n",
      "------------------------------\n",
      "Unique errors: 1294\n",
      "Unique inputs: 1940\n",
      "Iterations: 2040\n",
      "------------------------------\n",
      "Unique errors: 1315\n",
      "Unique inputs: 1969\n",
      "Iterations: 2070\n",
      "------------------------------\n",
      "Unique errors: 1329\n",
      "Unique inputs: 1999\n",
      "Iterations: 2100\n",
      "------------------------------\n",
      "Unique errors: 1352\n",
      "Unique inputs: 2027\n",
      "Iterations: 2130\n",
      "------------------------------\n",
      "Unique errors: 1369\n",
      "Unique inputs: 2055\n",
      "Iterations: 2160\n",
      "------------------------------\n",
      "Unique errors: 1385\n",
      "Unique inputs: 2081\n",
      "Iterations: 2190\n",
      "------------------------------\n",
      "Unique errors: 1399\n",
      "Unique inputs: 2109\n",
      "Iterations: 2220\n",
      "------------------------------\n",
      "Unique errors: 1418\n",
      "Unique inputs: 2139\n",
      "Iterations: 2250\n",
      "------------------------------\n",
      "Unique errors: 1437\n",
      "Unique inputs: 2168\n",
      "Iterations: 2280\n",
      "------------------------------\n",
      "Unique errors: 1453\n",
      "Unique inputs: 2193\n",
      "Iterations: 2310\n",
      "------------------------------\n",
      "Unique errors: 1468\n",
      "Unique inputs: 2221\n",
      "Iterations: 2340\n",
      "------------------------------\n",
      "Unique errors: 1491\n",
      "Unique inputs: 2250\n",
      "Iterations: 2370\n",
      "------------------------------\n",
      "Unique errors: 1509\n",
      "Unique inputs: 2278\n",
      "Iterations: 2400\n",
      "------------------------------\n",
      "Unique errors: 1530\n",
      "Unique inputs: 2308\n",
      "Iterations: 2430\n",
      "------------------------------\n",
      "Unique errors: 1550\n",
      "Unique inputs: 2336\n",
      "Iterations: 2460\n",
      "------------------------------\n",
      "Unique errors: 1568\n",
      "Unique inputs: 2363\n",
      "Iterations: 2490\n",
      "------------------------------\n",
      "Unique errors: 1586\n",
      "Unique inputs: 2391\n",
      "Iterations: 2520\n",
      "------------------------------\n",
      "Unique errors: 1603\n",
      "Unique inputs: 2420\n",
      "Iterations: 2550\n",
      "------------------------------\n",
      "Unique errors: 1623\n",
      "Unique inputs: 2449\n",
      "Iterations: 2580\n",
      "------------------------------\n",
      "Unique errors: 1636\n",
      "Unique inputs: 2473\n",
      "Iterations: 2610\n",
      "------------------------------\n",
      "Unique errors: 1658\n",
      "Unique inputs: 2502\n",
      "Iterations: 2640\n",
      "------------------------------\n",
      "Unique errors: 1676\n",
      "Unique inputs: 2529\n",
      "Iterations: 2670\n",
      "------------------------------\n",
      "Unique errors: 1697\n",
      "Unique inputs: 2559\n",
      "Iterations: 2700\n",
      "------------------------------\n",
      "Unique errors: 1714\n",
      "Unique inputs: 2586\n",
      "Iterations: 2730\n",
      "------------------------------\n",
      "Unique errors: 1730\n",
      "Unique inputs: 2614\n",
      "Iterations: 2760\n",
      "------------------------------\n",
      "Unique errors: 1748\n",
      "Unique inputs: 2642\n",
      "Iterations: 2790\n",
      "------------------------------\n",
      "Unique errors: 1767\n",
      "Unique inputs: 2672\n",
      "Iterations: 2820\n",
      "------------------------------\n",
      "Unique errors: 1783\n",
      "Unique inputs: 2700\n",
      "Iterations: 2850\n",
      "------------------------------\n",
      "Unique errors: 1806\n",
      "Unique inputs: 2728\n",
      "Iterations: 2880\n",
      "------------------------------\n",
      "Unique errors: 1825\n",
      "Unique inputs: 2754\n",
      "Iterations: 2910\n",
      "------------------------------\n",
      "Unique errors: 1843\n",
      "Unique inputs: 2783\n",
      "Iterations: 2940\n",
      "------------------------------\n",
      "Unique errors: 1865\n",
      "Unique inputs: 2812\n",
      "Iterations: 2970\n",
      "------------------------------\n",
      "1994\n",
      "0.6646666666666666\n",
      "Final Unique errors: 1885\n",
      "Final Unique inputs: 2840\n"
     ]
    }
   ],
   "source": [
    "THRESH_RANGE = range(5, 10, 5)\n",
    "\n",
    "for THRESHOLD in THRESH_RANGE:\n",
    "    THRESHOLD = (THRESHOLD/100)\n",
    "    print(\"THRESHOLD: \", str(THRESHOLD))\n",
    "    \n",
    "    with open('saved_pickles/Exploration/unique_input1_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        unique_input1_set = pickle.load(handle)\n",
    "\n",
    "    with open('saved_pickles/Exploration/unique_input1_error_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        unique_input1_error_set = pickle.load(handle)\n",
    "\n",
    "    with open('saved_pickles/Exploration/occupation_pair_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        occupation_pair_error = pickle.load(handle)\n",
    "\n",
    "    with open('saved_pickles/Exploration/occupation1_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        occupation1_error = pickle.load(handle)\n",
    "\n",
    "    with open('saved_pickles/Exploration/occupation2_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        occupation2_error = pickle.load(handle)  \n",
    "\n",
    "    with open('saved_pickles/Exploration/verb_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        verb_error = pickle.load(handle)    \n",
    "\n",
    "    with open('saved_pickles/Exploration/action_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        action_error = pickle.load(handle)\n",
    "\n",
    "    with open('saved_pickles/Exploration/occupation_pair_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        occupation_pair_count = pickle.load(handle)\n",
    "\n",
    "    with open('saved_pickles/Exploration/occupation1_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        occupation1_count = pickle.load(handle)\n",
    "\n",
    "    with open('saved_pickles/Exploration/occupation2_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        occupation2_count = pickle.load(handle)  \n",
    "\n",
    "    with open('saved_pickles/Exploration/verb_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        verb_count = pickle.load(handle)    \n",
    "\n",
    "    with open('saved_pickles/Exploration/action_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        action_count = pickle.load(handle)\n",
    "        \n",
    "    unique_input1_set_exploitation = set()\n",
    "    unique_input1_error_set_exploitation = set()\n",
    "\n",
    "    occupation_pair_error_exploitation = {}\n",
    "\n",
    "    occupation1_error_exploitation = {}\n",
    "\n",
    "    occupation2_error_exploitation = {}\n",
    "\n",
    "    verb_error_exploitation = {}\n",
    "\n",
    "    action_error_exploitation = {}\n",
    "\n",
    "    occupation_pair_count_exploitation = {}\n",
    "\n",
    "    occupation1_count_exploitation = {}\n",
    "\n",
    "    occupation2_count_exploitation = {}\n",
    "\n",
    "    verb_count_exploitation = {}\n",
    "\n",
    "    action_count_exploitation = {}\n",
    "    \n",
    "    generate_test_sentences(ITERS=3000)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/unique_input1_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(unique_input1_set_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/unique_input1_error_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(unique_input1_error_set_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/occupation_pair_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation_pair_count_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/occupation1_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation1_count_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/occupation2_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation2_count_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/verb_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(verb_count_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/action_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(action_count_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/occupation_pair_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation_pair_error_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/occupation1_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation1_error_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/occupation2_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation2_error_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/verb_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(verb_error_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/Exploitation/action_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "        pickle.dump(action_error_exploitation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_input1_set_exploitation = set()\n",
    "# unique_input1_error_set_exploitation = set()\n",
    "\n",
    "# occupation_pair_error_exploitation = {}\n",
    "\n",
    "# occupation1_error_exploitation = {}\n",
    "\n",
    "# occupation2_error_exploitation = {}\n",
    "\n",
    "# verb_error_exploitation = {}\n",
    "\n",
    "# action_error_exploitation = {}\n",
    "\n",
    "# occupation_pair_count_exploitation = {}\n",
    "\n",
    "# occupation1_count_exploitation = {}\n",
    "\n",
    "# occupation2_count_exploitation = {}\n",
    "\n",
    "# verb_count_exploitation = {}\n",
    "\n",
    "# action_count_exploitation = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_test_sentences(ITERS=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('saved_pickles/Exploitation/unique_input1_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/unique_input1_error_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(unique_input1_error_set, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation_pair_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation1_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation2_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/verb_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_count, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploitation/action_count-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation_pair_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation1_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation2_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/verb_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_error, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploitation/action_error-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e8d428c364e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# model_type = \"bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# model_type = \"bert-base-cased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Unique errors Exploration: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mTHRESHOLD\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTHRESH_RANGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_type' is not defined"
     ]
    }
   ],
   "source": [
    "# for THRESHOLD in range(5, 35, 5):\n",
    "#     THRESHOLD = (THRESHOLD/100)\n",
    "    \n",
    "#     with open('saved_pickles/Exploration/unique_input1_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "#         unique_input1_set = pickle.load(handle)\n",
    "\n",
    "#     with open('saved_pickles/Exploration/unique_input1_error_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "#         unique_input1_error_set = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "#     with open('saved_pickles/Exploitation/unique_input1_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "#         unique_input1_set_exploitation = pickle.load(handle)\n",
    "\n",
    "#     with open('saved_pickles/Exploitation/unique_input1_error_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "#         unique_input1_error_set_exploitation = pickle.load(handle)\n",
    "    \n",
    "\n",
    "    \n",
    "#     print(model_type, THRESHOLD)\n",
    "#     print()\n",
    "#     print(\"Final Unique errors Exploration: \" + str(len(unique_input1_error_set)))\n",
    "#     print(\"Final Unique inputs Exploration: \" + str(len(unique_input1_set)))\n",
    "#     print()\n",
    "#     print(\"Final Unique errors Exploitation: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "#     print(\"Final Unique inputs Exploitation: \" + str(len(unique_input1_set_exploitation)))\n",
    "#     print(\"--------------------\")\n",
    "# THRESH_RANGE = range(5, 35, 5)\n",
    "# model_type = \"bert-base-uncased\"\n",
    "# model_type = \"bert-base-cased\"\n",
    "print(model_type)\n",
    "print(\"Final Unique errors Exploration: \")\n",
    "for THRESHOLD in THRESH_RANGE:\n",
    "    THRESHOLD = (THRESHOLD/100)\n",
    "    with open('saved_pickles/Exploration/unique_input1_error_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        unique_input1_error_set = pickle.load(handle)\n",
    "    print(str(len(unique_input1_error_set)))\n",
    "print()\n",
    "print(\"Final Unique inputs Exploration: \")\n",
    "for THRESHOLD in THRESH_RANGE:\n",
    "    THRESHOLD = (THRESHOLD/100)\n",
    "    with open('saved_pickles/Exploration/unique_input1_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        unique_input1_error = pickle.load(handle)\n",
    "    print(str(len(unique_input1_error)))\n",
    "print()\n",
    "print(\"Final Unique errors Exploitation: \")\n",
    "for THRESHOLD in THRESH_RANGE:\n",
    "    THRESHOLD = (THRESHOLD/100)\n",
    "    with open('saved_pickles/Exploitation/unique_input1_error_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        unique_input1_error_set_exploitation = pickle.load(handle)\n",
    "    print(str(len(unique_input1_error_set_exploitation)))\n",
    "print()\n",
    "print(\"Final Unique inputs Exploitation: \")\n",
    "for THRESHOLD in THRESH_RANGE:\n",
    "    THRESHOLD = (THRESHOLD/100)\n",
    "    with open('saved_pickles/Exploitation/unique_input1_set-'+ model_type + '-' + str(THRESHOLD) +'.pickle', 'rb') as handle:\n",
    "        unique_input1_error_exploitation = pickle.load(handle)\n",
    "    print(str(len(unique_input1_error_exploitation)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
