{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CoreNLP Exploitation Unambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 2.66MB/s]                    \n",
      "2020-07-20 15:51:10 INFO: Downloading default packages for language: en (English)...\n",
      "2020-07-20 15:51:11 INFO: File exists: /Users/sakshiudeshi/stanza_resources/en/default.zip.\n",
      "2020-07-20 15:51:14 INFO: Finished downloading models and saved to /Users/sakshiudeshi/stanza_resources.\n",
      "2020-07-20 15:51:14 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-07-20 15:51:14 INFO: Use device: cpu\n",
      "2020-07-20 15:51:14 INFO: Loading: tokenize\n",
      "2020-07-20 15:51:14 INFO: Loading: pos\n",
      "2020-07-20 15:51:14 INFO: Loading: lemma\n",
      "2020-07-20 15:51:15 INFO: Loading: depparse\n",
      "2020-07-20 15:51:15 INFO: Loading: ner\n",
      "2020-07-20 15:51:16 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "stanza.download('en') \n",
    "import sys, os\n",
    "\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "nlp = stanza.Pipeline()\n",
    "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()\n",
    "pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-c46ac54451bb48a3.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CORENLP_HOME\"] = \"/Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0\"\n",
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "with CoreNLPClient(\n",
    "        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],\n",
    "        timeout=30000,\n",
    "        memory='16G') as client:\n",
    "    ann = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked', 'disclosed to', 'told'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified', 'spoke with'] ##  pronoun2 = ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    return ['He', 'She', 'They']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"about \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "\n",
    "action1 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p1[1]] = [action1] ### disclosed to\n",
    "verb_action[verb_list_p1[2]] = [action1] ### told\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action1] ### notified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spoke with \n",
    "### asked\n",
    "action2 = ['painting', 'dancing',\n",
    "           'fencing', 'gymnastics',\n",
    "           'independent cinema', 'woodworking',\n",
    "           'studying', 'horse racing',\n",
    "           'singing', 'kayaking', 'football',\n",
    "           'baseball', 'basketball', 'quizzing', \n",
    "           'gardening', 'blogging', 'board games', \n",
    "           'breadmaking', 'baking']\n",
    "verb_action[verb_list_p2[1]] = [action2]\n",
    "verb_action[verb_list_p1[0]] = [action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_verb = ['standing', 'speaking', 'talking', \n",
    "                  'passing by', 'sitting', 'sitting on the chair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    doc = client.annotate(sentence)\n",
    "#     print((doc.))\n",
    "    return(doc.corefChain)\n",
    "#     if doc._.has_coref: \n",
    "#         return (doc._.coref_resolved, doc._.coref_clusters)\n",
    "#     else:\n",
    "#         return ('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_list(pred_str):\n",
    "    pred_list = []\n",
    "    for line in pred_str.split('\\n'):\n",
    "        if 'gender' not in line:\n",
    "            pred_list.append(line)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_equivalence(pred1, pred2):\n",
    "    pred1_list = get_pred_list(str(pred1))\n",
    "    pred2_list = get_pred_list(str(pred2))\n",
    "#     print(pred1_list)\n",
    "#     print(pred2_list)\n",
    "    return pred1_list == pred2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(oc1, oc2, verb, action, pronoun):\n",
    "\n",
    "\n",
    "    aux_verb = random.choice(auxiliary_verb)\n",
    "    input1 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[0] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "\n",
    "    input2 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[1] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "        \n",
    "    return input1, input2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/unique_input1_set.pickle', 'rb') as handle:\n",
    "    unique_input1_set = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'rb') as handle:\n",
    "    unique_input1_error_set = pickle.load(handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'rb') as handle:\n",
    "    occupation_pair_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_error.pickle', 'rb') as handle:\n",
    "    occupation1_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error.pickle', 'rb') as handle:\n",
    "    occupation2_error = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error.pickle', 'rb') as handle:\n",
    "    verb_error = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/Exploration/action_error.pickle', 'rb') as handle:\n",
    "    action_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'rb') as handle:\n",
    "    occupation_pair_count = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_count.pickle', 'rb') as handle:\n",
    "    occupation1_count = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count.pickle', 'rb') as handle:\n",
    "    occupation2_count = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count.pickle', 'rb') as handle:\n",
    "    verb_count = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/Exploration/action_count.pickle', 'rb') as handle:\n",
    "    action_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_dict(D):\n",
    "    return {k: v for k, v in sorted(D.items(), key=lambda item: item[1], reverse=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate_dict(error_dict, count_dict):\n",
    "    error_rate_dict = {}\n",
    "    for key in error_dict:\n",
    "        error_rate_dict[key] = error_dict[key]/count_dict[key]\n",
    "    return get_sorted_dict(error_rate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dict(error_dict, count_dict):\n",
    "    error_rate_dict = get_error_rate_dict(error_dict, count_dict)\n",
    "    \n",
    "    probability_dict = {}\n",
    "    error_rate_sum = sum(error_rate_dict.values())\n",
    "    for error_rate in error_rate_dict:\n",
    "        probability_dict[error_rate] = error_rate_dict[error_rate]/error_rate_sum\n",
    "    \n",
    "    return probability_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_random_choice(error_dict, count_dict, probablilities_dict = None):\n",
    "    if probablilities_dict == None:\n",
    "        probability_dict = get_probability_dict(error_dict, count_dict)\n",
    "    else:\n",
    "        probability_dict = probablilities_dict\n",
    "    \n",
    "    return list(probability_dict.keys())[np.random.choice(len(list(probability_dict.keys())), p=list(probability_dict.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set_exploitation = set()\n",
    "unique_input1_error_set_exploitation = set()\n",
    "\n",
    "occupation_pair_error_exploitation = {}\n",
    "\n",
    "occupation1_error_exploitation = {}\n",
    "\n",
    "occupation2_error_exploitation = {}\n",
    "\n",
    "verb_error_exploitation = {}\n",
    "\n",
    "action_error_exploitation = {}\n",
    "\n",
    "occupation_pair_count_exploitation = {}\n",
    "\n",
    "occupation1_count_exploitation = {}\n",
    "\n",
    "occupation2_count_exploitation = {}\n",
    "\n",
    "verb_count_exploitation = {}\n",
    "\n",
    "action_count_exploitation = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'technician': 0.805964295571082, 'manager': 0.06766120012201676, 'developer': 0.06667344537570993, 'driver': 0.05970105893119126}\n",
      "\n",
      "{'technician': 0.08823529411764706, 'manager': 0.007407407407407408, 'developer': 0.0072992700729927005, 'driver': 0.006535947712418301}\n",
      "\n",
      "{'technician': 80907, 'manager': 6715, 'developer': 6494, 'driver': 5884}\n"
     ]
    }
   ],
   "source": [
    "oc1_probability = get_probability_dict(occupation1_error, occupation1_count)\n",
    "\n",
    "print(oc1_probability)\n",
    "print()\n",
    "\n",
    "error_rate_dict = get_error_rate_dict(occupation1_error, occupation1_count)\n",
    "print(error_rate_dict)\n",
    "print()\n",
    "\n",
    "output_dict = {}\n",
    "for i in range(100000):\n",
    "    oc1 = get_weighted_random_choice(occupation1_error, occupation1_count, probablilities_dict=oc1_probability)\n",
    "    update_dict(output_dict, oc1)\n",
    "print(get_sorted_dict(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_sentences(ITERS=3000):\n",
    "    err_count = 0\n",
    "\n",
    "    oc1_probability = get_probability_dict(occupation1_error, occupation1_count)\n",
    "    oc2_probability = get_probability_dict(occupation2_error, occupation2_count)\n",
    "\n",
    "    for i in range(ITERS):\n",
    "#         oc1 = random.choice(occupations_1)\n",
    "#         oc2 = random.choice(occupations_2)\n",
    "        oc1 = get_weighted_random_choice(occupation1_error, occupation1_count, probablilities_dict=oc1_probability)\n",
    "        oc2 = get_weighted_random_choice(occupation2_error, occupation2_count, probablilities_dict=oc2_probability)\n",
    "        verb = random.choice(list(verb_action.keys()))\n",
    "        action = random.choice(random.choice(verb_action[verb]))\n",
    "        pronoun = choose_pronoun_type(verb)\n",
    "        input1, input2 = generate_sentences(oc1, oc2, verb, action, pronoun)\n",
    "\n",
    "\n",
    "#         input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#                + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "        pred1 = predict_clusters(input1)\n",
    "        pred2 = predict_clusters(input2)\n",
    "#         pred3, _ = predict_clusters(input3)\n",
    "\n",
    "\n",
    "        if(i % 30 == 0):\n",
    "            print(\"Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "            print(\"Unique inputs: \" + str(len(unique_input1_set_exploitation)))\n",
    "            print(\"Iterations: \" + str(i))\n",
    "            print(\"------------------------------\")\n",
    "\n",
    "        if input1 not in unique_input1_set:\n",
    "            unique_input1_set_exploitation.add(input1)\n",
    "\n",
    "        update_dict(occupation_pair_count_exploitation, (oc1, oc2))\n",
    "        update_dict(occupation1_count_exploitation, oc1)\n",
    "        update_dict(occupation2_count_exploitation, oc2)\n",
    "        update_dict(verb_count_exploitation, verb)\n",
    "        update_dict(action_count_exploitation, action)\n",
    "\n",
    "\n",
    "\n",
    "        if not (get_pred_equivalence(pred1, pred2)):\n",
    "#             if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#                 if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0]) ):\n",
    "    #         if(True):\n",
    "                    err_count += 1\n",
    "        \n",
    "                    \n",
    "                    if input1 not in unique_input1_error_set:\n",
    "                        unique_input1_error_set_exploitation.add(input1)\n",
    "                    \n",
    "#                         if (pred2 != ''):\n",
    "#                         print(pred1, pred2)\n",
    "#                         print()\n",
    "#                         print(input1)\n",
    "#                         print(input2)\n",
    "#                         print(\"--------------\")\n",
    "#                         else:\n",
    "#                             print(\"empty pred2 error\")\n",
    "    #                 print(input3)\n",
    "\n",
    "                    update_dict(occupation_pair_error_exploitation, (oc1, oc2))\n",
    "                    update_dict(occupation1_error_exploitation, oc1)\n",
    "                    update_dict(occupation2_error_exploitation, oc2)\n",
    "                    update_dict(verb_error_exploitation, verb)\n",
    "                    update_dict(action_error_exploitation, action)\n",
    "\n",
    "\n",
    "\n",
    "    print(err_count)\n",
    "    print(err_count/ITERS)\n",
    "    print(\"Final Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "    print(\"Final Unique inputs: \" + str(len(unique_input1_set_exploitation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-c46ac54451bb48a3.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n",
      "Unique errors: 0\n",
      "Unique inputs: 0\n",
      "Iterations: 0\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 29\n",
      "Iterations: 30\n",
      "------------------------------\n",
      "Unique errors: 9\n",
      "Unique inputs: 59\n",
      "Iterations: 60\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 87\n",
      "Iterations: 90\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 115\n",
      "Iterations: 120\n",
      "------------------------------\n",
      "Unique errors: 15\n",
      "Unique inputs: 145\n",
      "Iterations: 150\n",
      "------------------------------\n",
      "Unique errors: 15\n",
      "Unique inputs: 174\n",
      "Iterations: 180\n",
      "------------------------------\n",
      "Unique errors: 19\n",
      "Unique inputs: 202\n",
      "Iterations: 210\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique inputs: 231\n",
      "Iterations: 240\n",
      "------------------------------\n",
      "Unique errors: 31\n",
      "Unique inputs: 261\n",
      "Iterations: 270\n",
      "------------------------------\n",
      "Unique errors: 34\n",
      "Unique inputs: 289\n",
      "Iterations: 300\n",
      "------------------------------\n",
      "Unique errors: 40\n",
      "Unique inputs: 317\n",
      "Iterations: 330\n",
      "------------------------------\n",
      "Unique errors: 43\n",
      "Unique inputs: 345\n",
      "Iterations: 360\n",
      "------------------------------\n",
      "Unique errors: 45\n",
      "Unique inputs: 373\n",
      "Iterations: 390\n",
      "------------------------------\n",
      "Unique errors: 49\n",
      "Unique inputs: 400\n",
      "Iterations: 420\n",
      "------------------------------\n",
      "Unique errors: 52\n",
      "Unique inputs: 430\n",
      "Iterations: 450\n",
      "------------------------------\n",
      "Unique errors: 56\n",
      "Unique inputs: 458\n",
      "Iterations: 480\n",
      "------------------------------\n",
      "Unique errors: 56\n",
      "Unique inputs: 483\n",
      "Iterations: 510\n",
      "------------------------------\n",
      "Unique errors: 61\n",
      "Unique inputs: 510\n",
      "Iterations: 540\n",
      "------------------------------\n",
      "Unique errors: 64\n",
      "Unique inputs: 538\n",
      "Iterations: 570\n",
      "------------------------------\n",
      "Unique errors: 69\n",
      "Unique inputs: 565\n",
      "Iterations: 600\n",
      "------------------------------\n",
      "Unique errors: 70\n",
      "Unique inputs: 591\n",
      "Iterations: 630\n",
      "------------------------------\n",
      "Unique errors: 73\n",
      "Unique inputs: 617\n",
      "Iterations: 660\n",
      "------------------------------\n",
      "Unique errors: 78\n",
      "Unique inputs: 642\n",
      "Iterations: 690\n",
      "------------------------------\n",
      "Unique errors: 80\n",
      "Unique inputs: 670\n",
      "Iterations: 720\n",
      "------------------------------\n",
      "Unique errors: 82\n",
      "Unique inputs: 696\n",
      "Iterations: 750\n",
      "------------------------------\n",
      "Unique errors: 87\n",
      "Unique inputs: 723\n",
      "Iterations: 780\n",
      "------------------------------\n",
      "Unique errors: 90\n",
      "Unique inputs: 747\n",
      "Iterations: 810\n",
      "------------------------------\n",
      "Unique errors: 91\n",
      "Unique inputs: 772\n",
      "Iterations: 840\n",
      "------------------------------\n",
      "Unique errors: 95\n",
      "Unique inputs: 798\n",
      "Iterations: 870\n",
      "------------------------------\n",
      "Unique errors: 98\n",
      "Unique inputs: 824\n",
      "Iterations: 900\n",
      "------------------------------\n",
      "Unique errors: 104\n",
      "Unique inputs: 849\n",
      "Iterations: 930\n",
      "------------------------------\n",
      "Unique errors: 106\n",
      "Unique inputs: 877\n",
      "Iterations: 960\n",
      "------------------------------\n",
      "Unique errors: 106\n",
      "Unique inputs: 901\n",
      "Iterations: 990\n",
      "------------------------------\n",
      "Unique errors: 108\n",
      "Unique inputs: 927\n",
      "Iterations: 1020\n",
      "------------------------------\n",
      "Unique errors: 111\n",
      "Unique inputs: 952\n",
      "Iterations: 1050\n",
      "------------------------------\n",
      "Unique errors: 113\n",
      "Unique inputs: 976\n",
      "Iterations: 1080\n",
      "------------------------------\n",
      "Unique errors: 118\n",
      "Unique inputs: 1002\n",
      "Iterations: 1110\n",
      "------------------------------\n",
      "Unique errors: 119\n",
      "Unique inputs: 1028\n",
      "Iterations: 1140\n",
      "------------------------------\n",
      "Unique errors: 122\n",
      "Unique inputs: 1054\n",
      "Iterations: 1170\n",
      "------------------------------\n",
      "Unique errors: 123\n",
      "Unique inputs: 1080\n",
      "Iterations: 1200\n",
      "------------------------------\n",
      "Unique errors: 127\n",
      "Unique inputs: 1107\n",
      "Iterations: 1230\n",
      "------------------------------\n",
      "Unique errors: 128\n",
      "Unique inputs: 1129\n",
      "Iterations: 1260\n",
      "------------------------------\n",
      "Unique errors: 130\n",
      "Unique inputs: 1151\n",
      "Iterations: 1290\n",
      "------------------------------\n",
      "Unique errors: 130\n",
      "Unique inputs: 1174\n",
      "Iterations: 1320\n",
      "------------------------------\n",
      "Unique errors: 131\n",
      "Unique inputs: 1194\n",
      "Iterations: 1350\n",
      "------------------------------\n",
      "Unique errors: 133\n",
      "Unique inputs: 1217\n",
      "Iterations: 1380\n",
      "------------------------------\n",
      "Unique errors: 137\n",
      "Unique inputs: 1240\n",
      "Iterations: 1410\n",
      "------------------------------\n",
      "Unique errors: 142\n",
      "Unique inputs: 1267\n",
      "Iterations: 1440\n",
      "------------------------------\n",
      "Unique errors: 143\n",
      "Unique inputs: 1288\n",
      "Iterations: 1470\n",
      "------------------------------\n",
      "Unique errors: 145\n",
      "Unique inputs: 1313\n",
      "Iterations: 1500\n",
      "------------------------------\n",
      "Unique errors: 147\n",
      "Unique inputs: 1338\n",
      "Iterations: 1530\n",
      "------------------------------\n",
      "Unique errors: 147\n",
      "Unique inputs: 1358\n",
      "Iterations: 1560\n",
      "------------------------------\n",
      "Unique errors: 148\n",
      "Unique inputs: 1379\n",
      "Iterations: 1590\n",
      "------------------------------\n",
      "Unique errors: 148\n",
      "Unique inputs: 1405\n",
      "Iterations: 1620\n",
      "------------------------------\n",
      "Unique errors: 151\n",
      "Unique inputs: 1425\n",
      "Iterations: 1650\n",
      "------------------------------\n",
      "Unique errors: 155\n",
      "Unique inputs: 1446\n",
      "Iterations: 1680\n",
      "------------------------------\n",
      "Unique errors: 160\n",
      "Unique inputs: 1471\n",
      "Iterations: 1710\n",
      "------------------------------\n",
      "Unique errors: 161\n",
      "Unique inputs: 1497\n",
      "Iterations: 1740\n",
      "------------------------------\n",
      "Unique errors: 162\n",
      "Unique inputs: 1520\n",
      "Iterations: 1770\n",
      "------------------------------\n",
      "Unique errors: 166\n",
      "Unique inputs: 1542\n",
      "Iterations: 1800\n",
      "------------------------------\n",
      "Unique errors: 169\n",
      "Unique inputs: 1563\n",
      "Iterations: 1830\n",
      "------------------------------\n",
      "Unique errors: 171\n",
      "Unique inputs: 1587\n",
      "Iterations: 1860\n",
      "------------------------------\n",
      "Unique errors: 171\n",
      "Unique inputs: 1607\n",
      "Iterations: 1890\n",
      "------------------------------\n",
      "Unique errors: 173\n",
      "Unique inputs: 1628\n",
      "Iterations: 1920\n",
      "------------------------------\n",
      "Unique errors: 173\n",
      "Unique inputs: 1653\n",
      "Iterations: 1950\n",
      "------------------------------\n",
      "Unique errors: 174\n",
      "Unique inputs: 1678\n",
      "Iterations: 1980\n",
      "------------------------------\n",
      "Unique errors: 179\n",
      "Unique inputs: 1698\n",
      "Iterations: 2010\n",
      "------------------------------\n",
      "Unique errors: 181\n",
      "Unique inputs: 1718\n",
      "Iterations: 2040\n",
      "------------------------------\n",
      "Unique errors: 182\n",
      "Unique inputs: 1737\n",
      "Iterations: 2070\n",
      "------------------------------\n",
      "Unique errors: 184\n",
      "Unique inputs: 1761\n",
      "Iterations: 2100\n",
      "------------------------------\n",
      "Unique errors: 186\n",
      "Unique inputs: 1781\n",
      "Iterations: 2130\n",
      "------------------------------\n",
      "Unique errors: 188\n",
      "Unique inputs: 1806\n",
      "Iterations: 2160\n",
      "------------------------------\n",
      "Unique errors: 189\n",
      "Unique inputs: 1830\n",
      "Iterations: 2190\n",
      "------------------------------\n",
      "Unique errors: 193\n",
      "Unique inputs: 1855\n",
      "Iterations: 2220\n",
      "------------------------------\n",
      "Unique errors: 195\n",
      "Unique inputs: 1878\n",
      "Iterations: 2250\n",
      "------------------------------\n",
      "Unique errors: 196\n",
      "Unique inputs: 1901\n",
      "Iterations: 2280\n",
      "------------------------------\n",
      "Unique errors: 198\n",
      "Unique inputs: 1923\n",
      "Iterations: 2310\n",
      "------------------------------\n",
      "Unique errors: 199\n",
      "Unique inputs: 1943\n",
      "Iterations: 2340\n",
      "------------------------------\n",
      "Unique errors: 202\n",
      "Unique inputs: 1964\n",
      "Iterations: 2370\n",
      "------------------------------\n",
      "Unique errors: 206\n",
      "Unique inputs: 1982\n",
      "Iterations: 2400\n",
      "------------------------------\n",
      "Unique errors: 208\n",
      "Unique inputs: 2000\n",
      "Iterations: 2430\n",
      "------------------------------\n",
      "Unique errors: 208\n",
      "Unique inputs: 2021\n",
      "Iterations: 2460\n",
      "------------------------------\n",
      "Unique errors: 210\n",
      "Unique inputs: 2045\n",
      "Iterations: 2490\n",
      "------------------------------\n",
      "Unique errors: 213\n",
      "Unique inputs: 2065\n",
      "Iterations: 2520\n",
      "------------------------------\n",
      "Unique errors: 216\n",
      "Unique inputs: 2085\n",
      "Iterations: 2550\n",
      "------------------------------\n",
      "Unique errors: 218\n",
      "Unique inputs: 2102\n",
      "Iterations: 2580\n",
      "------------------------------\n",
      "Unique errors: 220\n",
      "Unique inputs: 2126\n",
      "Iterations: 2610\n",
      "------------------------------\n",
      "Unique errors: 221\n",
      "Unique inputs: 2148\n",
      "Iterations: 2640\n",
      "------------------------------\n",
      "Unique errors: 224\n",
      "Unique inputs: 2173\n",
      "Iterations: 2670\n",
      "------------------------------\n",
      "Unique errors: 224\n",
      "Unique inputs: 2190\n",
      "Iterations: 2700\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 224\n",
      "Unique inputs: 2209\n",
      "Iterations: 2730\n",
      "------------------------------\n",
      "Unique errors: 225\n",
      "Unique inputs: 2226\n",
      "Iterations: 2760\n",
      "------------------------------\n",
      "Unique errors: 227\n",
      "Unique inputs: 2241\n",
      "Iterations: 2790\n",
      "------------------------------\n",
      "Unique errors: 229\n",
      "Unique inputs: 2265\n",
      "Iterations: 2820\n",
      "------------------------------\n",
      "Unique errors: 229\n",
      "Unique inputs: 2286\n",
      "Iterations: 2850\n",
      "------------------------------\n",
      "Unique errors: 229\n",
      "Unique inputs: 2304\n",
      "Iterations: 2880\n",
      "------------------------------\n",
      "Unique errors: 232\n",
      "Unique inputs: 2325\n",
      "Iterations: 2910\n",
      "------------------------------\n",
      "Unique errors: 235\n",
      "Unique inputs: 2343\n",
      "Iterations: 2940\n",
      "------------------------------\n",
      "Unique errors: 238\n",
      "Unique inputs: 2360\n",
      "Iterations: 2970\n",
      "------------------------------\n",
      "333\n",
      "0.111\n",
      "Final Unique errors: 239\n",
      "Final Unique inputs: 2379\n"
     ]
    }
   ],
   "source": [
    "generate_test_sentences(ITERS=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('saved_pickles/Exploitation/unique_input1_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_set_exploitation, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploitation/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_error_set_exploitation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploitation/occupation_pair_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_count_exploitation, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploitation/occupation1_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_count_exploitation, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploitation/occupation2_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_count_exploitation, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploitation/verb_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_count_exploitation, handle)\n",
    "\n",
    "with open('saved_pickles/Exploitation/action_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_count_exploitation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploitation/occupation_pair_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_error_exploitation, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploitation/occupation1_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_error_exploitation, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploitation/occupation2_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_error_exploitation, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploitation/verb_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_error_exploitation, handle)\n",
    "\n",
    "with open('saved_pickles/Exploitation/action_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_error_exploitation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The technician was passing by. He asked the housekeeper about fencing.', 'The technician was passing by. He notified the housekeeper that the project scope had been finalised.', 'The technician was passing by. He notified the teacher that the application was under review.', 'The technician was passing by. He disclosed to the teacher that the game was excellent.', 'The technician was passing by. He disclosed to the clerk that the application was approved.', 'The technician was passing by. He told the clerk that the application was not approved.', 'The technician was passing by. He told the auditor that the meat was eaten.', 'The technician was passing by. He disclosed to the clerk that the meat was eaten.', 'The technician was passing by. He notified the paralegal that the waiting times were estimated.', 'The technician was passing by. He spoke with the auditor about horse racing.', 'The technician was passing by. He disclosed to the clerk that the performance was excellent.', 'The technician was passing by. He notified the paralegal that the work was unsatisfactory.', 'The technician was passing by. He disclosed to the secretary that the application was under review.', 'The technician was passing by. He asked the housekeeper about kayaking.', 'The technician was passing by. He told the assistant that the delivery was made.', 'The technician was passing by. He told the assistant that the tournament was excellent.', 'The technician was passing by. He disclosed to the housekeeper that the performance was excellent.', 'The technician was passing by. He told the auditor that the work was unsatisfactory.', 'The technician was passing by. He told the paralegal that the performance was excellent.', 'The technician was passing by. He told the auditor that the waiting times were estimated.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set_exploitation)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
