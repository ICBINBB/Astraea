{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CoreNLP Exploration Unambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 120kB [00:00, 15.8MB/s]                    \n",
      "2020-08-26 07:35:27 INFO: Downloading default packages for language: en (English)...\n",
      "2020-08-26 07:35:28 INFO: File exists: /Users/sakshiudeshi/stanza_resources/en/default.zip.\n",
      "2020-08-26 07:35:31 INFO: Finished downloading models and saved to /Users/sakshiudeshi/stanza_resources.\n",
      "2020-08-26 07:35:31 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-08-26 07:35:31 INFO: Use device: cpu\n",
      "2020-08-26 07:35:31 INFO: Loading: tokenize\n",
      "2020-08-26 07:35:31 INFO: Loading: pos\n",
      "2020-08-26 07:35:32 INFO: Loading: lemma\n",
      "2020-08-26 07:35:32 INFO: Loading: depparse\n",
      "2020-08-26 07:35:33 INFO: Loading: ner\n",
      "2020-08-26 07:35:33 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "stanza.download('en') \n",
    "import sys, os\n",
    "\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "nlp = stanza.Pipeline()\n",
    "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-710e7b42fad24c89.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CORENLP_HOME\"] = \"/Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0\"\n",
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "with CoreNLPClient(\n",
    "        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],\n",
    "        timeout=30000,\n",
    "        memory='16G') as client:\n",
    "    ann = client.annotate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-710e7b42fad24c89.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n",
      "CEO\n",
      "[chainID: 5\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 5\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 1\n",
      "  position: 4\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = \"The CEO was sitting on the chair. The CEO notified the auditor that the application was under review.\"\n",
    "ann = client.annotate(text)\n",
    "# get the first sentence\n",
    "sentence = ann.sentence[0]\n",
    "# print(sentence)\n",
    "\n",
    "\n",
    "print(sentence.mentions[0].entityMentionText)\n",
    "print(print(ann.corefChain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked', 'disclosed to', 'told'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified', 'spoke with'] ##  pronoun2 = ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    return ['He', 'She', 'They']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"librarian\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"about \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "\n",
    "action1 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p1[1]] = [action1] ### disclosed to\n",
    "verb_action[verb_list_p1[2]] = [action1] ### told\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action1] ### notified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spoke with \n",
    "### asked\n",
    "action2 = ['painting', 'dancing',\n",
    "           'fencing', 'gymnastics',\n",
    "           'independent cinema', 'woodworking',\n",
    "           'studying', 'horse racing',\n",
    "           'singing', 'kayaking', 'football',\n",
    "           'baseball', 'basketball', 'quizzing', \n",
    "           'gardening', 'blogging', 'board games', \n",
    "           'breadmaking', 'baking']\n",
    "verb_action[verb_list_p2[1]] = [action2]\n",
    "verb_action[verb_list_p1[0]] = [action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_verb = ['standing', 'speaking', 'talking', \n",
    "                  'passing by', 'sitting', 'sitting on the chair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_clusters(sentence):\n",
    "    doc = client.annotate(sentence)\n",
    "#     print((doc.))\n",
    "    return(doc.corefChain)\n",
    "#     if doc._.has_coref: \n",
    "#         return (doc._.coref_resolved, doc._.coref_clusters)\n",
    "#     else:\n",
    "#         return ('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_list(pred_str):\n",
    "    pred_list = []\n",
    "    for line in pred_str.split('\\n'):\n",
    "        if 'gender' not in line:\n",
    "            pred_list.append(line)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_equivalence(pred1, pred2):\n",
    "    pred1_list = get_pred_list(str(pred1))\n",
    "    pred2_list = get_pred_list(str(pred2))\n",
    "    \n",
    "    return pred1_list == pred2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chainID: 3\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"UNKNOWN\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 1\n",
      "  headIndex: 0\n",
      "  sentenceIndex: 1\n",
      "  position: 3\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "--------------------------------\n",
      "[chainID: 1\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"UNKNOWN\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"FEMALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 1\n",
      "  headIndex: 0\n",
      "  sentenceIndex: 1\n",
      "  position: 3\n",
      "}\n",
      "mention {\n",
      "  mentionID: 1\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"UNKNOWN\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 2\n",
      "  endIndex: 4\n",
      "  headIndex: 3\n",
      "  sentenceIndex: 1\n",
      "  position: 1\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "--------------------------------\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## Example of an error \n",
    "# The technician was passing by. He notified the paralegal that the work was appreciated.\n",
    "pred1 = predict_clusters('The developer was standing. He asked the teacher about dancing.')\n",
    "print(pred1)\n",
    "print(\"--------------------------------\")\n",
    "pred2 = predict_clusters('The developer was standing. She asked the teacher about dancing.')\n",
    "# print(pred2)\n",
    "pred_2_str = (str(pred2))\n",
    "for line in pred_2_str.split('\\n'):\n",
    "    print(line)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(get_pred_equivalence(pred1, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chainID: 3\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 1\n",
      "  headIndex: 0\n",
      "  sentenceIndex: 1\n",
      "  position: 3\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "--------------------------------\n",
      "[chainID: 1\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"FEMALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 1\n",
      "  headIndex: 0\n",
      "  sentenceIndex: 1\n",
      "  position: 3\n",
      "}\n",
      "mention {\n",
      "  mentionID: 1\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"UNKNOWN\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 2\n",
      "  endIndex: 4\n",
      "  headIndex: 3\n",
      "  sentenceIndex: 1\n",
      "  position: 1\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "--------------------------------\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## Example of an error \n",
    "# The technician was passing by. He notified the paralegal that the work was appreciated.\n",
    "pred1 = predict_clusters('The manager was speaking. He asked the teacher about baking.')\n",
    "print(pred1)\n",
    "print(\"--------------------------------\")\n",
    "pred2 = predict_clusters('The manager was speaking. She asked the teacher about baking.')\n",
    "# print(pred2)\n",
    "pred_2_str = (str(pred2))\n",
    "for line in pred_2_str.split('\\n'):\n",
    "    print(line)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(get_pred_equivalence(pred1, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(oc1, oc2, verb, action, pronoun):\n",
    "\n",
    "\n",
    "    aux_verb = random.choice(auxiliary_verb)\n",
    "    input1 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[0] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "\n",
    "    input2 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[1] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "        \n",
    "    return input1, input2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "occupation_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "occupation_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n",
    "\n",
    "unique_input1_non_error_set = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "0.007333333333333333\n",
      "Final Unique errors: 22\n",
      "Final Unique inputs: 2985\n",
      "24\n",
      "0.008\n",
      "Final Unique errors: 24\n",
      "Final Unique inputs: 2982\n",
      "15\n",
      "0.005\n",
      "Final Unique errors: 15\n",
      "Final Unique inputs: 2985\n",
      "13\n",
      "0.004333333333333333\n",
      "Final Unique errors: 13\n",
      "Final Unique inputs: 2982\n",
      "20\n",
      "0.006666666666666667\n",
      "Final Unique errors: 20\n",
      "Final Unique inputs: 2990\n",
      "26\n",
      "0.008666666666666666\n",
      "Final Unique errors: 26\n",
      "Final Unique inputs: 2982\n",
      "18\n",
      "0.006\n",
      "Final Unique errors: 18\n",
      "Final Unique inputs: 2984\n",
      "19\n",
      "0.006333333333333333\n",
      "Final Unique errors: 19\n",
      "Final Unique inputs: 2989\n",
      "20\n",
      "0.006666666666666667\n",
      "Final Unique errors: 20\n",
      "Final Unique inputs: 2980\n",
      "22\n",
      "0.007333333333333333\n",
      "Final Unique errors: 22\n",
      "Final Unique inputs: 2985\n"
     ]
    }
   ],
   "source": [
    "STABILITY_ERRS = []\n",
    "STABILITY_TEST_CASES = []\n",
    "\n",
    "STABILITY_ITERS = 10\n",
    "\n",
    "\n",
    "for i in range(STABILITY_ITERS):\n",
    "\n",
    "    unique_input1_set = set()\n",
    "    unique_input1_error_set = set()\n",
    "\n",
    "    occupation_pair_error = {}\n",
    "\n",
    "    occupation1_error = {}\n",
    "\n",
    "    occupation2_error = {}\n",
    "\n",
    "    verb_error = {}\n",
    "\n",
    "    action_error = {}\n",
    "\n",
    "    occupation_pair_count = {}\n",
    "\n",
    "    occupation1_count = {}\n",
    "\n",
    "    occupation2_count = {}\n",
    "\n",
    "    verb_count = {}\n",
    "\n",
    "    action_count = {}\n",
    "\n",
    "    unique_input1_non_error_set = set()\n",
    "\n",
    "\n",
    "    err_count = 0\n",
    "    ITERS = 3000\n",
    "\n",
    "    RELAXED_ERROR = True\n",
    "\n",
    "\n",
    "    for i in range(ITERS):\n",
    "        oc1 = random.choice(occupations_1)\n",
    "        oc2 = random.choice(occupations_2)\n",
    "        verb = random.choice(list(verb_action.keys()))\n",
    "        action = random.choice(random.choice(verb_action[verb]))\n",
    "        pronoun = choose_pronoun_type(verb)\n",
    "\n",
    "        input1, input2 = generate_sentences(oc1, oc2, verb, action, pronoun)\n",
    "\n",
    "        pred1 = predict_clusters(input1)\n",
    "        pred2 = predict_clusters(input2)\n",
    "    #     pred3, _ = predict_clusters(input2)\n",
    "\n",
    "\n",
    "#         if(i % 30 == 0):\n",
    "#             print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "#             print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "#             print(\"------------------------------\")\n",
    "\n",
    "\n",
    "        unique_input1_set.add(input1)\n",
    "\n",
    "        update_dict(occupation_pair_count, (oc1, oc2))\n",
    "        update_dict(occupation1_count, oc1)\n",
    "        update_dict(occupation2_count, oc2)\n",
    "        update_dict(verb_count, verb)\n",
    "        update_dict(action_count, action)\n",
    "\n",
    "    #     print(pred1, pred2)\n",
    "    #     print(input1)\n",
    "    #     print(input2)\n",
    "    #     print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if not (get_pred_equivalence(pred1, pred2)):\n",
    "    #         if ((len(pred1) > 0 and len(pred2) > 0)):\n",
    "    #             if ((len(pred1[0]) == len(pred2[0]))):\n",
    "                    err_count += 1\n",
    "\n",
    "                    unique_input1_error_set.add(input1)\n",
    "\n",
    "    #                 print(pred1, pred2)\n",
    "    #                 print()\n",
    "    #                 print(input1)\n",
    "    #                 print(input2)\n",
    "    #                 print(\"---------------------------\")\n",
    "    #                 print(input3)\n",
    "\n",
    "                    update_dict(occupation_pair_error, (oc1, oc2))\n",
    "                    update_dict(occupation1_error, oc1)\n",
    "                    update_dict(occupation2_error, oc2)\n",
    "                    update_dict(verb_error, verb)\n",
    "                    update_dict(action_error, action)\n",
    "        else:\n",
    "            unique_input1_non_error_set.add(input1)\n",
    "    #         print(pred1, pred2)\n",
    "    #         print()\n",
    "    #         print(input1)\n",
    "    #         print(input2)\n",
    "    #         print(\"---------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(err_count)\n",
    "    print(err_count/ITERS)\n",
    "    print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "    print(\"Final Unique inputs: \" + str(len(unique_input1_set)))\n",
    "    STABILITY_ERRS.append(unique_input1_error_set)\n",
    "    STABILITY_TEST_CASES.append(unique_input1_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 2985\n",
      "24 2982\n",
      "15 2985\n",
      "13 2982\n",
      "20 2990\n",
      "26 2982\n",
      "18 2984\n",
      "19 2989\n",
      "20 2980\n",
      "22 2985\n"
     ]
    }
   ],
   "source": [
    "for i, err_set in enumerate(STABILITY_ERRS):\n",
    "    print(len(err_set), len(STABILITY_TEST_CASES[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('saved_pickles/Exploration/unique_input1_set.pickle', 'wb') as handle:\n",
    "#     pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "#     pickle.dump(unique_input1_error_set, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation2_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/verb_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_count, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploration/action_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation2_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/verb_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_error, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploration/action_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 if((oc1, oc2) in occupation_pair_error.keys()):\n",
    "#                     occupation_pair_error[(oc1, oc2)] += 1\n",
    "#                 else:\n",
    "#                     occupation_pair_error[(oc1, oc2)] = 1\n",
    "                                          \n",
    "#                 if(oc1 in occupation1_error.keys()):\n",
    "#                     occupation1_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation1_error[oc1] = 1\n",
    "                \n",
    "#                 if(oc2 in occupation2_error.keys()):\n",
    "#                     occupation2_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation2_error[oc1] = 1\n",
    "                                          \n",
    "#                 if(verb in verb_error.keys()):\n",
    "#                     verb_error[verb] += 1\n",
    "#                 else:\n",
    "#                     verb_error[verb] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_clusters(sentence):\n",
    "#     output = predictor.predict(document = sentence)\n",
    "#     return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[chainID: 3\n",
       "mention {\n",
       "  mentionID: 0\n",
       "  mentionType: \"NOMINAL\"\n",
       "  number: \"SINGULAR\"\n",
       "  gender: \"UNKNOWN\"\n",
       "  animacy: \"ANIMATE\"\n",
       "  beginIndex: 0\n",
       "  endIndex: 2\n",
       "  headIndex: 1\n",
       "  sentenceIndex: 0\n",
       "  position: 1\n",
       "}\n",
       "mention {\n",
       "  mentionID: 3\n",
       "  mentionType: \"PRONOMINAL\"\n",
       "  number: \"SINGULAR\"\n",
       "  gender: \"FEMALE\"\n",
       "  animacy: \"ANIMATE\"\n",
       "  beginIndex: 0\n",
       "  endIndex: 1\n",
       "  headIndex: 0\n",
       "  sentenceIndex: 1\n",
       "  position: 3\n",
       "}\n",
       "representative: 0\n",
       "]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_clusters(\"The developer was standing. She notified the nurse that the work was unsatisfactory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_count = 0\n",
    "# ITERS = 20\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(ITERS):\n",
    "#     oc1 = random.choice(occupations_1)\n",
    "#     oc2 = random.choice(occupations_2)\n",
    "#     verb = random.choice(list(verb_action.keys()))\n",
    "#     action = random.choice(random.choice(verb_action[verb]))\n",
    "#     in1 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     in2 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action) \n",
    "    \n",
    "#     in3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "#     pred1, _ = predict_clusters(in1)\n",
    "#     pred2, _ = predict_clusters(in2)\n",
    "#     pred3, _ = predict_clusters(in2)\n",
    "    \n",
    "#     if not (pred1 == pred2 and pred2 == pred3):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#             err_count += 1\n",
    "\n",
    "#             print(pred1, pred2, pred3)\n",
    "#             print(in1)\n",
    "#             print(in2)\n",
    "#             print(in3)\n",
    "#             print()\n",
    "    \n",
    "\n",
    "# print(err_count)\n",
    "# print(err_count/ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
