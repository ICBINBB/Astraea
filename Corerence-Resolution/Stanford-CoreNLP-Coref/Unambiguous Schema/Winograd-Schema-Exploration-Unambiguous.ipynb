{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CoreNLP Exploration Unambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 2.93MB/s]                    \n",
      "2020-07-21 15:05:41 INFO: Downloading default packages for language: en (English)...\n",
      "2020-07-21 15:05:42 INFO: File exists: /Users/sakshiudeshi/stanza_resources/en/default.zip.\n",
      "2020-07-21 15:05:45 INFO: Finished downloading models and saved to /Users/sakshiudeshi/stanza_resources.\n",
      "2020-07-21 15:05:45 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-07-21 15:05:45 INFO: Use device: cpu\n",
      "2020-07-21 15:05:45 INFO: Loading: tokenize\n",
      "2020-07-21 15:05:45 INFO: Loading: pos\n",
      "2020-07-21 15:05:46 INFO: Loading: lemma\n",
      "2020-07-21 15:05:46 INFO: Loading: depparse\n",
      "2020-07-21 15:05:47 INFO: Loading: ner\n",
      "2020-07-21 15:05:47 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "stanza.download('en') \n",
    "import sys, os\n",
    "\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "nlp = stanza.Pipeline()\n",
    "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-a36ccd5809b7413b.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CORENLP_HOME\"] = \"/Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0\"\n",
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "with CoreNLPClient(\n",
    "        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],\n",
    "        timeout=30000,\n",
    "        memory='16G') as client:\n",
    "    ann = client.annotate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-a36ccd5809b7413b.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n",
      "CEO\n",
      "[chainID: 5\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 5\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 1\n",
      "  position: 4\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = \"The CEO was sitting on the chair. The CEO notified the auditor that the application was under review.\"\n",
    "ann = client.annotate(text)\n",
    "# get the first sentence\n",
    "sentence = ann.sentence[0]\n",
    "# print(sentence)\n",
    "\n",
    "\n",
    "print(sentence.mentions[0].entityMentionText)\n",
    "print(print(ann.corefChain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked', 'disclosed to', 'told'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified', 'spoke with'] ##  pronoun2 = ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    return ['He', 'She', 'They']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"librarian\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"about \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "\n",
    "action1 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p1[1]] = [action1] ### disclosed to\n",
    "verb_action[verb_list_p1[2]] = [action1] ### told\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action1] ### notified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spoke with \n",
    "### asked\n",
    "action2 = ['painting', 'dancing',\n",
    "           'fencing', 'gymnastics',\n",
    "           'independent cinema', 'woodworking',\n",
    "           'studying', 'horse racing',\n",
    "           'singing', 'kayaking', 'football',\n",
    "           'baseball', 'basketball', 'quizzing', \n",
    "           'gardening', 'blogging', 'board games', \n",
    "           'breadmaking', 'baking']\n",
    "verb_action[verb_list_p2[1]] = [action2]\n",
    "verb_action[verb_list_p1[0]] = [action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_verb = ['standing', 'speaking', 'talking', \n",
    "                  'passing by', 'sitting', 'sitting on the chair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_clusters(sentence):\n",
    "    doc = client.annotate(sentence)\n",
    "#     print((doc.))\n",
    "    return(doc.corefChain)\n",
    "#     if doc._.has_coref: \n",
    "#         return (doc._.coref_resolved, doc._.coref_clusters)\n",
    "#     else:\n",
    "#         return ('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_list(pred_str):\n",
    "    pred_list = []\n",
    "    for line in pred_str.split('\\n'):\n",
    "        if 'gender' not in line:\n",
    "            pred_list.append(line)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_equivalence(pred1, pred2):\n",
    "    pred1_list = get_pred_list(str(pred1))\n",
    "    pred2_list = get_pred_list(str(pred2))\n",
    "    \n",
    "    return pred1_list == pred2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chainID: 3\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"UNKNOWN\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 1\n",
      "  headIndex: 0\n",
      "  sentenceIndex: 1\n",
      "  position: 3\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "--------------------------------\n",
      "[chainID: 1\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"UNKNOWN\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"FEMALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 1\n",
      "  headIndex: 0\n",
      "  sentenceIndex: 1\n",
      "  position: 3\n",
      "}\n",
      "mention {\n",
      "  mentionID: 1\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"UNKNOWN\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 2\n",
      "  endIndex: 4\n",
      "  headIndex: 3\n",
      "  sentenceIndex: 1\n",
      "  position: 1\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "--------------------------------\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## Example of an error \n",
    "# The technician was passing by. He notified the paralegal that the work was appreciated.\n",
    "pred1 = predict_clusters('The developer was standing. He asked the teacher about dancing.')\n",
    "print(pred1)\n",
    "print(\"--------------------------------\")\n",
    "pred2 = predict_clusters('The developer was standing. She asked the teacher about dancing.')\n",
    "# print(pred2)\n",
    "pred_2_str = (str(pred2))\n",
    "for line in pred_2_str.split('\\n'):\n",
    "    print(line)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(get_pred_equivalence(pred1, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chainID: 3\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 1\n",
      "  headIndex: 0\n",
      "  sentenceIndex: 1\n",
      "  position: 3\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "--------------------------------\n",
      "[chainID: 1\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"FEMALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 1\n",
      "  headIndex: 0\n",
      "  sentenceIndex: 1\n",
      "  position: 3\n",
      "}\n",
      "mention {\n",
      "  mentionID: 1\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"UNKNOWN\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 2\n",
      "  endIndex: 4\n",
      "  headIndex: 3\n",
      "  sentenceIndex: 1\n",
      "  position: 1\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "--------------------------------\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## Example of an error \n",
    "# The technician was passing by. He notified the paralegal that the work was appreciated.\n",
    "pred1 = predict_clusters('The manager was speaking. He asked the teacher about baking.')\n",
    "print(pred1)\n",
    "print(\"--------------------------------\")\n",
    "pred2 = predict_clusters('The manager was speaking. She asked the teacher about baking.')\n",
    "# print(pred2)\n",
    "pred_2_str = (str(pred2))\n",
    "for line in pred_2_str.split('\\n'):\n",
    "    print(line)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(get_pred_equivalence(pred1, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(oc1, oc2, verb, action, pronoun):\n",
    "\n",
    "\n",
    "    aux_verb = random.choice(auxiliary_verb)\n",
    "    input1 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[0] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "\n",
    "    input2 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[1] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "        \n",
    "    return input1, input2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "occupation_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "occupation_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n",
    "\n",
    "unique_input1_non_error_set = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 0\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 30\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 60\n",
      "------------------------------\n",
      "Unique errors: 1\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 1\n",
      "Unique inputs: 120\n",
      "------------------------------\n",
      "Unique errors: 2\n",
      "Unique inputs: 150\n",
      "------------------------------\n",
      "Unique errors: 2\n",
      "Unique inputs: 180\n",
      "------------------------------\n",
      "Unique errors: 2\n",
      "Unique inputs: 209\n",
      "------------------------------\n",
      "Unique errors: 2\n",
      "Unique inputs: 239\n",
      "------------------------------\n",
      "Unique errors: 2\n",
      "Unique inputs: 269\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 299\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 329\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 359\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 389\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 419\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 449\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 479\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 509\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 539\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 569\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 599\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 629\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 659\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 689\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 719\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 749\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 779\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 809\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 839\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 869\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 899\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 929\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 959\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 989\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1019\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1049\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1079\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1109\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1139\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1169\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1199\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1229\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1259\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1289\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1319\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 1349\n",
      "------------------------------\n",
      "Unique errors: 5\n",
      "Unique inputs: 1379\n",
      "------------------------------\n",
      "Unique errors: 5\n",
      "Unique inputs: 1408\n",
      "------------------------------\n",
      "Unique errors: 5\n",
      "Unique inputs: 1438\n",
      "------------------------------\n",
      "Unique errors: 5\n",
      "Unique inputs: 1467\n",
      "------------------------------\n",
      "Unique errors: 5\n",
      "Unique inputs: 1496\n",
      "------------------------------\n",
      "Unique errors: 5\n",
      "Unique inputs: 1526\n",
      "------------------------------\n",
      "Unique errors: 6\n",
      "Unique inputs: 1556\n",
      "------------------------------\n",
      "Unique errors: 7\n",
      "Unique inputs: 1586\n",
      "------------------------------\n",
      "Unique errors: 8\n",
      "Unique inputs: 1615\n",
      "------------------------------\n",
      "Unique errors: 8\n",
      "Unique inputs: 1645\n",
      "------------------------------\n",
      "Unique errors: 8\n",
      "Unique inputs: 1674\n",
      "------------------------------\n",
      "Unique errors: 8\n",
      "Unique inputs: 1704\n",
      "------------------------------\n",
      "Unique errors: 9\n",
      "Unique inputs: 1734\n",
      "------------------------------\n",
      "Unique errors: 9\n",
      "Unique inputs: 1763\n",
      "------------------------------\n",
      "Unique errors: 9\n",
      "Unique inputs: 1793\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 1823\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 1852\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 1881\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 1911\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 1941\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 1971\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2001\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2031\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2061\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2091\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2120\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2150\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2180\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2209\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2239\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2268\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 2298\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2328\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2358\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2387\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2417\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2447\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2477\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2507\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2537\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2567\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2597\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2627\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2656\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2686\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2716\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 2745\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 2775\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 2805\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 2835\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 2865\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 2895\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 2925\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 2955\n",
      "------------------------------\n",
      "15\n",
      "0.005\n",
      "Final Unique errors: 15\n",
      "Final Unique inputs: 2984\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 3000\n",
    "\n",
    "RELAXED_ERROR = True\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    oc1 = random.choice(occupations_1)\n",
    "    oc2 = random.choice(occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    \n",
    "    input1, input2 = generate_sentences(oc1, oc2, verb, action, pronoun)\n",
    "    \n",
    "    pred1 = predict_clusters(input1)\n",
    "    pred2 = predict_clusters(input2)\n",
    "#     pred3, _ = predict_clusters(input2)\n",
    "    \n",
    "    \n",
    "    if(i % 30 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add(input1)\n",
    "    \n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    \n",
    "#     print(pred1, pred2)\n",
    "#     print(input1)\n",
    "#     print(input2)\n",
    "#     print()\n",
    "\n",
    "                \n",
    "\n",
    "    \n",
    "\n",
    "    if not (get_pred_equivalence(pred1, pred2)):\n",
    "#         if ((len(pred1) > 0 and len(pred2) > 0)):\n",
    "#             if ((len(pred1[0]) == len(pred2[0]))):\n",
    "                err_count += 1\n",
    "                \n",
    "                unique_input1_error_set.add(input1)\n",
    "                \n",
    "#                 print(pred1, pred2)\n",
    "#                 print()\n",
    "#                 print(input1)\n",
    "#                 print(input2)\n",
    "#                 print(\"---------------------------\")\n",
    "#                 print(input3)\n",
    "                \n",
    "                update_dict(occupation_pair_error, (oc1, oc2))\n",
    "                update_dict(occupation1_error, oc1)\n",
    "                update_dict(occupation2_error, oc2)\n",
    "                update_dict(verb_error, verb)\n",
    "                update_dict(action_error, action)\n",
    "    else:\n",
    "        unique_input1_non_error_set.add(input1)\n",
    "#         print(pred1, pred2)\n",
    "#         print()\n",
    "#         print(input1)\n",
    "#         print(input2)\n",
    "#         print(\"---------------------------\")\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The construction worker was speaking. He notified the secretary that the application was approved.', 'The carpenter was sitting. He asked the teacher about independent cinema.', 'The manager was sitting. He notified the receptionist that the application was not approved.', 'The supervisor was sitting. He spoke with the baker about football.', 'The driver was talking. He spoke with the librarian about quizzing.']\n",
      "\n",
      "['The manager was speaking. He asked the teacher about baking.', 'The technician was passing by. He notified the paralegal that the work was appreciated.', 'The technician was passing by. He disclosed to the housekeeper that the game was excellent.', 'The technician was passing by. He disclosed to the assistant that the game was excellent.', 'The developer was standing. He asked the teacher about dancing.', 'The driver was speaking. He asked the teacher about baking.', 'The technician was passing by. He asked the housekeeper about painting.', 'The technician was passing by. He told the clerk that the performance was not up to the mark.', 'The technician was passing by. He told the cleaner that the meat was eaten.', 'The technician was passing by. He disclosed to the secretary that the delivery was made.']\n",
      "\n",
      "['The construction worker was speaking. He notified the secretary that the application was approved.', 'The carpenter was sitting. He asked the teacher about independent cinema.', 'The manager was sitting. He notified the receptionist that the application was not approved.', 'The supervisor was sitting. He spoke with the baker about football.', 'The driver was talking. He spoke with the librarian about quizzing.', 'The technician was sitting. He disclosed to the baker that the cookies were baked.', 'The mechanic was speaking. He asked the assistant about kayaking.', 'The farmer was sitting. He notified the baker that the performance was unsatisfactory.', 'The laborer was talking. He disclosed to the secretary that the cookies were baked.', 'The supervisor was speaking. He spoke with the accountant about quizzing.']\n"
     ]
    }
   ],
   "source": [
    "# print(occupation_pair_count)\n",
    "# print(occupation1_count)\n",
    "# print(occupation2_count)\n",
    "# print(verb_count)\n",
    "# print(action_count)\n",
    "print(list(unique_input1_set)[0:5])\n",
    "print()\n",
    "print(list(unique_input1_error_set)[0:10])\n",
    "print()\n",
    "print(list(unique_input1_non_error_set)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('saved_pickles/Exploration/unique_input1_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_error_set, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_count, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_error, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 if((oc1, oc2) in occupation_pair_error.keys()):\n",
    "#                     occupation_pair_error[(oc1, oc2)] += 1\n",
    "#                 else:\n",
    "#                     occupation_pair_error[(oc1, oc2)] = 1\n",
    "                                          \n",
    "#                 if(oc1 in occupation1_error.keys()):\n",
    "#                     occupation1_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation1_error[oc1] = 1\n",
    "                \n",
    "#                 if(oc2 in occupation2_error.keys()):\n",
    "#                     occupation2_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation2_error[oc1] = 1\n",
    "                                          \n",
    "#                 if(verb in verb_error.keys()):\n",
    "#                     verb_error[verb] += 1\n",
    "#                 else:\n",
    "#                     verb_error[verb] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_clusters(sentence):\n",
    "#     output = predictor.predict(document = sentence)\n",
    "#     return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " {'top_spans': [[0, 1], [5, 5], [6, 6], [7, 8], [10, 11], [13, 13]],\n",
       "  'antecedent_indices': [[0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5]],\n",
       "  'predicted_antecedents': [-1, -1, -1, -1, -1, -1],\n",
       "  'document': ['The',\n",
       "   'developer',\n",
       "   'was',\n",
       "   'standing',\n",
       "   '.',\n",
       "   'She',\n",
       "   'notified',\n",
       "   'the',\n",
       "   'nurse',\n",
       "   'that',\n",
       "   'the',\n",
       "   'work',\n",
       "   'was',\n",
       "   'unsatisfactory',\n",
       "   '.'],\n",
       "  'clusters': []})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_clusters(\"The developer was standing. She notified the nurse that the work was unsatisfactory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_count = 0\n",
    "# ITERS = 20\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(ITERS):\n",
    "#     oc1 = random.choice(occupations_1)\n",
    "#     oc2 = random.choice(occupations_2)\n",
    "#     verb = random.choice(list(verb_action.keys()))\n",
    "#     action = random.choice(random.choice(verb_action[verb]))\n",
    "#     in1 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     in2 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action) \n",
    "    \n",
    "#     in3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "#     pred1, _ = predict_clusters(in1)\n",
    "#     pred2, _ = predict_clusters(in2)\n",
    "#     pred3, _ = predict_clusters(in2)\n",
    "    \n",
    "#     if not (pred1 == pred2 and pred2 == pred3):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#             err_count += 1\n",
    "\n",
    "#             print(pred1, pred2, pred3)\n",
    "#             print(in1)\n",
    "#             print(in2)\n",
    "#             print(in3)\n",
    "#             print()\n",
    "    \n",
    "\n",
    "# print(err_count)\n",
    "# print(err_count/ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
