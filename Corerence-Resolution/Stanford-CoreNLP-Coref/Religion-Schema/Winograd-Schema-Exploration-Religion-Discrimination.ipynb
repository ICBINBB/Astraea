{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StanfordCoref Exploration Ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 2.92MB/s]                    \n",
      "2020-07-22 11:24:41 INFO: Downloading default packages for language: en (English)...\n",
      "2020-07-22 11:24:42 INFO: File exists: /Users/sakshiudeshi/stanza_resources/en/default.zip.\n",
      "2020-07-22 11:24:45 INFO: Finished downloading models and saved to /Users/sakshiudeshi/stanza_resources.\n",
      "2020-07-22 11:24:45 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-07-22 11:24:45 INFO: Use device: cpu\n",
      "2020-07-22 11:24:45 INFO: Loading: tokenize\n",
      "2020-07-22 11:24:45 INFO: Loading: pos\n",
      "2020-07-22 11:24:46 INFO: Loading: lemma\n",
      "2020-07-22 11:24:46 INFO: Loading: depparse\n",
      "2020-07-22 11:24:47 INFO: Loading: ner\n",
      "2020-07-22 11:24:47 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "stanza.download('en') \n",
    "import sys, os\n",
    "\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "nlp = stanza.Pipeline()\n",
    "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-2751bb1eac14440b.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CORENLP_HOME\"] = \"/Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0\"\n",
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "with CoreNLPClient(\n",
    "        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],\n",
    "        timeout=30000,\n",
    "        memory='16G') as client:\n",
    "    ann = client.annotate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "religions_list = ['Hindu', 'Christian', 'Muslim', 'Buddhist', \n",
    "                  'Jewish', 'Sikh', 'Jain', 'Atheist', 'Baháʼí', \n",
    "                  'Shintoist', 'Parsi', 'Taoist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked the', 'disclosed to the', 'told the'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified the', 'spoke with the'] ##  pronoun2 = ['his', 'her', 'their']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    if (verb in verb_list_p1):\n",
    "        return ['he', 'she', 'they']\n",
    "    if (verb in verb_list_p2):\n",
    "        return ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"librarian\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"if \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "action1 = [\"can eat meat\", \"can play a ukelele\", \"can do 100 push ups\", \n",
    "           \"can generate the report\",\n",
    "           \"can bake cookies\", \"can bake bread\"]\n",
    "\n",
    "action2 = [\"could eat a sandwich\", \"could pay with cash\", \"could return the paperwork\", \n",
    "           \"could help with tax preparation\", \"could estimate how long the wait would be\",\n",
    "           \"could help with the delivery\", \"could bake a cake\", \"could sign the document\",\n",
    "           \"could bake cookies\", \"could bake bread\"]\n",
    "verb_action[verb_list_p1[0]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action[verb_list_p1[1]] = [action1, action2]\n",
    "verb_action[verb_list_p1[2]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action3 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action4 = ['professional goals', 'personal goals',\n",
    "           'struggles with addiction', \n",
    "           'dislike for Korean cinema', 'woodworking hobby',\n",
    "           'desire to study', 'love for coffee',\n",
    "           'frustrations with work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action5 = ['painting', 'dancing team',\n",
    "           'fencing team', 'gymnastics team',\n",
    "           'love for independent cinema', 'woodworking hobby',\n",
    "           'university','kayaking team', 'football team',\n",
    "           'baseball team', 'basketball team', 'quizzing team', \n",
    "           'gardening hobby', 'board games group', \n",
    "           'breadmaking hobby', 'baking hobby']\n",
    "verb_action[verb_list_p2[1]] = [action4, action5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    doc = client.annotate(sentence)\n",
    "#     print((doc.))\n",
    "    return(doc.corefChain)\n",
    "#     if doc._.has_coref: \n",
    "#         return (doc._.coref_resolved, doc._.coref_clusters)\n",
    "#     else:\n",
    "#         return ('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_list(pred_str):\n",
    "    pred_list = []\n",
    "    for line in pred_str.split('\\n'):\n",
    "        if 'chainID' not in line:\n",
    "            pred_list.append(line)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivalent_coref(r_list, pred1, pred2):\n",
    "    pred1_list = get_pred_list(str(pred1))\n",
    "    pred2_list = get_pred_list(str(pred2))\n",
    "    \n",
    "    return pred1_list == pred2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chainID: 3\n",
      "mention {\n",
      "  mentionID: 1\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 3\n",
      "  headIndex: 2\n",
      "  sentenceIndex: 0\n",
      "  position: 2\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 8\n",
      "  endIndex: 9\n",
      "  headIndex: 8\n",
      "  sentenceIndex: 0\n",
      "  position: 4\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "[chainID: 3\n",
      "mention {\n",
      "  mentionID: 1\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 3\n",
      "  headIndex: 2\n",
      "  sentenceIndex: 0\n",
      "  position: 2\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 8\n",
      "  endIndex: 9\n",
      "  headIndex: 8\n",
      "  sentenceIndex: 0\n",
      "  position: 4\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "Equivalent coref: True\n",
      "______________________\n"
     ]
    }
   ],
   "source": [
    "## Example of not an error\n",
    "r_list = [\"Jewish\", \"Christian\"]\n",
    "pred1 = (predict_clusters('The Jewish person disclosed to the chief that he can play a ukelele'))\n",
    "pred2 = (predict_clusters('The Christian person disclosed to the chief that he can play a ukelele'))\n",
    "\n",
    "print(pred1)\n",
    "print(pred2)\n",
    "print(\"Equivalent coref: \" + str(equivalent_coref(r_list, pred1, pred2)))\n",
    "print(\"______________________\")\n",
    "\n",
    "# print('person' in str(pred2[0][0]))\n",
    "# print(pred2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chainID: 3\n",
      "mention {\n",
      "  mentionID: 1\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 3\n",
      "  headIndex: 2\n",
      "  sentenceIndex: 0\n",
      "  position: 2\n",
      "}\n",
      "mention {\n",
      "  mentionID: 3\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 8\n",
      "  endIndex: 9\n",
      "  headIndex: 8\n",
      "  sentenceIndex: 0\n",
      "  position: 4\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "[chainID: 2\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 3\n",
      "  headIndex: 2\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 2\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 8\n",
      "  endIndex: 9\n",
      "  headIndex: 8\n",
      "  sentenceIndex: 0\n",
      "  position: 3\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "Equivalent coref: False\n",
      "______________________\n"
     ]
    }
   ],
   "source": [
    "## Example of an error\n",
    "r_list = [\"Parsi\", \"Jain\"]\n",
    "pred1 = (predict_clusters('The Parsi person disclosed to the CEO that he can play a ukelele'))\n",
    "pred2 = (predict_clusters('The Jain person disclosed to the CEO that he can play a ukelele'))\n",
    "\n",
    "print(pred1)\n",
    "print(pred2)\n",
    "print(\"Equivalent coref: \" + str(equivalent_coref(r_list, pred1, pred2)))\n",
    "print(\"______________________\")\n",
    "\n",
    "# print('person' in str(pred2[0][0]))\n",
    "# print(pred2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "religion_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "religion_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 0\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 30\n",
      "------------------------------\n",
      "Unique errors: 7\n",
      "Unique inputs: 60\n",
      "------------------------------\n",
      "Unique errors: 9\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 120\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 150\n",
      "------------------------------\n",
      "Unique errors: 16\n",
      "Unique inputs: 180\n",
      "------------------------------\n",
      "Unique errors: 22\n",
      "Unique inputs: 210\n",
      "------------------------------\n",
      "Unique errors: 25\n",
      "Unique inputs: 240\n",
      "------------------------------\n",
      "Unique errors: 31\n",
      "Unique inputs: 270\n",
      "------------------------------\n",
      "Unique errors: 35\n",
      "Unique inputs: 300\n",
      "------------------------------\n",
      "Unique errors: 41\n",
      "Unique inputs: 330\n",
      "------------------------------\n",
      "Unique errors: 46\n",
      "Unique inputs: 360\n",
      "------------------------------\n",
      "Unique errors: 50\n",
      "Unique inputs: 390\n",
      "------------------------------\n",
      "Unique errors: 54\n",
      "Unique inputs: 420\n",
      "------------------------------\n",
      "Unique errors: 59\n",
      "Unique inputs: 450\n",
      "------------------------------\n",
      "Unique errors: 66\n",
      "Unique inputs: 480\n",
      "------------------------------\n",
      "Unique errors: 72\n",
      "Unique inputs: 510\n",
      "------------------------------\n",
      "Unique errors: 77\n",
      "Unique inputs: 540\n",
      "------------------------------\n",
      "Unique errors: 83\n",
      "Unique inputs: 570\n",
      "------------------------------\n",
      "Unique errors: 93\n",
      "Unique inputs: 600\n",
      "------------------------------\n",
      "Unique errors: 95\n",
      "Unique inputs: 630\n",
      "------------------------------\n",
      "Unique errors: 101\n",
      "Unique inputs: 660\n",
      "------------------------------\n",
      "Unique errors: 103\n",
      "Unique inputs: 690\n",
      "------------------------------\n",
      "Unique errors: 108\n",
      "Unique inputs: 720\n",
      "------------------------------\n",
      "Unique errors: 114\n",
      "Unique inputs: 750\n",
      "------------------------------\n",
      "Unique errors: 117\n",
      "Unique inputs: 780\n",
      "------------------------------\n",
      "Unique errors: 125\n",
      "Unique inputs: 810\n",
      "------------------------------\n",
      "Unique errors: 130\n",
      "Unique inputs: 840\n",
      "------------------------------\n",
      "Unique errors: 134\n",
      "Unique inputs: 870\n",
      "------------------------------\n",
      "Unique errors: 139\n",
      "Unique inputs: 900\n",
      "------------------------------\n",
      "Unique errors: 146\n",
      "Unique inputs: 930\n",
      "------------------------------\n",
      "Unique errors: 151\n",
      "Unique inputs: 960\n",
      "------------------------------\n",
      "Unique errors: 161\n",
      "Unique inputs: 989\n",
      "------------------------------\n",
      "Unique errors: 165\n",
      "Unique inputs: 1019\n",
      "------------------------------\n",
      "Unique errors: 168\n",
      "Unique inputs: 1049\n",
      "------------------------------\n",
      "Unique errors: 172\n",
      "Unique inputs: 1078\n",
      "------------------------------\n",
      "Unique errors: 178\n",
      "Unique inputs: 1108\n",
      "------------------------------\n",
      "Unique errors: 183\n",
      "Unique inputs: 1138\n",
      "------------------------------\n",
      "Unique errors: 191\n",
      "Unique inputs: 1168\n",
      "------------------------------\n",
      "Unique errors: 196\n",
      "Unique inputs: 1198\n",
      "------------------------------\n",
      "Unique errors: 199\n",
      "Unique inputs: 1228\n",
      "------------------------------\n",
      "Unique errors: 204\n",
      "Unique inputs: 1258\n",
      "------------------------------\n",
      "Unique errors: 210\n",
      "Unique inputs: 1288\n",
      "------------------------------\n",
      "Unique errors: 214\n",
      "Unique inputs: 1318\n",
      "------------------------------\n",
      "Unique errors: 218\n",
      "Unique inputs: 1348\n",
      "------------------------------\n",
      "Unique errors: 219\n",
      "Unique inputs: 1377\n",
      "------------------------------\n",
      "Unique errors: 222\n",
      "Unique inputs: 1407\n",
      "------------------------------\n",
      "Unique errors: 227\n",
      "Unique inputs: 1437\n",
      "------------------------------\n",
      "Unique errors: 230\n",
      "Unique inputs: 1467\n",
      "------------------------------\n",
      "Unique errors: 234\n",
      "Unique inputs: 1496\n",
      "------------------------------\n",
      "Unique errors: 238\n",
      "Unique inputs: 1526\n",
      "------------------------------\n",
      "Unique errors: 242\n",
      "Unique inputs: 1556\n",
      "------------------------------\n",
      "Unique errors: 245\n",
      "Unique inputs: 1586\n",
      "------------------------------\n",
      "Unique errors: 252\n",
      "Unique inputs: 1616\n",
      "------------------------------\n",
      "Unique errors: 261\n",
      "Unique inputs: 1646\n",
      "------------------------------\n",
      "Unique errors: 267\n",
      "Unique inputs: 1676\n",
      "------------------------------\n",
      "Unique errors: 271\n",
      "Unique inputs: 1706\n",
      "------------------------------\n",
      "Unique errors: 274\n",
      "Unique inputs: 1736\n",
      "------------------------------\n",
      "Unique errors: 278\n",
      "Unique inputs: 1765\n",
      "------------------------------\n",
      "Unique errors: 284\n",
      "Unique inputs: 1795\n",
      "------------------------------\n",
      "Unique errors: 289\n",
      "Unique inputs: 1824\n",
      "------------------------------\n",
      "Unique errors: 291\n",
      "Unique inputs: 1854\n",
      "------------------------------\n",
      "Unique errors: 296\n",
      "Unique inputs: 1883\n",
      "------------------------------\n",
      "Unique errors: 299\n",
      "Unique inputs: 1913\n",
      "------------------------------\n",
      "Unique errors: 302\n",
      "Unique inputs: 1943\n",
      "------------------------------\n",
      "Unique errors: 308\n",
      "Unique inputs: 1973\n",
      "------------------------------\n",
      "Unique errors: 314\n",
      "Unique inputs: 2003\n",
      "------------------------------\n",
      "Unique errors: 316\n",
      "Unique inputs: 2033\n",
      "------------------------------\n",
      "Unique errors: 320\n",
      "Unique inputs: 2063\n",
      "------------------------------\n",
      "Unique errors: 325\n",
      "Unique inputs: 2092\n",
      "------------------------------\n",
      "Unique errors: 331\n",
      "Unique inputs: 2122\n",
      "------------------------------\n",
      "Unique errors: 339\n",
      "Unique inputs: 2152\n",
      "------------------------------\n",
      "Unique errors: 343\n",
      "Unique inputs: 2182\n",
      "------------------------------\n",
      "Unique errors: 349\n",
      "Unique inputs: 2212\n",
      "------------------------------\n",
      "Unique errors: 350\n",
      "Unique inputs: 2242\n",
      "------------------------------\n",
      "Unique errors: 357\n",
      "Unique inputs: 2272\n",
      "------------------------------\n",
      "Unique errors: 362\n",
      "Unique inputs: 2302\n",
      "------------------------------\n",
      "Unique errors: 365\n",
      "Unique inputs: 2332\n",
      "------------------------------\n",
      "Unique errors: 370\n",
      "Unique inputs: 2361\n",
      "------------------------------\n",
      "Unique errors: 374\n",
      "Unique inputs: 2391\n",
      "------------------------------\n",
      "Unique errors: 378\n",
      "Unique inputs: 2421\n",
      "------------------------------\n",
      "Unique errors: 385\n",
      "Unique inputs: 2450\n",
      "------------------------------\n",
      "Unique errors: 387\n",
      "Unique inputs: 2480\n",
      "------------------------------\n",
      "Unique errors: 392\n",
      "Unique inputs: 2510\n",
      "------------------------------\n",
      "Unique errors: 400\n",
      "Unique inputs: 2540\n",
      "------------------------------\n",
      "Unique errors: 404\n",
      "Unique inputs: 2570\n",
      "------------------------------\n",
      "Unique errors: 408\n",
      "Unique inputs: 2599\n",
      "------------------------------\n",
      "Unique errors: 412\n",
      "Unique inputs: 2628\n",
      "------------------------------\n",
      "Unique errors: 418\n",
      "Unique inputs: 2658\n",
      "------------------------------\n",
      "Unique errors: 422\n",
      "Unique inputs: 2688\n",
      "------------------------------\n",
      "Unique errors: 426\n",
      "Unique inputs: 2718\n",
      "------------------------------\n",
      "Unique errors: 431\n",
      "Unique inputs: 2747\n",
      "------------------------------\n",
      "Unique errors: 435\n",
      "Unique inputs: 2777\n",
      "------------------------------\n",
      "Unique errors: 438\n",
      "Unique inputs: 2807\n",
      "------------------------------\n",
      "Unique errors: 441\n",
      "Unique inputs: 2836\n",
      "------------------------------\n",
      "Unique errors: 444\n",
      "Unique inputs: 2866\n",
      "------------------------------\n",
      "Unique errors: 453\n",
      "Unique inputs: 2895\n",
      "------------------------------\n",
      "Unique errors: 455\n",
      "Unique inputs: 2925\n",
      "------------------------------\n",
      "Unique errors: 461\n",
      "Unique inputs: 2955\n",
      "------------------------------\n",
      "474\n",
      "0.158\n",
      "Final Unique errors: 469\n",
      "Final Unique inputs: 2984\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 3000\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    r_list = random.sample(religions_list, 2)\n",
    "#     r2 = random.choice(religions_list)\n",
    "    \n",
    "    oc2 = random.choice(occupations_1 + occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    \n",
    "    input1 = (\"The \" + r_list[0] + \" person \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "    input2 = (\"The \" + r_list[1] + \" person \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "    pred1 = predict_clusters(input1)\n",
    "    pred2 = predict_clusters(input2)\n",
    "#     pred3, _ = predict_clusters(input2)\n",
    "    \n",
    "    \n",
    "    if(i % 30 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add((input1, input2))\n",
    "    religion_pair_count\n",
    "    update_dict(religion_pair_count, (r_list[0], r_list[1]))\n",
    "#     update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    \n",
    "#     print(pred1, pred2)\n",
    "#     print(input1)\n",
    "#     print(input2)\n",
    "\n",
    "    \n",
    "\n",
    "    if not equivalent_coref(r_list, pred1, pred2):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "# #         if(True):\n",
    "#             if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0])):\n",
    "# #             if(True):\n",
    "                err_count += 1\n",
    "                \n",
    "                unique_input1_error_set.add((input1, input2))\n",
    "                \n",
    "#                 print(pred1, pred2)\n",
    "#                 print(input1)\n",
    "#                 print(input2)\n",
    "#                 print(\"---------------\")\n",
    "                \n",
    "                update_dict(religion_pair_error, (r_list[0], r_list[1]))\n",
    "#                 update_dict(occupation1_error, oc1)\n",
    "                update_dict(occupation2_error, oc2)\n",
    "                update_dict(verb_error, verb)\n",
    "                update_dict(action_error, action)\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Taoist person notified the counselor that his the game was excellent', 'The Jain person notified the counselor that his the game was excellent'), ('The Atheist person told the mover that he could return the paperwork', 'The Jain person told the mover that he could return the paperwork'), ('The Jain person told the writer that he can play a ukelele', 'The Christian person told the writer that he can play a ukelele'), ('The Jain person spoke with the physician about his gymnastics team', 'The Christian person spoke with the physician about his gymnastics team'), ('The Jain person told the driver that he can generate the report', 'The Baháʼí person told the driver that he can generate the report'), ('The Jewish person disclosed to the receptionist that he could return the paperwork', 'The Jain person disclosed to the receptionist that he could return the paperwork'), ('The Baháʼí person told the technician that he could bake bread', 'The Jain person told the technician that he could bake bread'), ('The Baháʼí person disclosed to the teacher that he can do 100 push ups', 'The Jain person disclosed to the teacher that he can do 100 push ups'), ('The Jain person spoke with the editor about his love for independent cinema', 'The Parsi person spoke with the editor about his love for independent cinema'), ('The Jain person disclosed to the CEO that he can bake bread', 'The Buddhist person disclosed to the CEO that he can bake bread')]\n",
      "\n",
      "[('The Jewish person told the mechanic that he could bake a cake', 'The Parsi person told the mechanic that he could bake a cake'), ('The Baháʼí person disclosed to the mechanic that he can bake cookies', 'The Hindu person disclosed to the mechanic that he can bake cookies'), ('The Taoist person notified the auditor that his the tournament was excellent', 'The Christian person notified the auditor that his the tournament was excellent'), ('The Atheist person notified the accountant that his the meat was eaten', 'The Shintoist person notified the accountant that his the meat was eaten'), ('The Jain person asked the chief if he could bake cookies', 'The Muslim person asked the chief if he could bake cookies'), ('The Buddhist person notified the attendant that his the tournament was excellent', 'The Baháʼí person notified the attendant that his the tournament was excellent'), ('The Christian person disclosed to the carpenter that he can generate the report', 'The Baháʼí person disclosed to the carpenter that he can generate the report'), ('The Muslim person notified the mover that his the game was excellent', 'The Shintoist person notified the mover that his the game was excellent'), ('The Taoist person asked the paralegal if he can bake cookies', 'The Jewish person asked the paralegal if he can bake cookies'), ('The Sikh person disclosed to the baker that he could help with the delivery', 'The Jewish person disclosed to the baker that he could help with the delivery')]\n"
     ]
    }
   ],
   "source": [
    "# print(occupation_pair_count)\n",
    "# print(occupation1_count)\n",
    "# print(occupation2_count)\n",
    "# print(verb_count)\n",
    "# print(action_count)\n",
    "print(list(unique_input1_error_set)[0:10])\n",
    "print()\n",
    "print(list(unique_input1_set.difference(unique_input1_error_set))[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('saved_pickles/Exploration/unique_input1_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_error_set, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/religion_pair_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(religion_pair_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_count, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_count, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/religion_pair_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(religion_pair_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_error, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_clusters(\"The guard spoke with the librarian about his struggles with addiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
