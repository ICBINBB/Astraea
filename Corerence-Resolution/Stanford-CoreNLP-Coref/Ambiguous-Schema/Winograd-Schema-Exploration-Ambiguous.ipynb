{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CoreNLP Exploration Ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 3.24MB/s]                    \n",
      "2020-07-20 15:04:33 INFO: Downloading default packages for language: en (English)...\n",
      "2020-07-20 15:04:33 INFO: File exists: /Users/sakshiudeshi/stanza_resources/en/default.zip.\n",
      "2020-07-20 15:04:36 INFO: Finished downloading models and saved to /Users/sakshiudeshi/stanza_resources.\n",
      "2020-07-20 15:04:36 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-07-20 15:04:36 INFO: Use device: cpu\n",
      "2020-07-20 15:04:36 INFO: Loading: tokenize\n",
      "2020-07-20 15:04:36 INFO: Loading: pos\n",
      "2020-07-20 15:04:37 INFO: Loading: lemma\n",
      "2020-07-20 15:04:37 INFO: Loading: depparse\n",
      "2020-07-20 15:04:38 INFO: Loading: ner\n",
      "2020-07-20 15:04:39 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "stanza.download('en') \n",
    "import sys, os\n",
    "\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "nlp = stanza.Pipeline()\n",
    "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e4983010f2f348fc.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CORENLP_HOME\"] = \"/Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0\"\n",
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "with CoreNLPClient(\n",
    "        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],\n",
    "        timeout=30000,\n",
    "        memory='16G') as client:\n",
    "    ann = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked the', 'disclosed to the', 'told the'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified the', 'spoke with the'] ##  pronoun2 = ['his', 'her', 'their']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    if (verb in verb_list_p1):\n",
    "        return ['he', 'she', 'they']\n",
    "    if (verb in verb_list_p2):\n",
    "        return ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"librarian\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"if \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "action1 = [\"can eat meat\", \"can play a ukelele\", \"can do 100 push ups\", \n",
    "           \"can generate the report\",\n",
    "           \"can bake cookies\", \"can bake bread\"]\n",
    "\n",
    "action2 = [\"could eat a sandwich\", \"could pay with cash\", \"could return the paperwork\", \n",
    "           \"could help with tax preparation\", \"could estimate how long the wait would be\",\n",
    "           \"could help with the delivery\", \"could bake a cake\", \"could sign the document\",\n",
    "           \"could bake cookies\", \"could bake bread\"]\n",
    "verb_action[verb_list_p1[0]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action[verb_list_p1[1]] = [action1, action2]\n",
    "verb_action[verb_list_p1[2]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "action3 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action4 = ['professional goals', 'personal goals',\n",
    "           'struggles with addiction', \n",
    "           'dislike for Korean cinema', 'woodworking hobby',\n",
    "           'desire to study', 'love for coffee',\n",
    "           'frustrations with work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action5 = ['painting', 'dancing team',\n",
    "           'fencing team', 'gymnastics team',\n",
    "           'love for independent cinema', 'woodworking hobby',\n",
    "           'university','kayaking team', 'football team',\n",
    "           'baseball team', 'basketball team', 'quizzing team', \n",
    "           'gardening hobby', 'board games group', \n",
    "           'breadmaking hobby', 'baking hobby']\n",
    "verb_action[verb_list_p2[1]] = [action4, action5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    doc = client.annotate(sentence)\n",
    "#     print((doc.))\n",
    "    return(doc.corefChain)\n",
    "#     if doc._.has_coref: \n",
    "#         return (doc._.coref_resolved, doc._.coref_clusters)\n",
    "#     else:\n",
    "#         return ('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_list(pred_str):\n",
    "    pred_list = []\n",
    "    for line in pred_str.split('\\n'):\n",
    "        if 'gender' not in line:\n",
    "            pred_list.append(line)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_equivalence(pred1, pred2):\n",
    "    pred1_list = get_pred_list(str(pred1))\n",
    "    pred2_list = get_pred_list(str(pred2))\n",
    "#     print(pred1_list)\n",
    "#     print(pred2_list)\n",
    "    return pred1_list == pred2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "occupation_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "occupation_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chainID: 2\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 2\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 7\n",
      "  endIndex: 8\n",
      "  headIndex: 7\n",
      "  sentenceIndex: 0\n",
      "  position: 3\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "[]\n",
      "['[chainID: 2', 'mention {', '  mentionID: 0', '  mentionType: \"NOMINAL\"', '  number: \"SINGULAR\"', '  animacy: \"ANIMATE\"', '  beginIndex: 0', '  endIndex: 2', '  headIndex: 1', '  sentenceIndex: 0', '  position: 1', '}', 'mention {', '  mentionID: 2', '  mentionType: \"PRONOMINAL\"', '  number: \"SINGULAR\"', '  animacy: \"ANIMATE\"', '  beginIndex: 6', '  endIndex: 7', '  headIndex: 6', '  sentenceIndex: 0', '  position: 3', '}', 'representative: 0', ']']\n",
      "['[chainID: 2', 'mention {', '  mentionID: 0', '  mentionType: \"NOMINAL\"', '  number: \"SINGULAR\"', '  animacy: \"ANIMATE\"', '  beginIndex: 0', '  endIndex: 2', '  headIndex: 1', '  sentenceIndex: 0', '  position: 1', '}', 'mention {', '  mentionID: 2', '  mentionType: \"PRONOMINAL\"', '  number: \"SINGULAR\"', '  animacy: \"ANIMATE\"', '  beginIndex: 6', '  endIndex: 7', '  headIndex: 6', '  sentenceIndex: 0', '  position: 3', '}', 'representative: 0', ']']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Example of an error\n",
    "print(predict_clusters('The driver spoke with the baker about his painting'))\n",
    "print(predict_clusters('The driver spoke with the baker about her painting'))\n",
    "print(get_pred_equivalence(pred1, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Example of not an error\n",
    "# print(predict_clusters('The technician asked the accountant if he could help with tax preparation'))\n",
    "# print(predict_clusters('The technician asked the accountant if she could help with tax preparation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e4983010f2f348fc.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n",
      "Unique errors: 0\n",
      "Unique inputs: 0\n",
      "------------------------------\n",
      "Unique errors: 5\n",
      "Unique inputs: 30\n",
      "------------------------------\n",
      "Unique errors: 17\n",
      "Unique inputs: 60\n",
      "------------------------------\n",
      "Unique errors: 29\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 37\n",
      "Unique inputs: 120\n",
      "------------------------------\n",
      "Unique errors: 53\n",
      "Unique inputs: 150\n",
      "------------------------------\n",
      "Unique errors: 66\n",
      "Unique inputs: 180\n",
      "------------------------------\n",
      "Unique errors: 76\n",
      "Unique inputs: 210\n",
      "------------------------------\n",
      "Unique errors: 85\n",
      "Unique inputs: 240\n",
      "------------------------------\n",
      "Unique errors: 93\n",
      "Unique inputs: 270\n",
      "------------------------------\n",
      "Unique errors: 105\n",
      "Unique inputs: 300\n",
      "------------------------------\n",
      "Unique errors: 114\n",
      "Unique inputs: 330\n",
      "------------------------------\n",
      "Unique errors: 127\n",
      "Unique inputs: 360\n",
      "------------------------------\n",
      "Unique errors: 138\n",
      "Unique inputs: 390\n",
      "------------------------------\n",
      "Unique errors: 151\n",
      "Unique inputs: 420\n",
      "------------------------------\n",
      "Unique errors: 162\n",
      "Unique inputs: 450\n",
      "------------------------------\n",
      "Unique errors: 175\n",
      "Unique inputs: 480\n",
      "------------------------------\n",
      "Unique errors: 182\n",
      "Unique inputs: 510\n",
      "------------------------------\n",
      "Unique errors: 194\n",
      "Unique inputs: 540\n",
      "------------------------------\n",
      "Unique errors: 205\n",
      "Unique inputs: 570\n",
      "------------------------------\n",
      "Unique errors: 212\n",
      "Unique inputs: 600\n",
      "------------------------------\n",
      "Unique errors: 225\n",
      "Unique inputs: 630\n",
      "------------------------------\n",
      "Unique errors: 243\n",
      "Unique inputs: 660\n",
      "------------------------------\n",
      "Unique errors: 248\n",
      "Unique inputs: 690\n",
      "------------------------------\n",
      "Unique errors: 258\n",
      "Unique inputs: 718\n",
      "------------------------------\n",
      "Unique errors: 268\n",
      "Unique inputs: 747\n",
      "------------------------------\n",
      "Unique errors: 278\n",
      "Unique inputs: 777\n",
      "------------------------------\n",
      "Unique errors: 290\n",
      "Unique inputs: 807\n",
      "------------------------------\n",
      "Unique errors: 303\n",
      "Unique inputs: 837\n",
      "------------------------------\n",
      "Unique errors: 315\n",
      "Unique inputs: 867\n",
      "------------------------------\n",
      "Unique errors: 327\n",
      "Unique inputs: 897\n",
      "------------------------------\n",
      "Unique errors: 340\n",
      "Unique inputs: 925\n",
      "------------------------------\n",
      "Unique errors: 344\n",
      "Unique inputs: 953\n",
      "------------------------------\n",
      "Unique errors: 352\n",
      "Unique inputs: 982\n",
      "------------------------------\n",
      "Unique errors: 364\n",
      "Unique inputs: 1011\n",
      "------------------------------\n",
      "Unique errors: 376\n",
      "Unique inputs: 1041\n",
      "------------------------------\n",
      "Unique errors: 386\n",
      "Unique inputs: 1068\n",
      "------------------------------\n",
      "Unique errors: 393\n",
      "Unique inputs: 1097\n",
      "------------------------------\n",
      "Unique errors: 403\n",
      "Unique inputs: 1126\n",
      "------------------------------\n",
      "Unique errors: 412\n",
      "Unique inputs: 1155\n",
      "------------------------------\n",
      "Unique errors: 423\n",
      "Unique inputs: 1183\n",
      "------------------------------\n",
      "Unique errors: 437\n",
      "Unique inputs: 1212\n",
      "------------------------------\n",
      "Unique errors: 445\n",
      "Unique inputs: 1242\n",
      "------------------------------\n",
      "Unique errors: 458\n",
      "Unique inputs: 1272\n",
      "------------------------------\n",
      "Unique errors: 469\n",
      "Unique inputs: 1302\n",
      "------------------------------\n",
      "Unique errors: 480\n",
      "Unique inputs: 1331\n",
      "------------------------------\n",
      "Unique errors: 489\n",
      "Unique inputs: 1360\n",
      "------------------------------\n",
      "Unique errors: 495\n",
      "Unique inputs: 1387\n",
      "------------------------------\n",
      "Unique errors: 508\n",
      "Unique inputs: 1417\n",
      "------------------------------\n",
      "Unique errors: 522\n",
      "Unique inputs: 1446\n",
      "------------------------------\n",
      "Unique errors: 526\n",
      "Unique inputs: 1474\n",
      "------------------------------\n",
      "Unique errors: 536\n",
      "Unique inputs: 1501\n",
      "------------------------------\n",
      "Unique errors: 548\n",
      "Unique inputs: 1529\n",
      "------------------------------\n",
      "Unique errors: 563\n",
      "Unique inputs: 1557\n",
      "------------------------------\n",
      "Unique errors: 576\n",
      "Unique inputs: 1585\n",
      "------------------------------\n",
      "Unique errors: 583\n",
      "Unique inputs: 1615\n",
      "------------------------------\n",
      "Unique errors: 591\n",
      "Unique inputs: 1644\n",
      "------------------------------\n",
      "Unique errors: 602\n",
      "Unique inputs: 1674\n",
      "------------------------------\n",
      "Unique errors: 614\n",
      "Unique inputs: 1702\n",
      "------------------------------\n",
      "Unique errors: 623\n",
      "Unique inputs: 1732\n",
      "------------------------------\n",
      "Unique errors: 635\n",
      "Unique inputs: 1762\n",
      "------------------------------\n",
      "Unique errors: 645\n",
      "Unique inputs: 1792\n",
      "------------------------------\n",
      "Unique errors: 655\n",
      "Unique inputs: 1821\n",
      "------------------------------\n",
      "Unique errors: 663\n",
      "Unique inputs: 1849\n",
      "------------------------------\n",
      "Unique errors: 671\n",
      "Unique inputs: 1877\n",
      "------------------------------\n",
      "Unique errors: 682\n",
      "Unique inputs: 1906\n",
      "------------------------------\n",
      "Unique errors: 690\n",
      "Unique inputs: 1935\n",
      "------------------------------\n",
      "Unique errors: 701\n",
      "Unique inputs: 1962\n",
      "------------------------------\n",
      "Unique errors: 714\n",
      "Unique inputs: 1991\n",
      "------------------------------\n",
      "Unique errors: 724\n",
      "Unique inputs: 2019\n",
      "------------------------------\n",
      "Unique errors: 733\n",
      "Unique inputs: 2048\n",
      "------------------------------\n",
      "Unique errors: 742\n",
      "Unique inputs: 2076\n",
      "------------------------------\n",
      "Unique errors: 750\n",
      "Unique inputs: 2105\n",
      "------------------------------\n",
      "Unique errors: 763\n",
      "Unique inputs: 2134\n",
      "------------------------------\n",
      "Unique errors: 774\n",
      "Unique inputs: 2162\n",
      "------------------------------\n",
      "Unique errors: 787\n",
      "Unique inputs: 2192\n",
      "------------------------------\n",
      "Unique errors: 795\n",
      "Unique inputs: 2220\n",
      "------------------------------\n",
      "Unique errors: 806\n",
      "Unique inputs: 2249\n",
      "------------------------------\n",
      "Unique errors: 824\n",
      "Unique inputs: 2276\n",
      "------------------------------\n",
      "Unique errors: 833\n",
      "Unique inputs: 2304\n",
      "------------------------------\n",
      "Unique errors: 839\n",
      "Unique inputs: 2332\n",
      "------------------------------\n",
      "Unique errors: 852\n",
      "Unique inputs: 2360\n",
      "------------------------------\n",
      "Unique errors: 864\n",
      "Unique inputs: 2388\n",
      "------------------------------\n",
      "Unique errors: 873\n",
      "Unique inputs: 2416\n",
      "------------------------------\n",
      "Unique errors: 886\n",
      "Unique inputs: 2445\n",
      "------------------------------\n",
      "Unique errors: 895\n",
      "Unique inputs: 2475\n",
      "------------------------------\n",
      "Unique errors: 906\n",
      "Unique inputs: 2504\n",
      "------------------------------\n",
      "Unique errors: 916\n",
      "Unique inputs: 2533\n",
      "------------------------------\n",
      "Unique errors: 926\n",
      "Unique inputs: 2561\n",
      "------------------------------\n",
      "Unique errors: 932\n",
      "Unique inputs: 2590\n",
      "------------------------------\n",
      "Unique errors: 941\n",
      "Unique inputs: 2618\n",
      "------------------------------\n",
      "Unique errors: 954\n",
      "Unique inputs: 2648\n",
      "------------------------------\n",
      "Unique errors: 968\n",
      "Unique inputs: 2677\n",
      "------------------------------\n",
      "Unique errors: 979\n",
      "Unique inputs: 2707\n",
      "------------------------------\n",
      "Unique errors: 987\n",
      "Unique inputs: 2732\n",
      "------------------------------\n",
      "Unique errors: 996\n",
      "Unique inputs: 2761\n",
      "------------------------------\n",
      "Unique errors: 1004\n",
      "Unique inputs: 2787\n",
      "------------------------------\n",
      "Unique errors: 1014\n",
      "Unique inputs: 2817\n",
      "------------------------------\n",
      "Unique errors: 1023\n",
      "Unique inputs: 2846\n",
      "------------------------------\n",
      "Unique errors: 1033\n",
      "Unique inputs: 2874\n",
      "------------------------------\n",
      "1074\n",
      "0.358\n",
      "Final Unique errors: 1044\n",
      "Final Unique inputs: 2903\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 3000\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    oc1 = random.choice(occupations_1)\n",
    "    oc2 = random.choice(occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    \n",
    "    input1 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "    input2 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action) \n",
    "    \n",
    "#     input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "    pred1 = predict_clusters(input1)\n",
    "    pred2 = predict_clusters(input2)\n",
    "#     pred3, _ = predict_clusters(input2)\n",
    "    \n",
    "    \n",
    "    if(i % 30 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add(input1)\n",
    "    \n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    \n",
    "#     print(pred1, pred2)\n",
    "#     print(input1)\n",
    "#     print(input2)\n",
    "\n",
    "    \n",
    "\n",
    "    if not (get_pred_equivalence(pred1, pred2)):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "# #         if(True):\n",
    "#             if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0])):\n",
    "# #             if(True):\n",
    "                err_count += 1\n",
    "                \n",
    "                unique_input1_error_set.add(input1)\n",
    "                \n",
    "#                 print(pred1, pred2, pred3)\n",
    "#                 print(input1)\n",
    "#                 print(input2)\n",
    "#                 print(input3)\n",
    "                \n",
    "                update_dict(occupation_pair_error, (oc1, oc2))\n",
    "                update_dict(occupation1_error, oc1)\n",
    "                update_dict(occupation2_error, oc2)\n",
    "                update_dict(verb_error, verb)\n",
    "                update_dict(action_error, action)\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The carpenter spoke with the clerk about his university', 'The CEO asked the paralegal if he can eat meat', 'The CEO spoke with the cashier about his kayaking team', 'The accountant notified the teacher that his the cookies were baked', 'The janitor notified the auditor that his the delivery was made', 'The driver notified the editor that his the project scope had been finalised', 'The janitor notified the librarian that his the delivery was made', 'The lawyer spoke with the secretary about his university', 'The laborer spoke with the nurse about his painting', 'The construction worker told the auditor that he can bake cookies']\n",
      "\n",
      "['The carpenter disclosed to the hairdresser that he can generate the report', 'The salesperson spoke with the clerk about his frustrations with work', 'The supervisor told the librarian that he can bake cookies', 'The mover spoke with the librarian about his frustrations with work', 'The CEO asked the teacher if he could return the paperwork', 'The lawyer told the librarian that he can bake cookies', 'The sheriff disclosed to the baker that he can do 100 push ups', 'The driver told the cashier that he could return the paperwork', 'The laborer told the writer that he could pay with cash', 'The accountant asked the housekeeper if he could bake a cake']\n"
     ]
    }
   ],
   "source": [
    "# print(occupation_pair_count)\n",
    "# print(occupation1_count)\n",
    "# print(occupation2_count)\n",
    "# print(verb_count)\n",
    "# print(action_count)\n",
    "print(list(unique_input1_error_set)[0:10])\n",
    "print()\n",
    "print(list(unique_input1_set)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('saved_pickles/Exploration/unique_input1_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_error_set, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_count, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_count, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_error, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_clusters(\"The guard spoke with the librarian about his struggles with addiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
