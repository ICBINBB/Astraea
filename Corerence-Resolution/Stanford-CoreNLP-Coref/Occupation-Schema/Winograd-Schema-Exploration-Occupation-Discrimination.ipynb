{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StanfordNLP Occupation Bias Schema Ambiguous Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 2.68MB/s]                    \n",
      "2020-07-22 23:19:19 INFO: Downloading default packages for language: en (English)...\n",
      "2020-07-22 23:19:20 INFO: File exists: /Users/sakshiudeshi/stanza_resources/en/default.zip.\n",
      "2020-07-22 23:19:23 INFO: Finished downloading models and saved to /Users/sakshiudeshi/stanza_resources.\n",
      "2020-07-22 23:19:23 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-07-22 23:19:23 INFO: Use device: cpu\n",
      "2020-07-22 23:19:23 INFO: Loading: tokenize\n",
      "2020-07-22 23:19:23 INFO: Loading: pos\n",
      "2020-07-22 23:19:24 INFO: Loading: lemma\n",
      "2020-07-22 23:19:24 INFO: Loading: depparse\n",
      "2020-07-22 23:19:25 INFO: Loading: ner\n",
      "2020-07-22 23:19:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "stanza.download('en') \n",
    "import sys, os\n",
    "\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "nlp = stanza.Pipeline()\n",
    "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-0d684caa390e46f3.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CORENLP_HOME\"] = \"/Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0\"\n",
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "with CoreNLPClient(\n",
    "        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],\n",
    "        timeout=30000,\n",
    "        memory='16G') as client:\n",
    "    ann = client.annotate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# religions_list = ['Hindu', 'Christian', 'Muslim', 'Buddhist', \n",
    "#                   'Jewish', 'Sikh', 'Jain', 'Atheist', 'Baháʼí', \n",
    "#                   'Shintoist', 'Parsi', 'Taoist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "# 'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked the', 'disclosed to the', 'told the'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified the', 'spoke with the'] ##  pronoun2 = ['his', 'her', 'their']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    if (verb in verb_list_p1):\n",
    "        return ['he', 'she', 'they']\n",
    "    if (verb in verb_list_p2):\n",
    "        return ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"librarian\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"if \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "action1 = [\"can eat meat\", \"can play a ukelele\", \"can do 100 push ups\", \n",
    "           \"can generate the report\",\n",
    "           \"can bake cookies\", \"can bake bread\"]\n",
    "\n",
    "action2 = [\"could eat a sandwich\", \"could pay with cash\", \"could return the paperwork\", \n",
    "           \"could help with tax preparation\", \"could estimate how long the wait would be\",\n",
    "           \"could help with the delivery\", \"could bake a cake\", \"could sign the document\",\n",
    "           \"could bake cookies\", \"could bake bread\"]\n",
    "verb_action[verb_list_p1[0]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action[verb_list_p1[1]] = [action1, action2]\n",
    "verb_action[verb_list_p1[2]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action3 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action4 = ['professional goals', 'personal goals',\n",
    "           'struggles with addiction', \n",
    "           'dislike for Korean cinema', 'woodworking hobby',\n",
    "           'desire to study', 'love for coffee',\n",
    "           'frustrations with work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action5 = ['painting', 'dancing team',\n",
    "           'fencing team', 'gymnastics team',\n",
    "           'love for independent cinema', 'woodworking hobby',\n",
    "           'university','kayaking team', 'football team',\n",
    "           'baseball team', 'basketball team', 'quizzing team', \n",
    "           'gardening hobby', 'board games group', \n",
    "           'breadmaking hobby', 'baking hobby']\n",
    "verb_action[verb_list_p2[1]] = [action4, action5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    doc = client.annotate(sentence)\n",
    "#     print((doc.))\n",
    "    return(doc.corefChain)\n",
    "#     if doc._.has_coref: \n",
    "#         return (doc._.coref_resolved, doc._.coref_clusters)\n",
    "#     else:\n",
    "#         return ('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_list(pred_str):\n",
    "    pred_list = []\n",
    "    for line in pred_str.split('\\n'):\n",
    "        if 'chainID' not in line:\n",
    "            pred_list.append(line)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivalent_coref(pred1, pred2):\n",
    "    pred1_list = get_pred_list(str(pred1))\n",
    "    pred2_list = get_pred_list(str(pred2))\n",
    "    \n",
    "    return pred1_list == pred2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/sakshiudeshi/Documents/SUTD/Research/Coref-Fairness-Test-Generation/Sakshi-Testbed/Stanford-CoreNLP-Coref/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-0d684caa390e46f3.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n",
      "[chainID: 2\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 2\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 7\n",
      "  endIndex: 8\n",
      "  headIndex: 7\n",
      "  sentenceIndex: 0\n",
      "  position: 3\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "[chainID: 2\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 2\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 7\n",
      "  endIndex: 8\n",
      "  headIndex: 7\n",
      "  sentenceIndex: 0\n",
      "  position: 3\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "Equivalent coref: True\n",
      "______________________\n"
     ]
    }
   ],
   "source": [
    "## Example of not an error\n",
    "inp1, inp2 = ('The person spoke with the guard about his struggles with addiction', \n",
    "              'The person spoke with the carpenter about his struggles with addiction')\n",
    "pred1 = (predict_clusters(inp1))\n",
    "pred2 = (predict_clusters(inp2))\n",
    "\n",
    "print(pred1)\n",
    "print(pred2)\n",
    "print(\"Equivalent coref: \" + str(equivalent_coref(pred1, pred2)))\n",
    "print(\"______________________\")\n",
    "\n",
    "# print('person' in str(pred2[0][0]))\n",
    "# print(pred2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chainID: 1\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 1\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 7\n",
      "  endIndex: 8\n",
      "  headIndex: 7\n",
      "  sentenceIndex: 0\n",
      "  position: 2\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "[chainID: 2\n",
      "mention {\n",
      "  mentionID: 0\n",
      "  mentionType: \"NOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 0\n",
      "  endIndex: 2\n",
      "  headIndex: 1\n",
      "  sentenceIndex: 0\n",
      "  position: 1\n",
      "}\n",
      "mention {\n",
      "  mentionID: 2\n",
      "  mentionType: \"PRONOMINAL\"\n",
      "  number: \"SINGULAR\"\n",
      "  gender: \"MALE\"\n",
      "  animacy: \"ANIMATE\"\n",
      "  beginIndex: 7\n",
      "  endIndex: 8\n",
      "  headIndex: 7\n",
      "  sentenceIndex: 0\n",
      "  position: 3\n",
      "}\n",
      "representative: 0\n",
      "]\n",
      "Equivalent coref: False\n",
      "______________________\n"
     ]
    }
   ],
   "source": [
    "## Example of an error\n",
    "inp1, inp2 = ('The person spoke with the cleaner about his baseball team', \n",
    "              'The person spoke with the librarian about his baseball team')\n",
    "pred1 = (predict_clusters(inp1))\n",
    "pred2 = (predict_clusters(inp2))\n",
    "\n",
    "print(pred1)\n",
    "print(pred2)\n",
    "print(\"Equivalent coref: \" + str(equivalent_coref(pred1, pred2)))\n",
    "print(\"______________________\")\n",
    "\n",
    "# print('person' in str(pred2[0][0]))\n",
    "# print(pred2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "occupation_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "occupation_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 0\n",
      "------------------------------\n",
      "Unique errors: 2\n",
      "Unique inputs: 30\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 60\n",
      "------------------------------\n",
      "Unique errors: 5\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 5\n",
      "Unique inputs: 120\n",
      "------------------------------\n",
      "Unique errors: 6\n",
      "Unique inputs: 150\n",
      "------------------------------\n",
      "Unique errors: 7\n",
      "Unique inputs: 180\n",
      "------------------------------\n",
      "Unique errors: 9\n",
      "Unique inputs: 210\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 240\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 270\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 300\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 330\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 360\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 389\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 419\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 449\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 479\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 509\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 539\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 569\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 599\n",
      "------------------------------\n",
      "Unique errors: 16\n",
      "Unique inputs: 629\n",
      "------------------------------\n",
      "Unique errors: 16\n",
      "Unique inputs: 659\n",
      "------------------------------\n",
      "Unique errors: 17\n",
      "Unique inputs: 689\n",
      "------------------------------\n",
      "Unique errors: 19\n",
      "Unique inputs: 719\n",
      "------------------------------\n",
      "Unique errors: 20\n",
      "Unique inputs: 749\n",
      "------------------------------\n",
      "Unique errors: 20\n",
      "Unique inputs: 779\n",
      "------------------------------\n",
      "Unique errors: 20\n",
      "Unique inputs: 809\n",
      "------------------------------\n",
      "Unique errors: 21\n",
      "Unique inputs: 839\n",
      "------------------------------\n",
      "Unique errors: 22\n",
      "Unique inputs: 868\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique inputs: 898\n",
      "------------------------------\n",
      "Unique errors: 24\n",
      "Unique inputs: 928\n",
      "------------------------------\n",
      "Unique errors: 24\n",
      "Unique inputs: 958\n",
      "------------------------------\n",
      "Unique errors: 24\n",
      "Unique inputs: 988\n",
      "------------------------------\n",
      "Unique errors: 24\n",
      "Unique inputs: 1018\n",
      "------------------------------\n",
      "Unique errors: 24\n",
      "Unique inputs: 1047\n",
      "------------------------------\n",
      "Unique errors: 25\n",
      "Unique inputs: 1077\n",
      "------------------------------\n",
      "Unique errors: 26\n",
      "Unique inputs: 1107\n",
      "------------------------------\n",
      "Unique errors: 27\n",
      "Unique inputs: 1137\n",
      "------------------------------\n",
      "Unique errors: 27\n",
      "Unique inputs: 1167\n",
      "------------------------------\n",
      "Unique errors: 27\n",
      "Unique inputs: 1197\n",
      "------------------------------\n",
      "Unique errors: 28\n",
      "Unique inputs: 1227\n",
      "------------------------------\n",
      "Unique errors: 29\n",
      "Unique inputs: 1257\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 1286\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 1316\n",
      "------------------------------\n",
      "Unique errors: 31\n",
      "Unique inputs: 1346\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 1375\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 1405\n",
      "------------------------------\n",
      "Unique errors: 33\n",
      "Unique inputs: 1435\n",
      "------------------------------\n",
      "Unique errors: 34\n",
      "Unique inputs: 1465\n",
      "------------------------------\n",
      "Unique errors: 35\n",
      "Unique inputs: 1495\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 1524\n",
      "------------------------------\n",
      "Unique errors: 37\n",
      "Unique inputs: 1553\n",
      "------------------------------\n",
      "Unique errors: 38\n",
      "Unique inputs: 1582\n",
      "------------------------------\n",
      "Unique errors: 40\n",
      "Unique inputs: 1612\n",
      "------------------------------\n",
      "Unique errors: 41\n",
      "Unique inputs: 1642\n",
      "------------------------------\n",
      "Unique errors: 43\n",
      "Unique inputs: 1671\n",
      "------------------------------\n",
      "Unique errors: 44\n",
      "Unique inputs: 1701\n",
      "------------------------------\n",
      "Unique errors: 44\n",
      "Unique inputs: 1731\n",
      "------------------------------\n",
      "Unique errors: 44\n",
      "Unique inputs: 1759\n",
      "------------------------------\n",
      "Unique errors: 44\n",
      "Unique inputs: 1789\n",
      "------------------------------\n",
      "Unique errors: 46\n",
      "Unique inputs: 1818\n",
      "------------------------------\n",
      "Unique errors: 47\n",
      "Unique inputs: 1848\n",
      "------------------------------\n",
      "Unique errors: 50\n",
      "Unique inputs: 1878\n",
      "------------------------------\n",
      "Unique errors: 51\n",
      "Unique inputs: 1908\n",
      "------------------------------\n",
      "Unique errors: 51\n",
      "Unique inputs: 1938\n",
      "------------------------------\n",
      "Unique errors: 51\n",
      "Unique inputs: 1968\n",
      "------------------------------\n",
      "Unique errors: 53\n",
      "Unique inputs: 1998\n",
      "------------------------------\n",
      "Unique errors: 54\n",
      "Unique inputs: 2028\n",
      "------------------------------\n",
      "Unique errors: 55\n",
      "Unique inputs: 2058\n",
      "------------------------------\n",
      "Unique errors: 56\n",
      "Unique inputs: 2087\n",
      "------------------------------\n",
      "Unique errors: 58\n",
      "Unique inputs: 2116\n",
      "------------------------------\n",
      "Unique errors: 61\n",
      "Unique inputs: 2146\n",
      "------------------------------\n",
      "Unique errors: 61\n",
      "Unique inputs: 2174\n",
      "------------------------------\n",
      "Unique errors: 61\n",
      "Unique inputs: 2202\n",
      "------------------------------\n",
      "Unique errors: 63\n",
      "Unique inputs: 2230\n",
      "------------------------------\n",
      "Unique errors: 64\n",
      "Unique inputs: 2260\n",
      "------------------------------\n",
      "Unique errors: 65\n",
      "Unique inputs: 2289\n",
      "------------------------------\n",
      "Unique errors: 66\n",
      "Unique inputs: 2319\n",
      "------------------------------\n",
      "Unique errors: 68\n",
      "Unique inputs: 2349\n",
      "------------------------------\n",
      "Unique errors: 68\n",
      "Unique inputs: 2379\n",
      "------------------------------\n",
      "Unique errors: 69\n",
      "Unique inputs: 2408\n",
      "------------------------------\n",
      "Unique errors: 70\n",
      "Unique inputs: 2437\n",
      "------------------------------\n",
      "Unique errors: 70\n",
      "Unique inputs: 2467\n",
      "------------------------------\n",
      "Unique errors: 71\n",
      "Unique inputs: 2497\n",
      "------------------------------\n",
      "Unique errors: 72\n",
      "Unique inputs: 2527\n",
      "------------------------------\n",
      "Unique errors: 72\n",
      "Unique inputs: 2557\n",
      "------------------------------\n",
      "Unique errors: 72\n",
      "Unique inputs: 2586\n",
      "------------------------------\n",
      "Unique errors: 73\n",
      "Unique inputs: 2616\n",
      "------------------------------\n",
      "Unique errors: 73\n",
      "Unique inputs: 2646\n",
      "------------------------------\n",
      "Unique errors: 74\n",
      "Unique inputs: 2676\n",
      "------------------------------\n",
      "Unique errors: 74\n",
      "Unique inputs: 2702\n",
      "------------------------------\n",
      "Unique errors: 75\n",
      "Unique inputs: 2732\n",
      "------------------------------\n",
      "Unique errors: 76\n",
      "Unique inputs: 2761\n",
      "------------------------------\n",
      "Unique errors: 76\n",
      "Unique inputs: 2791\n",
      "------------------------------\n",
      "Unique errors: 76\n",
      "Unique inputs: 2819\n",
      "------------------------------\n",
      "Unique errors: 78\n",
      "Unique inputs: 2849\n",
      "------------------------------\n",
      "Unique errors: 78\n",
      "Unique inputs: 2878\n",
      "------------------------------\n",
      "Unique errors: 79\n",
      "Unique inputs: 2907\n",
      "------------------------------\n",
      "Unique errors: 80\n",
      "Unique inputs: 2937\n",
      "------------------------------\n",
      "82\n",
      "0.027333333333333334\n",
      "Final Unique errors: 80\n",
      "Final Unique inputs: 2964\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 3000\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    oc_list = random.sample(occupations_1 + occupations_2, 2)\n",
    "    \n",
    "    oc1 = oc_list[0]\n",
    "    oc2 = oc_list[1]\n",
    "    \n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    \n",
    "    input1 = (\"The person \" + verb + \" \"\n",
    "           + oc1 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "    input2 = (\"The person \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "    pred1 = predict_clusters(input1)\n",
    "    pred2 = predict_clusters(input2)\n",
    "#     pred3, _ = predict_clusters(input2)\n",
    "    \n",
    "    \n",
    "    if(i % 30 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add((input1, input2))\n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    \n",
    "#     print(pred1, pred2)\n",
    "#     print(input1)\n",
    "#     print(input2)\n",
    "\n",
    "    \n",
    "\n",
    "    if not equivalent_coref(pred1, pred2):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "# #         if(True):\n",
    "#             if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0])):\n",
    "# #             if(True):\n",
    "                err_count += 1\n",
    "                \n",
    "                unique_input1_error_set.add((input1, input2))\n",
    "                \n",
    "#                 print(pred1, pred2)\n",
    "#                 print(input1)\n",
    "#                 print(input2)\n",
    "#                 print(\"---------------\")\n",
    "                \n",
    "                update_dict(occupation_pair_error, (oc1, oc2))\n",
    "                update_dict(occupation1_error, oc1)\n",
    "                update_dict(occupation2_error, oc2)\n",
    "                update_dict(verb_error, verb)\n",
    "                update_dict(action_error, action)\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The person spoke with the cleaner about his professional goals', 'The person spoke with the analyst about his professional goals'), ('The person asked the cleaner if he can do 100 push ups', 'The person asked the salesperson if he can do 100 push ups'), ('The person asked the cleaner if he can bake cookies', 'The person asked the cook if he can bake cookies'), ('The person asked the lawyer if he could bake cookies', 'The person asked the cleaner if he could bake cookies'), ('The person asked the writer if he could bake bread', 'The person asked the cleaner if he could bake bread'), ('The person asked the cleaner if he can generate the report', 'The person asked the receptionist if he can generate the report'), ('The person spoke with the chief about his football team', 'The person spoke with the cashier about his football team'), ('The person spoke with the cleaner about his frustrations with work', 'The person spoke with the baker about his frustrations with work'), ('The person spoke with the cleaner about his desire to study', 'The person spoke with the baker about his desire to study'), ('The person asked the cleaner if he can generate the report', 'The person asked the lawyer if he can generate the report')]\n",
      "\n",
      "[('The person told the baker that he could sign the document', 'The person told the librarian that he could sign the document'), ('The person told the counselor that he can bake cookies', 'The person told the baker that he can bake cookies'), ('The person asked the paralegal if he could help with the delivery', 'The person asked the cashier if he could help with the delivery'), ('The person disclosed to the salesperson that he could bake a cake', 'The person disclosed to the technician that he could bake a cake'), ('The person notified the cashier that his the waiting times were estimated', 'The person notified the writer that his the waiting times were estimated'), ('The person disclosed to the sheriff that he can bake bread', 'The person disclosed to the lawyer that he can bake bread'), ('The person told the mechanic that he can bake bread', 'The person told the sheriff that he can bake bread'), ('The person disclosed to the engineer that he could return the paperwork', 'The person disclosed to the analyst that he could return the paperwork'), ('The person spoke with the accountant about his woodworking hobby', 'The person spoke with the carpenter about his woodworking hobby'), ('The person spoke with the attendant about his woodworking hobby', 'The person spoke with the sheriff about his woodworking hobby')]\n"
     ]
    }
   ],
   "source": [
    "# print(occupation_pair_count)\n",
    "# print(occupation1_count)\n",
    "# print(occupation2_count)\n",
    "# print(verb_count)\n",
    "# print(action_count)\n",
    "print(list(unique_input1_error_set)[0:10])\n",
    "print()\n",
    "print(list(unique_input1_set.difference(unique_input1_error_set))[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('saved_pickles/Exploration/unique_input1_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_error_set, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_count, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_count, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_error, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_clusters(\"The guard spoke with the librarian about his struggles with addiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
