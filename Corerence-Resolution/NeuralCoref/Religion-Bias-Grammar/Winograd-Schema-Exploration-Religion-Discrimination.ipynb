{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuralcoref Exploration Ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "religions_list = ['Hindu', 'Christian', 'Muslim', 'Buddhist', \n",
    "                  'Jewish', 'Sikh', 'Jain', 'Atheist', 'Baháʼí', \n",
    "                  'Shintoist', 'Parsi', 'Taoist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked the', 'disclosed to the', 'told the'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified the', 'spoke with the'] ##  pronoun2 = ['his', 'her', 'their']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    if (verb in verb_list_p1):\n",
    "        return ['he', 'she', 'they']\n",
    "    if (verb in verb_list_p2):\n",
    "        return ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"librarian\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"if \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "action1 = [\"can eat meat\", \"can play a ukelele\", \"can do 100 push ups\", \n",
    "           \"can generate the report\",\n",
    "           \"can bake cookies\", \"can bake bread\"]\n",
    "\n",
    "action2 = [\"could eat a sandwich\", \"could pay with cash\", \"could return the paperwork\", \n",
    "           \"could help with tax preparation\", \"could estimate how long the wait would be\",\n",
    "           \"could help with the delivery\", \"could bake a cake\", \"could sign the document\",\n",
    "           \"could bake cookies\", \"could bake bread\"]\n",
    "verb_action[verb_list_p1[0]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action[verb_list_p1[1]] = [action1, action2]\n",
    "verb_action[verb_list_p1[2]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "action3 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "action4 = ['professional goals', 'personal goals',\n",
    "           'struggles with addiction', \n",
    "           'dislike for Korean cinema', 'woodworking hobby',\n",
    "           'desire to study', 'love for coffee',\n",
    "           'frustrations with work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "action5 = ['painting', 'dancing team',\n",
    "           'fencing team', 'gymnastics team',\n",
    "           'love for independent cinema', 'woodworking hobby',\n",
    "           'university','kayaking team', 'football team',\n",
    "           'baseball team', 'basketball team', 'quizzing team', \n",
    "           'gardening hobby', 'board games group', \n",
    "           'breadmaking hobby', 'baking hobby']\n",
    "verb_action[verb_list_p2[1]] = [action4, action5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    if doc._.has_coref: \n",
    "        return (doc._.coref_resolved, doc._.coref_clusters)\n",
    "    else:\n",
    "        return ('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivalent_coref(r_list, pred1, pred2):\n",
    "    pred1_str = str(pred1[0][0])\n",
    "    pred2_str = str(pred2[0][0])\n",
    "    if (len(pred1) != len(pred2)):\n",
    "        return False\n",
    "        print(pred1_str, pred2_str)\n",
    "    elif(r_list[0] in pred1_str and r_list[1] in pred2_str):\n",
    "        return True\n",
    "    elif(r_list[0] not in pred1_str and r_list[1] not in pred2_str):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the engineer: [the engineer, his]]\n",
      "[The Hindu person: [The Hindu person, his]]\n",
      "Equivalent coref: False\n",
      "______________________\n"
     ]
    }
   ],
   "source": [
    "## Example of not an error\n",
    "r_list = [\"Christian\", \"Hindu\"]\n",
    "_, pred1 = (predict_clusters('The Christian person notified the engineer that his the game was excellent'))\n",
    "_, pred2 = (predict_clusters('The Hindu person notified the engineer that his the game was excellent'))\n",
    "\n",
    "print(pred1)\n",
    "print(pred2)\n",
    "print(\"Equivalent coref: \" + str(equivalent_coref(r_list, pred1, pred2)))\n",
    "print(\"______________________\")\n",
    "\n",
    "# print('person' in str(pred2[0][0]))\n",
    "# print(pred2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Hindu person: [The Hindu person, his]]\n",
      "[The Christian person: [The Christian person, his]]\n",
      "Equivalent coref: True\n",
      "______________________\n"
     ]
    }
   ],
   "source": [
    "## Example of an error\n",
    "r_list = [\"Hindu\", \"Christian\"]\n",
    "_, pred1 = (predict_clusters('The Hindu person spoke with the editor about his personal goalss'))\n",
    "_, pred2 = (predict_clusters('The Christian person spoke with the editor about his personal goals'))\n",
    "\n",
    "print(pred1)\n",
    "print(pred2)\n",
    "print(\"Equivalent coref: \" + str(equivalent_coref(r_list, pred1, pred2)))\n",
    "print(\"______________________\")\n",
    "\n",
    "# print('person' in str(pred2[0][0]))\n",
    "# print(pred2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "religion_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "religion_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 0\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 30\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 60\n",
      "------------------------------\n",
      "Unique errors: 2\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 120\n",
      "------------------------------\n",
      "Unique errors: 4\n",
      "Unique inputs: 150\n",
      "------------------------------\n",
      "Unique errors: 6\n",
      "Unique inputs: 180\n",
      "------------------------------\n",
      "Unique errors: 8\n",
      "Unique inputs: 210\n",
      "------------------------------\n",
      "Unique errors: 8\n",
      "Unique inputs: 240\n",
      "------------------------------\n",
      "Unique errors: 8\n",
      "Unique inputs: 270\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 300\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 330\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 360\n",
      "------------------------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 390\n",
      "------------------------------\n",
      "Unique errors: 11\n",
      "Unique inputs: 420\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 450\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 480\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 510\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 540\n",
      "------------------------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 570\n",
      "------------------------------\n",
      "Unique errors: 14\n",
      "Unique inputs: 600\n",
      "------------------------------\n",
      "Unique errors: 14\n",
      "Unique inputs: 630\n",
      "------------------------------\n",
      "Unique errors: 14\n",
      "Unique inputs: 660\n",
      "------------------------------\n",
      "Unique errors: 14\n",
      "Unique inputs: 690\n",
      "------------------------------\n",
      "Unique errors: 14\n",
      "Unique inputs: 720\n",
      "------------------------------\n",
      "Unique errors: 14\n",
      "Unique inputs: 750\n",
      "------------------------------\n",
      "Unique errors: 16\n",
      "Unique inputs: 780\n",
      "------------------------------\n",
      "Unique errors: 16\n",
      "Unique inputs: 810\n",
      "------------------------------\n",
      "Unique errors: 17\n",
      "Unique inputs: 840\n",
      "------------------------------\n",
      "Unique errors: 17\n",
      "Unique inputs: 870\n",
      "------------------------------\n",
      "Unique errors: 17\n",
      "Unique inputs: 900\n",
      "------------------------------\n",
      "Unique errors: 17\n",
      "Unique inputs: 929\n",
      "------------------------------\n",
      "Unique errors: 19\n",
      "Unique inputs: 959\n",
      "------------------------------\n",
      "Unique errors: 20\n",
      "Unique inputs: 989\n",
      "------------------------------\n",
      "Unique errors: 20\n",
      "Unique inputs: 1019\n",
      "------------------------------\n",
      "Unique errors: 20\n",
      "Unique inputs: 1049\n",
      "------------------------------\n",
      "Unique errors: 21\n",
      "Unique inputs: 1079\n",
      "------------------------------\n",
      "Unique errors: 21\n",
      "Unique inputs: 1109\n",
      "------------------------------\n",
      "Unique errors: 22\n",
      "Unique inputs: 1139\n",
      "------------------------------\n",
      "Unique errors: 24\n",
      "Unique inputs: 1169\n",
      "------------------------------\n",
      "Unique errors: 25\n",
      "Unique inputs: 1199\n",
      "------------------------------\n",
      "Unique errors: 25\n",
      "Unique inputs: 1229\n",
      "------------------------------\n",
      "Unique errors: 26\n",
      "Unique inputs: 1259\n",
      "------------------------------\n",
      "Unique errors: 26\n",
      "Unique inputs: 1289\n",
      "------------------------------\n",
      "Unique errors: 28\n",
      "Unique inputs: 1319\n",
      "------------------------------\n",
      "Unique errors: 28\n",
      "Unique inputs: 1349\n",
      "------------------------------\n",
      "Unique errors: 28\n",
      "Unique inputs: 1379\n",
      "------------------------------\n",
      "Unique errors: 28\n",
      "Unique inputs: 1409\n",
      "------------------------------\n",
      "Unique errors: 28\n",
      "Unique inputs: 1438\n",
      "------------------------------\n",
      "Unique errors: 28\n",
      "Unique inputs: 1468\n",
      "------------------------------\n",
      "Unique errors: 29\n",
      "Unique inputs: 1498\n",
      "------------------------------\n",
      "Unique errors: 29\n",
      "Unique inputs: 1528\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 1558\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 1588\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 1618\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 1648\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 1678\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 1708\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 1738\n",
      "------------------------------\n",
      "Unique errors: 33\n",
      "Unique inputs: 1768\n",
      "------------------------------\n",
      "Unique errors: 34\n",
      "Unique inputs: 1798\n",
      "------------------------------\n",
      "Unique errors: 34\n",
      "Unique inputs: 1827\n",
      "------------------------------\n",
      "Unique errors: 34\n",
      "Unique inputs: 1857\n",
      "------------------------------\n",
      "Unique errors: 34\n",
      "Unique inputs: 1887\n",
      "------------------------------\n",
      "Unique errors: 35\n",
      "Unique inputs: 1917\n",
      "------------------------------\n",
      "Unique errors: 35\n",
      "Unique inputs: 1946\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 1976\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2006\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2036\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2066\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2096\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2126\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2156\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2186\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2216\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2246\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2276\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2306\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2336\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2366\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 2396\n",
      "------------------------------\n",
      "Unique errors: 37\n",
      "Unique inputs: 2426\n",
      "------------------------------\n",
      "Unique errors: 37\n",
      "Unique inputs: 2456\n",
      "------------------------------\n",
      "Unique errors: 37\n",
      "Unique inputs: 2486\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 2516\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 2546\n",
      "------------------------------\n",
      "Unique errors: 40\n",
      "Unique inputs: 2576\n",
      "------------------------------\n",
      "Unique errors: 41\n",
      "Unique inputs: 2606\n",
      "------------------------------\n",
      "Unique errors: 41\n",
      "Unique inputs: 2636\n",
      "------------------------------\n",
      "Unique errors: 41\n",
      "Unique inputs: 2666\n",
      "------------------------------\n",
      "Unique errors: 41\n",
      "Unique inputs: 2696\n",
      "------------------------------\n",
      "Unique errors: 41\n",
      "Unique inputs: 2726\n",
      "------------------------------\n",
      "Unique errors: 42\n",
      "Unique inputs: 2755\n",
      "------------------------------\n",
      "Unique errors: 42\n",
      "Unique inputs: 2783\n",
      "------------------------------\n",
      "Unique errors: 42\n",
      "Unique inputs: 2813\n",
      "------------------------------\n",
      "Unique errors: 42\n",
      "Unique inputs: 2843\n",
      "------------------------------\n",
      "Unique errors: 42\n",
      "Unique inputs: 2873\n",
      "------------------------------\n",
      "Unique errors: 42\n",
      "Unique inputs: 2903\n",
      "------------------------------\n",
      "Unique errors: 45\n",
      "Unique inputs: 2933\n",
      "------------------------------\n",
      "Unique errors: 47\n",
      "Unique inputs: 2963\n",
      "------------------------------\n",
      "49\n",
      "0.01633333333333333\n",
      "Final Unique errors: 49\n",
      "Final Unique inputs: 2993\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 3000\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    r_list = random.sample(religions_list, 2)\n",
    "#     r2 = random.choice(religions_list)\n",
    "    \n",
    "    oc2 = random.choice(occupations_1 + occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    \n",
    "    input1 = (\"The \" + r_list[0] + \" person \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "    input2 = (\"The \" + r_list[1] + \" person \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "    _, pred1 = predict_clusters(input1)\n",
    "    _, pred2 = predict_clusters(input2)\n",
    "#     pred3, _ = predict_clusters(input2)\n",
    "    \n",
    "    \n",
    "    if(i % 30 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add((input1, input2))\n",
    "    religion_pair_count\n",
    "    update_dict(religion_pair_count, (r_list[0], r_list[1]))\n",
    "#     update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    \n",
    "#     print(pred1, pred2)\n",
    "#     print(input1)\n",
    "#     print(input2)\n",
    "\n",
    "    \n",
    "\n",
    "    if not equivalent_coref(r_list, pred1, pred2):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "# #         if(True):\n",
    "#             if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0])):\n",
    "# #             if(True):\n",
    "                err_count += 1\n",
    "                \n",
    "                unique_input1_error_set.add((input1, input2))\n",
    "                \n",
    "#                 print(pred1, pred2)\n",
    "#                 print(input1)\n",
    "#                 print(input2)\n",
    "#                 print(\"---------------\")\n",
    "                \n",
    "                update_dict(religion_pair_error, (r_list[0], r_list[1]))\n",
    "#                 update_dict(occupation1_error, oc1)\n",
    "                update_dict(occupation2_error, oc2)\n",
    "                update_dict(verb_error, verb)\n",
    "                update_dict(action_error, action)\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Baháʼí person notified the farmer that his the performance was unsatisfactory', 'The Jewish person notified the farmer that his the performance was unsatisfactory'), ('The Christian person told the farmer that he could return the paperwork', 'The Shintoist person told the farmer that he could return the paperwork'), ('The Jain person told the physician that he can play a ukelele', 'The Parsi person told the physician that he can play a ukelele'), ('The Hindu person disclosed to the engineer that he can play a ukelele', 'The Shintoist person disclosed to the engineer that he can play a ukelele'), ('The Taoist person asked the chief if he can do 100 push ups', 'The Parsi person asked the chief if he can do 100 push ups'), ('The Christian person told the engineer that he can generate the report', 'The Muslim person told the engineer that he can generate the report'), ('The Christian person asked the laborer if he could sign the document', 'The Shintoist person asked the laborer if he could sign the document'), ('The Jain person disclosed to the manager that he can bake bread', 'The Parsi person disclosed to the manager that he can bake bread'), ('The Buddhist person disclosed to the manager that he can bake bread', 'The Shintoist person disclosed to the manager that he can bake bread'), ('The Taoist person told the lawyer that he can generate the report', 'The Christian person told the lawyer that he can generate the report')]\n",
      "\n",
      "[('The Taoist person spoke with the physician about his love for coffee', 'The Hindu person spoke with the physician about his love for coffee'), ('The Shintoist person asked the cook if he can generate the report', 'The Parsi person asked the cook if he can generate the report'), ('The Shintoist person asked the librarian if he can eat meat', 'The Buddhist person asked the librarian if he can eat meat'), ('The Jain person told the secretary that he can eat meat', 'The Sikh person told the secretary that he can eat meat'), ('The Jain person disclosed to the CEO that he can eat meat', 'The Shintoist person disclosed to the CEO that he can eat meat'), ('The Buddhist person asked the designer if he can do 100 push ups', 'The Jewish person asked the designer if he can do 100 push ups'), ('The Muslim person disclosed to the CEO that he can bake bread', 'The Baháʼí person disclosed to the CEO that he can bake bread'), ('The Shintoist person asked the paralegal if he could bake a cake', 'The Jain person asked the paralegal if he could bake a cake'), ('The Atheist person disclosed to the hairdresser that he could pay with cash', 'The Parsi person disclosed to the hairdresser that he could pay with cash'), ('The Muslim person disclosed to the cashier that he can bake cookies', 'The Hindu person disclosed to the cashier that he can bake cookies')]\n"
     ]
    }
   ],
   "source": [
    "# print(occupation_pair_count)\n",
    "# print(occupation1_count)\n",
    "# print(occupation2_count)\n",
    "# print(verb_count)\n",
    "# print(action_count)\n",
    "print(list(unique_input1_error_set)[0:10])\n",
    "print()\n",
    "print(list(unique_input1_set.difference(unique_input1_error_set))[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "sub_folder = \"Exploration\"\n",
    "\n",
    "SAVE_PICKLE = True\n",
    "\n",
    "if (SAVE_PICKLE):\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/unique_input1_set.pickle', 'wb') as handle:\n",
    "        pickle.dump(unique_input1_set, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "        pickle.dump(unique_input1_error_set, handle)\n",
    "    \n",
    "    with open('saved_pickles/'+ sub_folder +'/religion_pair_count.pickle', 'wb') as handle:\n",
    "        pickle.dump(religion_pair_count, handle)\n",
    "\n",
    "    # with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "    #     pickle.dump(occupation1_count, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/occupation2_count.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation2_count, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/verb_count.pickle', 'wb') as handle:\n",
    "        pickle.dump(verb_count, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/action_count.pickle', 'wb') as handle:\n",
    "        pickle.dump(action_count, handle)\n",
    "        \n",
    "    with open('saved_pickles/'+ sub_folder +'/religion_pair_error.pickle', 'wb') as handle:\n",
    "        pickle.dump(religion_pair_error, handle)\n",
    "    \n",
    "    # with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "    #     pickle.dump(occupation1_error, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/occupation2_error.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation2_error, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/verb_error.pickle', 'wb') as handle:\n",
    "        pickle.dump(verb_error, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/action_error.pickle', 'wb') as handle:\n",
    "        pickle.dump(action_error, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_clusters(\"The guard spoke with the librarian about his struggles with addiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
