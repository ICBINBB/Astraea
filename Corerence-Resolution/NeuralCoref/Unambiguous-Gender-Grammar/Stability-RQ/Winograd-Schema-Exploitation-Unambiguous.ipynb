{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuralcoref Exploitation Unambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked', 'disclosed to', 'told'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified', 'spoke with'] ##  pronoun2 = ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    return ['He', 'She', 'They']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"about \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "\n",
    "action1 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p1[1]] = [action1] ### disclosed to\n",
    "verb_action[verb_list_p1[2]] = [action1] ### told\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action1] ### notified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spoke with \n",
    "### asked\n",
    "action2 = ['painting', 'dancing',\n",
    "           'fencing', 'gymnastics',\n",
    "           'independent cinema', 'woodworking',\n",
    "           'studying', 'horse racing',\n",
    "           'singing', 'kayaking', 'football',\n",
    "           'baseball', 'basketball', 'quizzing', \n",
    "           'gardening', 'blogging', 'board games', \n",
    "           'breadmaking', 'baking']\n",
    "verb_action[verb_list_p2[1]] = [action2]\n",
    "verb_action[verb_list_p1[0]] = [action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_verb = ['standing', 'speaking', 'talking', \n",
    "                  'passing by', 'sitting', 'sitting on the chair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    if doc._.has_coref: \n",
    "        return (doc._.coref_resolved, doc._.coref_clusters)\n",
    "    else:\n",
    "        return ('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(oc1, oc2, verb, action, pronoun):\n",
    "\n",
    "\n",
    "    aux_verb = random.choice(auxiliary_verb)\n",
    "    input1 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[0] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "\n",
    "    input2 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[1] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "        \n",
    "    return input1, input2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/unique_input1_set.pickle', 'rb') as handle:\n",
    "    unique_input1_set = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'rb') as handle:\n",
    "    unique_input1_error_set = pickle.load(handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'rb') as handle:\n",
    "    occupation_pair_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_error.pickle', 'rb') as handle:\n",
    "    occupation1_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error.pickle', 'rb') as handle:\n",
    "    occupation2_error = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error.pickle', 'rb') as handle:\n",
    "    verb_error = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/Exploration/action_error.pickle', 'rb') as handle:\n",
    "    action_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'rb') as handle:\n",
    "    occupation_pair_count = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_count.pickle', 'rb') as handle:\n",
    "    occupation1_count = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count.pickle', 'rb') as handle:\n",
    "    occupation2_count = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count.pickle', 'rb') as handle:\n",
    "    verb_count = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/Exploration/action_count.pickle', 'rb') as handle:\n",
    "    action_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_dict(D):\n",
    "    return {k: v for k, v in sorted(D.items(), key=lambda item: item[1], reverse=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate_dict(error_dict, count_dict):\n",
    "    error_rate_dict = {}\n",
    "    for key in error_dict:\n",
    "        error_rate_dict[key] = error_dict[key]/count_dict[key]\n",
    "    return get_sorted_dict(error_rate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dict(error_dict, count_dict):\n",
    "    error_rate_dict = get_error_rate_dict(error_dict, count_dict)\n",
    "    \n",
    "    probability_dict = {}\n",
    "    error_rate_sum = sum(error_rate_dict.values())\n",
    "    for error_rate in error_rate_dict:\n",
    "        probability_dict[error_rate] = error_rate_dict[error_rate]/error_rate_sum\n",
    "    \n",
    "    return probability_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_random_choice(error_dict, count_dict, probablilities_dict = None):\n",
    "    if probablilities_dict == None:\n",
    "        probability_dict = get_probability_dict(error_dict, count_dict)\n",
    "    else:\n",
    "        probability_dict = probablilities_dict\n",
    "    \n",
    "    return list(probability_dict.keys())[np.random.choice(len(list(probability_dict.keys())), p=list(probability_dict.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set_exploitation = set()\n",
    "unique_input1_error_set_exploitation = set()\n",
    "\n",
    "occupation_pair_error_exploitation = {}\n",
    "\n",
    "occupation1_error_exploitation = {}\n",
    "\n",
    "occupation2_error_exploitation = {}\n",
    "\n",
    "verb_error_exploitation = {}\n",
    "\n",
    "action_error_exploitation = {}\n",
    "\n",
    "occupation_pair_count_exploitation = {}\n",
    "\n",
    "occupation1_count_exploitation = {}\n",
    "\n",
    "occupation2_count_exploitation = {}\n",
    "\n",
    "verb_count_exploitation = {}\n",
    "\n",
    "action_count_exploitation = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CEO': 0.45043637993403984, 'analyst': 0.17288310791828373, 'manager': 0.11643415374193503, 'guard': 0.09691099358060573, 'engineer': 0.09529581035426231, 'developer': 0.060540867754472524, 'mover': 0.007498686716400969}\n",
      "\n",
      "{'CEO': 0.9847328244274809, 'analyst': 0.3779527559055118, 'manager': 0.2545454545454545, 'guard': 0.211864406779661, 'engineer': 0.20833333333333334, 'developer': 0.1323529411764706, 'mover': 0.01639344262295082}\n",
      "\n",
      "{'CEO': 45271, 'analyst': 17260, 'manager': 11459, 'guard': 9641, 'engineer': 9585, 'developer': 5981, 'mover': 803}\n"
     ]
    }
   ],
   "source": [
    "oc1_probability = get_probability_dict(occupation1_error, occupation1_count)\n",
    "\n",
    "print(oc1_probability)\n",
    "print()\n",
    "\n",
    "error_rate_dict = get_error_rate_dict(occupation1_error, occupation1_count)\n",
    "print(error_rate_dict)\n",
    "print()\n",
    "\n",
    "output_dict = {}\n",
    "for i in range(100000):\n",
    "    oc1 = get_weighted_random_choice(occupation1_error, occupation1_count, probablilities_dict=oc1_probability)\n",
    "    update_dict(output_dict, oc1)\n",
    "print(get_sorted_dict(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "STABILITY_ERRS = []\n",
    "STABILITY_TEST_CASES = []\n",
    "\n",
    "STABILITY_ITERS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_sentences(ITERS=3000):\n",
    "    err_count = 0\n",
    "\n",
    "    oc1_probability = get_probability_dict(occupation1_error, occupation1_count)\n",
    "    oc2_probability = get_probability_dict(occupation2_error, occupation2_count)\n",
    "\n",
    "    for i in range(ITERS):\n",
    "#         oc1 = random.choice(occupations_1)\n",
    "#         oc2 = random.choice(occupations_2)\n",
    "        oc1 = get_weighted_random_choice(occupation1_error, occupation1_count, probablilities_dict=oc1_probability)\n",
    "        oc2 = get_weighted_random_choice(occupation2_error, occupation2_count, probablilities_dict=oc2_probability)\n",
    "        verb = random.choice(list(verb_action.keys()))\n",
    "        action = random.choice(random.choice(verb_action[verb]))\n",
    "        pronoun = choose_pronoun_type(verb)\n",
    "        input1, input2 = generate_sentences(oc1, oc2, verb, action, pronoun)\n",
    "\n",
    "\n",
    "#         input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#                + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "        pred1, _ = predict_clusters(input1)\n",
    "        pred2, _ = predict_clusters(input2)\n",
    "#         pred3, _ = predict_clusters(input3)\n",
    "\n",
    "\n",
    "#         if(i % 30 == 0):\n",
    "#             print(\"Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "#             print(\"Unique inputs: \" + str(len(unique_input1_set_exploitation)))\n",
    "#             print(\"Iterations: \" + str(i))\n",
    "#             print(\"------------------------------\")\n",
    "\n",
    "        if input1 not in unique_input1_set:\n",
    "            unique_input1_set_exploitation.add(input1)\n",
    "\n",
    "        update_dict(occupation_pair_count_exploitation, (oc1, oc2))\n",
    "        update_dict(occupation1_count_exploitation, oc1)\n",
    "        update_dict(occupation2_count_exploitation, oc2)\n",
    "        update_dict(verb_count_exploitation, verb)\n",
    "        update_dict(action_count_exploitation, action)\n",
    "\n",
    "\n",
    "\n",
    "        if not (pred1 == pred2):\n",
    "#             if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#                 if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0]) ):\n",
    "    #         if(True):\n",
    "                    err_count += 1\n",
    "        \n",
    "                    \n",
    "                    if input1 not in unique_input1_error_set:\n",
    "                        unique_input1_error_set_exploitation.add(input1)\n",
    "                    \n",
    "#                         if (pred2 != ''):\n",
    "#                         print(pred1, pred2)\n",
    "#                         print()\n",
    "#                         print(input1)\n",
    "#                         print(input2)\n",
    "#                         print(\"--------------\")\n",
    "#                         else:\n",
    "#                             print(\"empty pred2 error\")\n",
    "    #                 print(input3)\n",
    "\n",
    "                    update_dict(occupation_pair_error_exploitation, (oc1, oc2))\n",
    "                    update_dict(occupation1_error_exploitation, oc1)\n",
    "                    update_dict(occupation2_error_exploitation, oc2)\n",
    "                    update_dict(verb_error_exploitation, verb)\n",
    "                    update_dict(action_error_exploitation, action)\n",
    "\n",
    "\n",
    "\n",
    "    print(err_count)\n",
    "    print(err_count/ITERS)\n",
    "    print(\"Final Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "    print(\"Final Unique inputs: \" + str(len(unique_input1_set_exploitation)))\n",
    "    \n",
    "    STABILITY_ERRS.append(unique_input1_error_set_exploitation)\n",
    "    STABILITY_TEST_CASES.append(unique_input1_set_exploitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1749\n",
      "0.583\n",
      "Final Unique errors: 1649\n",
      "Final Unique inputs: 2867\n",
      "1729\n",
      "0.5763333333333334\n",
      "Final Unique errors: 1641\n",
      "Final Unique inputs: 2882\n",
      "1735\n",
      "0.5783333333333334\n",
      "Final Unique errors: 1637\n",
      "Final Unique inputs: 2873\n",
      "1732\n",
      "0.5773333333333334\n",
      "Final Unique errors: 1650\n",
      "Final Unique inputs: 2889\n",
      "1752\n",
      "0.584\n",
      "Final Unique errors: 1641\n",
      "Final Unique inputs: 2851\n",
      "1760\n",
      "0.5866666666666667\n",
      "Final Unique errors: 1666\n",
      "Final Unique inputs: 2882\n",
      "1764\n",
      "0.588\n",
      "Final Unique errors: 1666\n",
      "Final Unique inputs: 2860\n",
      "1720\n",
      "0.5733333333333334\n",
      "Final Unique errors: 1629\n",
      "Final Unique inputs: 2881\n",
      "1743\n",
      "0.581\n",
      "Final Unique errors: 1644\n",
      "Final Unique inputs: 2870\n",
      "1768\n",
      "0.5893333333333334\n",
      "Final Unique errors: 1660\n",
      "Final Unique inputs: 2867\n"
     ]
    }
   ],
   "source": [
    "for i in range(STABILITY_ITERS):\n",
    "    \n",
    "    unique_input1_set_exploitation = set()\n",
    "    unique_input1_error_set_exploitation = set()\n",
    "\n",
    "    occupation_pair_error_exploitation = {}\n",
    "\n",
    "    occupation1_error_exploitation = {}\n",
    "\n",
    "    occupation2_error_exploitation = {}\n",
    "\n",
    "    verb_error_exploitation = {}\n",
    "\n",
    "    action_error_exploitation = {}\n",
    "\n",
    "    occupation_pair_count_exploitation = {}\n",
    "\n",
    "    occupation1_count_exploitation = {}\n",
    "\n",
    "    occupation2_count_exploitation = {}\n",
    "\n",
    "    verb_count_exploitation = {}\n",
    "\n",
    "    action_count_exploitation = {}\n",
    "\n",
    "    generate_test_sentences(ITERS=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1649 2867\n",
      "1641 2882\n",
      "1637 2873\n",
      "1650 2889\n",
      "1641 2851\n",
      "1666 2882\n",
      "1666 2860\n",
      "1629 2881\n",
      "1644 2870\n",
      "1660 2867\n"
     ]
    }
   ],
   "source": [
    "for i, err_set in enumerate(STABILITY_ERRS):\n",
    "    print(len(err_set), len(STABILITY_TEST_CASES[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('saved_pickles/Exploitation/occupation_pair_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_count_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation1_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_count_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation2_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_count_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/verb_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_count_exploitation, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploitation/action_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_count_exploitation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('saved_pickles/Exploitation/occupation_pair_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_error_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation1_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_error_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation2_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_error_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/verb_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_error_exploitation, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploitation/action_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_error_exploitation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
