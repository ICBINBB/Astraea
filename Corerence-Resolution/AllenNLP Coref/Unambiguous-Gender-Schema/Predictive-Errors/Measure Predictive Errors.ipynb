{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:12:31.107472 4400377280 file_utils.py:39] PyTorch version 1.5.1 available.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "I0826 14:12:36.298513 4400377280 file_utils.py:339] checking cache for https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz at /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0826 14:12:36.299572 4400377280 file_utils.py:340] waiting to acquire lock on /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0826 14:12:36.303516 4400377280 filelock.py:274] Lock 5580389064 acquired on /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174.lock\n",
      "I0826 14:12:36.304389 4400377280 file_utils.py:343] cache of https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz is up-to-date\n",
      "I0826 14:12:36.305064 4400377280 filelock.py:318] Lock 5580389064 released on /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174.lock\n",
      "I0826 14:12:36.305846 4400377280 archival.py:164] loading archive file https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz from cache at /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0826 14:12:36.306699 4400377280 archival.py:171] extracting archive file /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174 to temp dir /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmpa42_zajr\n",
      "I0826 14:12:44.638078 4400377280 params.py:247] type = from_instances\n",
      "I0826 14:12:44.638705 4400377280 vocabulary.py:321] Loading token dictionary from /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmpa42_zajr/vocabulary.\n",
      "I0826 14:12:44.639408 4400377280 filelock.py:274] Lock 5568314840 acquired on /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmpa42_zajr/vocabulary/.lock\n",
      "I0826 14:12:44.640942 4400377280 filelock.py:318] Lock 5568314840 released on /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmpa42_zajr/vocabulary/.lock\n",
      "I0826 14:12:44.642108 4400377280 params.py:247] model.type = coref\n",
      "I0826 14:12:44.643262 4400377280 params.py:247] model.regularizer = None\n",
      "I0826 14:12:44.644129 4400377280 params.py:247] model.text_field_embedder.type = basic\n",
      "I0826 14:12:44.644942 4400377280 params.py:247] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "I0826 14:12:44.645862 4400377280 params.py:247] model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0826 14:12:44.646473 4400377280 params.py:247] model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "I0826 14:12:44.647416 4400377280 params.py:247] model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
      "I0826 14:12:45.709995 4400377280 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0826 14:12:45.711039 4400377280 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0826 14:12:46.116933 4400377280 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/SpanBERT/spanbert-large-cased/pytorch_model.bin from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c8041a5fa85cc2725744bab27890d410f0f47dde0c9f1152de9a1b1e5df049e.d1ce6dff7f84348ad7c77a33a9a6e8751099db9c9d609ac7752e61804befe4da\n",
      "I0826 14:12:50.046380 4400377280 modeling_utils.py:741] Weights of BertModel not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "I0826 14:12:51.102414 4400377280 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0826 14:12:51.103507 4400377280 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0826 14:12:51.104303 4400377280 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0826 14:12:55.237007 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0826 14:12:55.238288 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0826 14:12:55.238937 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0826 14:12:55.239493 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0826 14:12:56.302309 4400377280 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0826 14:12:56.303620 4400377280 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:12:56.304585 4400377280 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0826 14:13:00.360485 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0826 14:13:00.361474 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0826 14:13:00.362147 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0826 14:13:00.363099 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0826 14:13:00.396064 4400377280 params.py:247] model.context_layer.type = pass_through\n",
      "I0826 14:13:00.396825 4400377280 params.py:247] model.context_layer.input_dim = 1024\n",
      "I0826 14:13:00.397415 4400377280 params.py:247] model.mention_feedforward.input_dim = 3092\n",
      "I0826 14:13:00.398245 4400377280 params.py:247] model.mention_feedforward.num_layers = 2\n",
      "I0826 14:13:00.398935 4400377280 params.py:247] model.mention_feedforward.hidden_dims = 1500\n",
      "I0826 14:13:00.399717 4400377280 params.py:247] model.mention_feedforward.activations = relu\n",
      "I0826 14:13:00.400547 4400377280 params.py:247] type = relu\n",
      "I0826 14:13:00.401765 4400377280 params.py:247] model.mention_feedforward.dropout = 0.3\n",
      "I0826 14:13:00.444787 4400377280 params.py:247] model.antecedent_feedforward.input_dim = 9296\n",
      "I0826 14:13:00.445434 4400377280 params.py:247] model.antecedent_feedforward.num_layers = 2\n",
      "I0826 14:13:00.445937 4400377280 params.py:247] model.antecedent_feedforward.hidden_dims = 1500\n",
      "I0826 14:13:00.446927 4400377280 params.py:247] model.antecedent_feedforward.activations = relu\n",
      "I0826 14:13:00.447562 4400377280 params.py:247] type = relu\n",
      "I0826 14:13:00.448612 4400377280 params.py:247] model.antecedent_feedforward.dropout = 0.3\n",
      "I0826 14:13:00.547549 4400377280 params.py:247] model.feature_size = 20\n",
      "I0826 14:13:00.548633 4400377280 params.py:247] model.max_span_width = 30\n",
      "I0826 14:13:00.549273 4400377280 params.py:247] model.spans_per_word = 0.4\n",
      "I0826 14:13:00.550128 4400377280 params.py:247] model.max_antecedents = 50\n",
      "I0826 14:13:00.550777 4400377280 params.py:247] model.coarse_to_fine = True\n",
      "I0826 14:13:00.551660 4400377280 params.py:247] model.inference_order = 2\n",
      "I0826 14:13:00.553008 4400377280 params.py:247] model.lexical_dropout = 0.2\n",
      "I0826 14:13:00.555062 4400377280 params.py:247] model.initializer.regexes.0.1.type = xavier_normal\n",
      "I0826 14:13:00.556101 4400377280 params.py:247] model.initializer.regexes.0.1.gain = 1.0\n",
      "I0826 14:13:00.557051 4400377280 params.py:247] model.initializer.regexes.1.1.type = xavier_normal\n",
      "I0826 14:13:00.558422 4400377280 params.py:247] model.initializer.regexes.1.1.gain = 1.0\n",
      "I0826 14:13:00.559499 4400377280 params.py:247] model.initializer.regexes.2.1.type = xavier_normal\n",
      "I0826 14:13:00.560635 4400377280 params.py:247] model.initializer.regexes.2.1.gain = 1.0\n",
      "I0826 14:13:00.561814 4400377280 params.py:247] model.initializer.regexes.3.1.type = xavier_normal\n",
      "I0826 14:13:00.562808 4400377280 params.py:247] model.initializer.regexes.3.1.gain = 1.0\n",
      "I0826 14:13:00.564115 4400377280 params.py:247] model.initializer.regexes.4.1.type = xavier_normal\n",
      "I0826 14:13:00.565442 4400377280 params.py:247] model.initializer.regexes.4.1.gain = 1.0\n",
      "I0826 14:13:00.566449 4400377280 params.py:247] model.initializer.regexes.5.1.type = xavier_normal\n",
      "I0826 14:13:00.567477 4400377280 params.py:247] model.initializer.regexes.5.1.gain = 1.0\n",
      "I0826 14:13:00.568439 4400377280 params.py:247] model.initializer.regexes.6.1.type = orthogonal\n",
      "I0826 14:13:00.569281 4400377280 params.py:247] model.initializer.regexes.6.1.gain = 1.0\n",
      "I0826 14:13:00.570091 4400377280 params.py:247] model.initializer.prevent_regexes = None\n",
      "I0826 14:13:00.630884 4400377280 initializers.py:471] Initializing parameters\n",
      "I0826 14:13:00.657210 4400377280 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0826 14:13:00.683913 4400377280 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0826 14:13:00.696078 4400377280 initializers.py:481] Initializing _mention_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0826 14:13:00.696963 4400377280 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0826 14:13:00.767477 4400377280 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0826 14:13:00.780305 4400377280 initializers.py:481] Initializing _antecedent_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0826 14:13:00.781078 4400377280 initializers.py:481] Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
      "I0826 14:13:00.781968 4400377280 initializers.py:481] Initializing _distance_embedding.weight using _distance_embedding.weight initializer\n",
      "I0826 14:13:00.782602 4400377280 initializers.py:481] Initializing _coarse2fine_scorer.weight using .*scorer.*weight initializer\n",
      "I0826 14:13:00.830594 4400377280 initializers.py:481] Initializing _span_updating_gated_sum._gate.weight using .*_span_updating_gated_sum.*weight initializer\n",
      "W0826 14:13:00.831877 4400377280 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "W0826 14:13:00.832497 4400377280 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
      "I0826 14:13:00.832972 4400377280 initializers.py:490] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0826 14:13:00.833603 4400377280 initializers.py:496]    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "I0826 14:13:00.834019 4400377280 initializers.py:496]    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "I0826 14:13:00.834589 4400377280 initializers.py:496]    _antecedent_scorer._module.bias\n",
      "I0826 14:13:00.835100 4400377280 initializers.py:496]    _attentive_span_extractor._global_attention._module.bias\n",
      "I0826 14:13:00.835888 4400377280 initializers.py:496]    _attentive_span_extractor._global_attention._module.weight\n",
      "I0826 14:13:00.836336 4400377280 initializers.py:496]    _coarse2fine_scorer.bias\n",
      "I0826 14:13:00.837136 4400377280 initializers.py:496]    _mention_feedforward._module._linear_layers.0.bias\n",
      "I0826 14:13:00.837568 4400377280 initializers.py:496]    _mention_feedforward._module._linear_layers.1.bias\n",
      "I0826 14:13:00.838328 4400377280 initializers.py:496]    _mention_scorer._module.bias\n",
      "I0826 14:13:00.838943 4400377280 initializers.py:496]    _span_updating_gated_sum._gate.bias\n",
      "I0826 14:13:00.839728 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "I0826 14:13:00.840261 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:00.841082 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "I0826 14:13:00.841542 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0826 14:13:00.842160 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "I0826 14:13:00.842633 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0826 14:13:00.843231 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I0826 14:13:00.843636 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0826 14:13:00.844112 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "I0826 14:13:00.844691 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0826 14:13:00.845116 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0826 14:13:00.845577 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0826 14:13:00.846203 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0826 14:13:00.846791 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0826 14:13:00.847383 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0826 14:13:00.847800 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0826 14:13:00.848261 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "I0826 14:13:00.848869 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "I0826 14:13:00.849296 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0826 14:13:00.849746 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0826 14:13:00.850353 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0826 14:13:00.850888 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0826 14:13:00.851562 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0826 14:13:00.852111 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0826 14:13:00.942871 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0826 14:13:00.943990 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0826 14:13:00.944698 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0826 14:13:00.945294 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0826 14:13:00.946050 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0826 14:13:00.947350 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0826 14:13:00.948704 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0826 14:13:00.949753 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0826 14:13:00.950571 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0826 14:13:00.951457 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0826 14:13:00.952446 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0826 14:13:00.953057 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0826 14:13:00.953881 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0826 14:13:00.954522 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0826 14:13:00.955227 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0826 14:13:00.955754 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0826 14:13:00.956535 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0826 14:13:00.957093 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0826 14:13:00.957840 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0826 14:13:00.958424 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0826 14:13:00.959181 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0826 14:13:00.959704 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0826 14:13:00.960443 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0826 14:13:00.961048 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:00.961760 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0826 14:13:00.962289 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0826 14:13:00.962952 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0826 14:13:00.963462 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0826 14:13:00.964326 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0826 14:13:00.964986 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0826 14:13:00.965595 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0826 14:13:00.966104 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0826 14:13:00.966789 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "I0826 14:13:00.967499 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0826 14:13:00.968228 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "I0826 14:13:00.968815 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0826 14:13:00.969434 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0826 14:13:00.969904 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0826 14:13:00.970629 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "I0826 14:13:00.971311 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "I0826 14:13:00.972004 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0826 14:13:00.972483 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0826 14:13:00.973120 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0826 14:13:00.973577 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0826 14:13:00.974112 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0826 14:13:00.974586 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "I0826 14:13:00.975021 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "I0826 14:13:00.975592 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "I0826 14:13:00.976022 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "I0826 14:13:00.976449 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "I0826 14:13:00.977003 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "I0826 14:13:00.977646 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "I0826 14:13:00.978176 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "I0826 14:13:00.978698 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "I0826 14:13:00.979088 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "I0826 14:13:00.979665 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "I0826 14:13:00.980083 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "I0826 14:13:00.980525 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "I0826 14:13:00.981128 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "I0826 14:13:00.981597 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "I0826 14:13:00.982170 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "I0826 14:13:00.982769 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "I0826 14:13:00.983353 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "I0826 14:13:00.984127 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "I0826 14:13:00.984838 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "I0826 14:13:00.985332 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "I0826 14:13:00.985842 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "I0826 14:13:00.986328 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "I0826 14:13:00.986863 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "I0826 14:13:00.987447 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:00.987857 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "I0826 14:13:00.988293 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "I0826 14:13:00.988752 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "I0826 14:13:00.989179 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "I0826 14:13:00.989608 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "I0826 14:13:00.990160 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "I0826 14:13:00.990796 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "I0826 14:13:00.991470 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "I0826 14:13:00.991906 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "I0826 14:13:00.992464 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "I0826 14:13:00.992901 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "I0826 14:13:00.993306 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "I0826 14:13:00.993834 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "I0826 14:13:00.994220 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "I0826 14:13:00.994687 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
      "I0826 14:13:00.995191 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
      "I0826 14:13:00.995629 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "I0826 14:13:00.996047 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "I0826 14:13:00.996562 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "I0826 14:13:00.996933 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "I0826 14:13:00.997397 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "I0826 14:13:00.997883 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "I0826 14:13:00.998266 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "I0826 14:13:00.998976 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "I0826 14:13:00.999525 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.000124 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "I0826 14:13:01.000674 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "I0826 14:13:01.001271 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "I0826 14:13:01.001904 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "I0826 14:13:01.002451 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "I0826 14:13:01.002925 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "I0826 14:13:01.003301 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "I0826 14:13:01.003909 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "I0826 14:13:01.004319 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "I0826 14:13:01.004786 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "I0826 14:13:01.005332 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "I0826 14:13:01.005877 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "I0826 14:13:01.006325 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "I0826 14:13:01.007061 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "I0826 14:13:01.007657 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.008122 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.008557 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "I0826 14:13:01.009026 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "I0826 14:13:01.009492 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "I0826 14:13:01.010096 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "I0826 14:13:01.010516 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:01.011024 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "I0826 14:13:01.011542 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "I0826 14:13:01.012015 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "I0826 14:13:01.012460 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "I0826 14:13:01.013077 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "I0826 14:13:01.013619 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "I0826 14:13:01.014192 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "I0826 14:13:01.014746 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "I0826 14:13:01.015382 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "I0826 14:13:01.123720 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.124875 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.125653 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "I0826 14:13:01.126250 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "I0826 14:13:01.127350 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "I0826 14:13:01.128525 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
      "I0826 14:13:01.129243 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
      "I0826 14:13:01.129851 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "I0826 14:13:01.130624 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "I0826 14:13:01.131298 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "I0826 14:13:01.132179 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "I0826 14:13:01.132901 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "I0826 14:13:01.133673 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "I0826 14:13:01.134287 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "I0826 14:13:01.135293 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "I0826 14:13:01.136283 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "I0826 14:13:01.137005 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.137908 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.138484 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "I0826 14:13:01.139398 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "I0826 14:13:01.140046 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "I0826 14:13:01.140958 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "I0826 14:13:01.141590 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "I0826 14:13:01.142480 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "I0826 14:13:01.143131 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "I0826 14:13:01.144251 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "I0826 14:13:01.145081 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "I0826 14:13:01.145826 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "I0826 14:13:01.146608 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "I0826 14:13:01.147333 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "I0826 14:13:01.148103 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "I0826 14:13:01.148715 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "I0826 14:13:01.149616 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.150290 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.151218 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "I0826 14:13:01.152333 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "I0826 14:13:01.153977 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:01.154968 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "I0826 14:13:01.155852 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "I0826 14:13:01.156629 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "I0826 14:13:01.157442 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "I0826 14:13:01.158199 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "I0826 14:13:01.159149 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "I0826 14:13:01.159939 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "I0826 14:13:01.160773 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "I0826 14:13:01.161399 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "I0826 14:13:01.162293 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "I0826 14:13:01.162968 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "I0826 14:13:01.163612 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.164229 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.165010 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0826 14:13:01.165621 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "I0826 14:13:01.166702 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "I0826 14:13:01.167424 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0826 14:13:01.168452 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0826 14:13:01.169178 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0826 14:13:01.169870 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0826 14:13:01.170639 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0826 14:13:01.171220 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0826 14:13:01.171988 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0826 14:13:01.172415 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0826 14:13:01.173138 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0826 14:13:01.173607 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0826 14:13:01.174231 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0826 14:13:01.174815 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.175408 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.176007 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "I0826 14:13:01.176662 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "I0826 14:13:01.177221 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "I0826 14:13:01.177800 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "I0826 14:13:01.178317 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "I0826 14:13:01.178930 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "I0826 14:13:01.179420 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "I0826 14:13:01.180029 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "I0826 14:13:01.180525 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "I0826 14:13:01.181168 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "I0826 14:13:01.181874 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "I0826 14:13:01.182440 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "I0826 14:13:01.182960 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "I0826 14:13:01.183553 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "I0826 14:13:01.184191 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.184815 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.185523 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:01.186038 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "I0826 14:13:01.186506 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "I0826 14:13:01.186906 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "I0826 14:13:01.187514 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "I0826 14:13:01.187905 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "I0826 14:13:01.188349 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "I0826 14:13:01.188807 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "I0826 14:13:01.189373 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "I0826 14:13:01.189777 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "I0826 14:13:01.190371 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "I0826 14:13:01.190769 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "I0826 14:13:01.191263 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "I0826 14:13:01.191832 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "I0826 14:13:01.192408 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.193001 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.193485 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
      "I0826 14:13:01.194061 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "I0826 14:13:01.194575 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "I0826 14:13:01.195161 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "I0826 14:13:01.195663 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "I0826 14:13:01.196171 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "I0826 14:13:01.196636 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "I0826 14:13:01.197112 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "I0826 14:13:01.197578 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "I0826 14:13:01.198121 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "I0826 14:13:01.198548 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "I0826 14:13:01.199083 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "I0826 14:13:01.199470 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "I0826 14:13:01.200392 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "I0826 14:13:01.200952 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.201618 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.202301 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "I0826 14:13:01.202965 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "I0826 14:13:01.203603 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "I0826 14:13:01.204205 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "I0826 14:13:01.204832 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "I0826 14:13:01.205530 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "I0826 14:13:01.206203 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "I0826 14:13:01.206871 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "I0826 14:13:01.207715 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "I0826 14:13:01.208630 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "I0826 14:13:01.209233 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "I0826 14:13:01.209915 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "I0826 14:13:01.210458 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "I0826 14:13:01.210994 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "I0826 14:13:01.211466 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:01.211849 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.212431 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0826 14:13:01.212898 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0826 14:13:01.213360 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0826 14:13:01.213761 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0826 14:13:01.214177 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0826 14:13:01.214693 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0826 14:13:01.215216 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0826 14:13:01.215982 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0826 14:13:01.216505 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0826 14:13:01.217251 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0826 14:13:01.217687 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0826 14:13:01.218490 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0826 14:13:01.219056 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0826 14:13:01.219933 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "I0826 14:13:01.220407 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.220968 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.221405 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0826 14:13:01.221861 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0826 14:13:01.222342 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0826 14:13:01.222759 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "I0826 14:13:01.223198 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0826 14:13:01.224079 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "I0826 14:13:01.224610 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0826 14:13:01.225203 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0826 14:13:01.225606 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0826 14:13:01.226099 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0826 14:13:01.226650 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0826 14:13:01.227166 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0826 14:13:01.227705 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0826 14:13:01.228134 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0826 14:13:01.228549 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.229125 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.229498 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0826 14:13:01.230005 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0826 14:13:01.230494 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0826 14:13:01.231003 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0826 14:13:01.231602 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0826 14:13:01.232341 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0826 14:13:01.232938 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0826 14:13:01.233580 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0826 14:13:01.233983 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0826 14:13:01.234529 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0826 14:13:01.235126 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0826 14:13:01.235730 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0826 14:13:01.236325 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0826 14:13:01.236764 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:01.237205 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.237718 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.238138 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0826 14:13:01.238559 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0826 14:13:01.239141 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0826 14:13:01.239566 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0826 14:13:01.240179 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0826 14:13:01.240920 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0826 14:13:01.241384 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0826 14:13:01.241979 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0826 14:13:01.242388 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0826 14:13:01.242889 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0826 14:13:01.243440 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0826 14:13:01.243919 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "I0826 14:13:01.244324 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0826 14:13:01.244920 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "I0826 14:13:01.245359 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.245826 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.246432 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0826 14:13:01.246897 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0826 14:13:01.247533 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0826 14:13:01.248158 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0826 14:13:01.248825 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0826 14:13:01.249476 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "I0826 14:13:01.250127 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0826 14:13:01.250573 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "I0826 14:13:01.251276 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0826 14:13:01.251918 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0826 14:13:01.252609 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0826 14:13:01.253144 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0826 14:13:01.253736 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0826 14:13:01.254192 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0826 14:13:01.254559 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.255136 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.255608 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0826 14:13:01.256278 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0826 14:13:01.256683 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0826 14:13:01.257122 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0826 14:13:01.257642 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0826 14:13:01.258096 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0826 14:13:01.258538 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0826 14:13:01.259096 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0826 14:13:01.259538 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0826 14:13:01.259995 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0826 14:13:01.260537 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0826 14:13:01.260971 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:01.261391 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0826 14:13:01.261936 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0826 14:13:01.262315 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0826 14:13:01.262666 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0826 14:13:01.263247 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0826 14:13:01.263753 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0826 14:13:01.264404 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0826 14:13:01.264899 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0826 14:13:01.265600 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0826 14:13:01.266115 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0826 14:13:01.266685 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0826 14:13:01.267078 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0826 14:13:01.267498 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0826 14:13:01.268059 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "I0826 14:13:01.268624 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0826 14:13:01.269446 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "I0826 14:13:01.270043 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0826 14:13:01.270607 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0826 14:13:01.271064 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "I0826 14:13:01.271518 4400377280 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "I0826 14:13:01.280538 4400377280 embedding.py:256] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0826 14:13:01.281326 4400377280 embedding.py:256] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0826 14:13:04.784048 4400377280 params.py:247] validation_dataset_reader.type = coref\n",
      "I0826 14:13:04.785782 4400377280 params.py:247] validation_dataset_reader.lazy = False\n",
      "I0826 14:13:04.786767 4400377280 params.py:247] validation_dataset_reader.cache_directory = None\n",
      "I0826 14:13:04.787271 4400377280 params.py:247] validation_dataset_reader.max_instances = None\n",
      "I0826 14:13:04.788084 4400377280 params.py:247] validation_dataset_reader.manual_distributed_sharding = False\n",
      "I0826 14:13:04.788707 4400377280 params.py:247] validation_dataset_reader.manual_multi_process_sharding = False\n",
      "I0826 14:13:04.789713 4400377280 params.py:247] validation_dataset_reader.max_span_width = 30\n",
      "I0826 14:13:04.790625 4400377280 params.py:247] validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0826 14:13:04.791764 4400377280 params.py:247] validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0826 14:13:04.792666 4400377280 params.py:247] validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0826 14:13:04.793323 4400377280 params.py:247] validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0826 14:13:04.793982 4400377280 params.py:247] validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0826 14:13:05.820908 4400377280 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0826 14:13:05.822207 4400377280 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0826 14:13:05.823786 4400377280 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0826 14:13:09.875009 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0826 14:13:09.875919 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0826 14:13:09.876558 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0826 14:13:09.877391 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0826 14:13:10.816048 4400377280 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 14:13:10.817348 4400377280 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0826 14:13:10.818380 4400377280 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0826 14:13:15.708942 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0826 14:13:15.709707 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0826 14:13:15.710232 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0826 14:13:15.710973 4400377280 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0826 14:13:15.735736 4400377280 params.py:247] validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0826 14:13:15.736326 4400377280 params.py:247] validation_dataset_reader.max_sentences = None\n",
      "I0826 14:13:15.736756 4400377280 params.py:247] validation_dataset_reader.remove_singleton_clusters = False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "pass\n",
    "\n",
    "E_PHASE = \"Exploitation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    output = predictor.predict(document = sentence)\n",
    "    return output, output['document'], output['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coref_pos(test_case, document):\n",
    "    if \"He\" in test_case:\n",
    "        index_he = document.index(\"He\")\n",
    "        return [index_he, index_he]\n",
    "    \n",
    "    elif \"She\" in test_case:\n",
    "        index_she = document.index(\"She\")\n",
    "        return [index_she, index_she]\n",
    "    else:\n",
    "        return [-1, -1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coref_ground_truth(test_case, document=None):\n",
    "    if document == None:\n",
    "        _, document, clusters = predict_clusters(test_case)\n",
    "    coref_pos = get_coref_pos(test_case, document)\n",
    "    oc_pos = [0, 1]\n",
    "    if (\"construction\" in test_case):\n",
    "        oc_pos = [0, 2]\n",
    "    return [oc_pos, coref_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_error(test_case):\n",
    "    _, document, clusters = predict_clusters(test_case)\n",
    "    \n",
    "    if (len(clusters) < 1): return True\n",
    "    \n",
    "    gt_oc = extract_coref_ground_truth(test_case)\n",
    "#     print(gt_oc)\n",
    "    return not gt_oc == clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/'+ E_PHASE +'/unique_input1_set.pickle', 'rb') as handle:\n",
    "    unique_input1_set = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive errors for Exploitation\n",
      "2692 5898\n"
     ]
    }
   ],
   "source": [
    "TEST_CASES = 0\n",
    "PRED_ERRORS = 0\n",
    "for test_input in list(unique_input1_set):\n",
    "    test_input1, test_input2 = test_input, test_input.replace('He', 'She')\n",
    "    \n",
    "#     _, document, clusters = predict_clusters(test_input1)\n",
    "#     print(clusters)\n",
    "#     print(get_pred_error(test_input1))\n",
    "#     print(test_input1)\n",
    "#     print()\n",
    "#     _, document, clusters = predict_clusters(test_input2)\n",
    "#     print(clusters)\n",
    "#     print(get_pred_error(test_input2))\n",
    "#     print(test_input2)\n",
    "#     print('------------------------------')\n",
    "    TEST_CASES += 1\n",
    "    if get_pred_error(test_input1):\n",
    "        PRED_ERRORS += 1\n",
    "        \n",
    "    TEST_CASES += 1\n",
    "    if get_pred_error(test_input2):\n",
    "        PRED_ERRORS += 1\n",
    "\n",
    "print(\"Predictive errors for \" + E_PHASE)\n",
    "print (PRED_ERRORS, TEST_CASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
