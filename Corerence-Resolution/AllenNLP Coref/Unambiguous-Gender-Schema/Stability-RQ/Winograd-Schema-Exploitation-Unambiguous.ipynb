{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:08.098265 4617412032 file_utils.py:39] PyTorch version 1.5.1 available.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "I0825 20:49:15.237124 4617412032 file_utils.py:339] checking cache for https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz at /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0825 20:49:15.238368 4617412032 file_utils.py:340] waiting to acquire lock on /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0825 20:49:15.240149 4617412032 filelock.py:274] Lock 4525797840 acquired on /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174.lock\n",
      "I0825 20:49:15.241701 4617412032 file_utils.py:343] cache of https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz is up-to-date\n",
      "I0825 20:49:15.242901 4617412032 filelock.py:318] Lock 4525797840 released on /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174.lock\n",
      "I0825 20:49:15.245540 4617412032 archival.py:164] loading archive file https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz from cache at /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0825 20:49:15.248728 4617412032 archival.py:171] extracting archive file /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174 to temp dir /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp3dm5kzc8\n",
      "I0825 20:49:29.737164 4617412032 params.py:247] type = from_instances\n",
      "I0825 20:49:29.738610 4617412032 vocabulary.py:321] Loading token dictionary from /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp3dm5kzc8/vocabulary.\n",
      "I0825 20:49:29.739952 4617412032 filelock.py:274] Lock 5741200664 acquired on /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp3dm5kzc8/vocabulary/.lock\n",
      "I0825 20:49:29.741447 4617412032 filelock.py:318] Lock 5741200664 released on /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp3dm5kzc8/vocabulary/.lock\n",
      "I0825 20:49:29.743319 4617412032 params.py:247] model.type = coref\n",
      "I0825 20:49:29.744994 4617412032 params.py:247] model.regularizer = None\n",
      "I0825 20:49:29.746412 4617412032 params.py:247] model.text_field_embedder.type = basic\n",
      "I0825 20:49:29.748346 4617412032 params.py:247] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "I0825 20:49:29.749870 4617412032 params.py:247] model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0825 20:49:29.751855 4617412032 params.py:247] model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "I0825 20:49:29.753158 4617412032 params.py:247] model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
      "I0825 20:49:30.903084 4617412032 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0825 20:49:30.905225 4617412032 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0825 20:49:30.996580 4617412032 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/SpanBERT/spanbert-large-cased/pytorch_model.bin from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c8041a5fa85cc2725744bab27890d410f0f47dde0c9f1152de9a1b1e5df049e.d1ce6dff7f84348ad7c77a33a9a6e8751099db9c9d609ac7752e61804befe4da\n",
      "I0825 20:49:37.659705 4617412032 modeling_utils.py:741] Weights of BertModel not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "I0825 20:49:38.789047 4617412032 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0825 20:49:38.790796 4617412032 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0825 20:49:38.792088 4617412032 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0825 20:49:43.032711 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0825 20:49:43.033845 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0825 20:49:43.034888 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0825 20:49:43.035565 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0825 20:49:44.049525 4617412032 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0825 20:49:44.051157 4617412032 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:44.052346 4617412032 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0825 20:49:48.309188 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0825 20:49:48.310345 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0825 20:49:48.311581 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0825 20:49:48.312582 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0825 20:49:48.368762 4617412032 params.py:247] model.context_layer.type = pass_through\n",
      "I0825 20:49:48.370106 4617412032 params.py:247] model.context_layer.input_dim = 1024\n",
      "I0825 20:49:48.372730 4617412032 params.py:247] model.mention_feedforward.input_dim = 3092\n",
      "I0825 20:49:48.374128 4617412032 params.py:247] model.mention_feedforward.num_layers = 2\n",
      "I0825 20:49:48.375649 4617412032 params.py:247] model.mention_feedforward.hidden_dims = 1500\n",
      "I0825 20:49:48.376810 4617412032 params.py:247] model.mention_feedforward.activations = relu\n",
      "I0825 20:49:48.378540 4617412032 params.py:247] type = relu\n",
      "I0825 20:49:48.380191 4617412032 params.py:247] model.mention_feedforward.dropout = 0.3\n",
      "I0825 20:49:48.450221 4617412032 params.py:247] model.antecedent_feedforward.input_dim = 9296\n",
      "I0825 20:49:48.451228 4617412032 params.py:247] model.antecedent_feedforward.num_layers = 2\n",
      "I0825 20:49:48.451988 4617412032 params.py:247] model.antecedent_feedforward.hidden_dims = 1500\n",
      "I0825 20:49:48.453071 4617412032 params.py:247] model.antecedent_feedforward.activations = relu\n",
      "I0825 20:49:48.454889 4617412032 params.py:247] type = relu\n",
      "I0825 20:49:48.456304 4617412032 params.py:247] model.antecedent_feedforward.dropout = 0.3\n",
      "I0825 20:49:48.617720 4617412032 params.py:247] model.feature_size = 20\n",
      "I0825 20:49:48.619164 4617412032 params.py:247] model.max_span_width = 30\n",
      "I0825 20:49:48.620203 4617412032 params.py:247] model.spans_per_word = 0.4\n",
      "I0825 20:49:48.621103 4617412032 params.py:247] model.max_antecedents = 50\n",
      "I0825 20:49:48.622372 4617412032 params.py:247] model.coarse_to_fine = True\n",
      "I0825 20:49:48.623633 4617412032 params.py:247] model.inference_order = 2\n",
      "I0825 20:49:48.624799 4617412032 params.py:247] model.lexical_dropout = 0.2\n",
      "I0825 20:49:48.627043 4617412032 params.py:247] model.initializer.regexes.0.1.type = xavier_normal\n",
      "I0825 20:49:48.628351 4617412032 params.py:247] model.initializer.regexes.0.1.gain = 1.0\n",
      "I0825 20:49:48.629930 4617412032 params.py:247] model.initializer.regexes.1.1.type = xavier_normal\n",
      "I0825 20:49:48.631373 4617412032 params.py:247] model.initializer.regexes.1.1.gain = 1.0\n",
      "I0825 20:49:48.633784 4617412032 params.py:247] model.initializer.regexes.2.1.type = xavier_normal\n",
      "I0825 20:49:48.635561 4617412032 params.py:247] model.initializer.regexes.2.1.gain = 1.0\n",
      "I0825 20:49:48.637063 4617412032 params.py:247] model.initializer.regexes.3.1.type = xavier_normal\n",
      "I0825 20:49:48.638369 4617412032 params.py:247] model.initializer.regexes.3.1.gain = 1.0\n",
      "I0825 20:49:48.639643 4617412032 params.py:247] model.initializer.regexes.4.1.type = xavier_normal\n",
      "I0825 20:49:48.640856 4617412032 params.py:247] model.initializer.regexes.4.1.gain = 1.0\n",
      "I0825 20:49:48.642182 4617412032 params.py:247] model.initializer.regexes.5.1.type = xavier_normal\n",
      "I0825 20:49:48.643666 4617412032 params.py:247] model.initializer.regexes.5.1.gain = 1.0\n",
      "I0825 20:49:48.644870 4617412032 params.py:247] model.initializer.regexes.6.1.type = orthogonal\n",
      "I0825 20:49:48.646128 4617412032 params.py:247] model.initializer.regexes.6.1.gain = 1.0\n",
      "I0825 20:49:48.647279 4617412032 params.py:247] model.initializer.prevent_regexes = None\n",
      "I0825 20:49:48.747159 4617412032 initializers.py:471] Initializing parameters\n",
      "I0825 20:49:48.789606 4617412032 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0825 20:49:48.829921 4617412032 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0825 20:49:48.851863 4617412032 initializers.py:481] Initializing _mention_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0825 20:49:48.853340 4617412032 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0825 20:49:48.966912 4617412032 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0825 20:49:48.993862 4617412032 initializers.py:481] Initializing _antecedent_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0825 20:49:48.995211 4617412032 initializers.py:481] Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
      "I0825 20:49:48.996741 4617412032 initializers.py:481] Initializing _distance_embedding.weight using _distance_embedding.weight initializer\n",
      "I0825 20:49:48.999387 4617412032 initializers.py:481] Initializing _coarse2fine_scorer.weight using .*scorer.*weight initializer\n",
      "I0825 20:49:49.084467 4617412032 initializers.py:481] Initializing _span_updating_gated_sum._gate.weight using .*_span_updating_gated_sum.*weight initializer\n",
      "W0825 20:49:49.085868 4617412032 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "W0825 20:49:49.086917 4617412032 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
      "I0825 20:49:49.087689 4617412032 initializers.py:490] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0825 20:49:49.089242 4617412032 initializers.py:496]    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "I0825 20:49:49.090723 4617412032 initializers.py:496]    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "I0825 20:49:49.092395 4617412032 initializers.py:496]    _antecedent_scorer._module.bias\n",
      "I0825 20:49:49.093605 4617412032 initializers.py:496]    _attentive_span_extractor._global_attention._module.bias\n",
      "I0825 20:49:49.095237 4617412032 initializers.py:496]    _attentive_span_extractor._global_attention._module.weight\n",
      "I0825 20:49:49.096446 4617412032 initializers.py:496]    _coarse2fine_scorer.bias\n",
      "I0825 20:49:49.097539 4617412032 initializers.py:496]    _mention_feedforward._module._linear_layers.0.bias\n",
      "I0825 20:49:49.098989 4617412032 initializers.py:496]    _mention_feedforward._module._linear_layers.1.bias\n",
      "I0825 20:49:49.100221 4617412032 initializers.py:496]    _mention_scorer._module.bias\n",
      "I0825 20:49:49.101796 4617412032 initializers.py:496]    _span_updating_gated_sum._gate.bias\n",
      "I0825 20:49:49.103142 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "I0825 20:49:49.104363 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:49.105792 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "I0825 20:49:49.107115 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0825 20:49:49.108784 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "I0825 20:49:49.110008 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.112252 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.113437 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0825 20:49:49.114644 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "I0825 20:49:49.115797 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0825 20:49:49.117582 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0825 20:49:49.119133 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0825 20:49:49.120604 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0825 20:49:49.121984 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0825 20:49:49.123220 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0825 20:49:49.124529 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0825 20:49:49.126787 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "I0825 20:49:49.128694 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "I0825 20:49:49.130379 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0825 20:49:49.132037 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0825 20:49:49.133072 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0825 20:49:49.134026 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.134972 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.137423 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0825 20:49:49.138956 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0825 20:49:49.140167 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0825 20:49:49.141342 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0825 20:49:49.142577 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0825 20:49:49.257364 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0825 20:49:49.258486 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0825 20:49:49.259829 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0825 20:49:49.261030 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0825 20:49:49.262353 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0825 20:49:49.263391 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0825 20:49:49.264227 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0825 20:49:49.265423 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0825 20:49:49.267030 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0825 20:49:49.268332 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.269570 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.270688 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0825 20:49:49.271703 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0825 20:49:49.272824 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0825 20:49:49.274070 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0825 20:49:49.275440 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0825 20:49:49.276878 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0825 20:49:49.278100 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0825 20:49:49.279401 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0825 20:49:49.280502 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:49.282544 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0825 20:49:49.283977 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0825 20:49:49.285695 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0825 20:49:49.286914 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0825 20:49:49.288420 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0825 20:49:49.289307 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.290277 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.291706 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0825 20:49:49.292791 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "I0825 20:49:49.294546 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0825 20:49:49.295660 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "I0825 20:49:49.296648 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0825 20:49:49.298408 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0825 20:49:49.299695 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0825 20:49:49.301480 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "I0825 20:49:49.303173 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "I0825 20:49:49.304634 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0825 20:49:49.306014 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0825 20:49:49.307469 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0825 20:49:49.308660 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0825 20:49:49.310861 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0825 20:49:49.312089 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.313230 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.315194 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "I0825 20:49:49.316510 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "I0825 20:49:49.317568 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "I0825 20:49:49.318896 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "I0825 20:49:49.320949 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "I0825 20:49:49.322179 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "I0825 20:49:49.323696 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "I0825 20:49:49.324855 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "I0825 20:49:49.326278 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "I0825 20:49:49.327515 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "I0825 20:49:49.328993 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "I0825 20:49:49.330153 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "I0825 20:49:49.332562 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "I0825 20:49:49.333760 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "I0825 20:49:49.335258 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.336212 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.337126 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "I0825 20:49:49.338408 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "I0825 20:49:49.339491 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "I0825 20:49:49.341242 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "I0825 20:49:49.342390 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "I0825 20:49:49.343710 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "I0825 20:49:49.344883 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:49.345917 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "I0825 20:49:49.348513 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "I0825 20:49:49.349969 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "I0825 20:49:49.351706 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "I0825 20:49:49.352974 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "I0825 20:49:49.354130 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "I0825 20:49:49.355226 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "I0825 20:49:49.356562 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.357838 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.359592 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "I0825 20:49:49.360997 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "I0825 20:49:49.362349 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "I0825 20:49:49.363279 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "I0825 20:49:49.364253 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "I0825 20:49:49.365607 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
      "I0825 20:49:49.368178 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
      "I0825 20:49:49.369601 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "I0825 20:49:49.370743 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "I0825 20:49:49.371876 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "I0825 20:49:49.373006 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "I0825 20:49:49.374168 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "I0825 20:49:49.376584 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "I0825 20:49:49.377912 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "I0825 20:49:49.379446 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.381025 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.382751 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "I0825 20:49:49.383984 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "I0825 20:49:49.385890 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "I0825 20:49:49.387124 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "I0825 20:49:49.388596 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "I0825 20:49:49.389762 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "I0825 20:49:49.391020 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "I0825 20:49:49.392183 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "I0825 20:49:49.394454 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "I0825 20:49:49.395822 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "I0825 20:49:49.396984 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "I0825 20:49:49.399644 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "I0825 20:49:49.401147 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "I0825 20:49:49.402661 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "I0825 20:49:49.403847 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.405365 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.406503 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "I0825 20:49:49.407856 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "I0825 20:49:49.409005 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "I0825 20:49:49.410389 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "I0825 20:49:49.411502 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:49.412502 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "I0825 20:49:49.413414 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "I0825 20:49:49.415481 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "I0825 20:49:49.416723 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "I0825 20:49:49.418853 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "I0825 20:49:49.420128 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "I0825 20:49:49.421573 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "I0825 20:49:49.422704 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "I0825 20:49:49.424485 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "I0825 20:49:49.425497 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.426964 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.428270 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "I0825 20:49:49.429363 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "I0825 20:49:49.430948 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "I0825 20:49:49.432237 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
      "I0825 20:49:49.433304 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
      "I0825 20:49:49.435120 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "I0825 20:49:49.436465 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "I0825 20:49:49.438633 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "I0825 20:49:49.439843 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "I0825 20:49:49.440990 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "I0825 20:49:49.442022 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "I0825 20:49:49.444087 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "I0825 20:49:49.445406 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "I0825 20:49:49.447211 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "I0825 20:49:49.448652 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.450288 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.451467 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "I0825 20:49:49.452716 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "I0825 20:49:49.454007 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "I0825 20:49:49.455400 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "I0825 20:49:49.456407 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "I0825 20:49:49.457625 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "I0825 20:49:49.458902 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "I0825 20:49:49.460181 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "I0825 20:49:49.461230 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "I0825 20:49:49.462533 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "I0825 20:49:49.465085 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "I0825 20:49:49.466856 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "I0825 20:49:49.468036 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "I0825 20:49:49.468994 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "I0825 20:49:49.469836 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.470909 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.471744 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "I0825 20:49:49.472553 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "I0825 20:49:49.473433 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:49.474317 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "I0825 20:49:49.475330 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "I0825 20:49:49.476161 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "I0825 20:49:49.477364 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "I0825 20:49:49.478320 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "I0825 20:49:49.479151 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "I0825 20:49:49.480268 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "I0825 20:49:49.481270 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "I0825 20:49:49.482500 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "I0825 20:49:49.484498 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "I0825 20:49:49.485649 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "I0825 20:49:49.487980 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.489022 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.489933 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0825 20:49:49.491271 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "I0825 20:49:49.492401 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "I0825 20:49:49.493757 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0825 20:49:49.494907 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0825 20:49:49.497612 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0825 20:49:49.498740 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0825 20:49:49.500148 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0825 20:49:49.501493 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0825 20:49:49.504508 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0825 20:49:49.506511 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0825 20:49:49.507807 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0825 20:49:49.509970 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0825 20:49:49.512224 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0825 20:49:49.513182 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.515119 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.516312 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "I0825 20:49:49.517407 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "I0825 20:49:49.518634 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "I0825 20:49:49.520282 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "I0825 20:49:49.521864 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "I0825 20:49:49.523582 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "I0825 20:49:49.525716 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "I0825 20:49:49.528849 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "I0825 20:49:49.530170 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "I0825 20:49:49.532142 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "I0825 20:49:49.533609 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "I0825 20:49:49.534904 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "I0825 20:49:49.535794 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "I0825 20:49:49.536623 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "I0825 20:49:49.537781 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.538926 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.540111 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:49.541074 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "I0825 20:49:49.542273 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "I0825 20:49:49.543511 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "I0825 20:49:49.544620 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "I0825 20:49:49.545612 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "I0825 20:49:49.546432 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "I0825 20:49:49.547866 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "I0825 20:49:49.548993 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "I0825 20:49:49.550199 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "I0825 20:49:49.551311 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "I0825 20:49:49.552278 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "I0825 20:49:49.553015 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "I0825 20:49:49.554172 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "I0825 20:49:49.555063 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.556041 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.557367 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
      "I0825 20:49:49.558492 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "I0825 20:49:49.560423 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "I0825 20:49:49.561850 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "I0825 20:49:49.562961 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "I0825 20:49:49.564373 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "I0825 20:49:49.565818 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "I0825 20:49:49.567392 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "I0825 20:49:49.568688 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "I0825 20:49:49.570250 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "I0825 20:49:49.571496 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "I0825 20:49:49.572749 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "I0825 20:49:49.573825 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "I0825 20:49:49.575203 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "I0825 20:49:49.576314 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.577767 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.579130 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "I0825 20:49:49.581808 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "I0825 20:49:49.583303 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "I0825 20:49:49.584810 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "I0825 20:49:49.585978 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "I0825 20:49:49.586847 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "I0825 20:49:49.587852 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "I0825 20:49:49.588901 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "I0825 20:49:49.589980 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "I0825 20:49:49.591155 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "I0825 20:49:49.592103 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "I0825 20:49:49.593068 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "I0825 20:49:49.594238 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "I0825 20:49:49.595488 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "I0825 20:49:49.597301 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:49.598883 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.600291 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0825 20:49:49.601474 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0825 20:49:49.602686 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0825 20:49:49.603851 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0825 20:49:49.604974 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0825 20:49:49.606497 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0825 20:49:49.607948 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0825 20:49:49.610152 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0825 20:49:49.611407 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0825 20:49:49.612378 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0825 20:49:49.613373 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0825 20:49:49.614714 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0825 20:49:49.616607 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0825 20:49:49.617841 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "I0825 20:49:49.618987 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.620471 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.621733 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0825 20:49:49.623167 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0825 20:49:49.624321 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0825 20:49:49.626493 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "I0825 20:49:49.627864 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0825 20:49:49.629226 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "I0825 20:49:49.630322 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0825 20:49:49.632788 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0825 20:49:49.633987 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0825 20:49:49.635936 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0825 20:49:49.637192 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0825 20:49:49.640122 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0825 20:49:49.641695 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0825 20:49:49.643262 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0825 20:49:49.644786 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.646492 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.648934 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0825 20:49:49.650656 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0825 20:49:49.651919 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0825 20:49:49.653100 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0825 20:49:49.654447 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0825 20:49:49.655619 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0825 20:49:49.656929 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0825 20:49:49.657931 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0825 20:49:49.658987 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0825 20:49:49.660199 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0825 20:49:49.661916 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0825 20:49:49.664264 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0825 20:49:49.665868 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0825 20:49:49.667683 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:49.669064 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.670229 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.671164 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0825 20:49:49.672407 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0825 20:49:49.673671 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0825 20:49:49.674843 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0825 20:49:49.675993 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0825 20:49:49.677347 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0825 20:49:49.678745 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0825 20:49:49.679850 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0825 20:49:49.680943 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0825 20:49:49.683171 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0825 20:49:49.684327 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0825 20:49:49.685946 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "I0825 20:49:49.687058 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0825 20:49:49.688193 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "I0825 20:49:49.689285 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.690753 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.692103 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0825 20:49:49.694278 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0825 20:49:49.695474 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0825 20:49:49.696496 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0825 20:49:49.697935 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0825 20:49:49.699184 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "I0825 20:49:49.700486 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0825 20:49:49.702705 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "I0825 20:49:49.704139 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0825 20:49:49.705388 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0825 20:49:49.706797 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0825 20:49:49.707988 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0825 20:49:49.709110 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0825 20:49:49.711112 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0825 20:49:49.712664 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.713875 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.714998 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0825 20:49:49.716532 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0825 20:49:49.717695 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0825 20:49:49.718962 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0825 20:49:49.720581 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0825 20:49:49.721678 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0825 20:49:49.723086 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0825 20:49:49.724211 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0825 20:49:49.725377 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0825 20:49:49.726555 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0825 20:49:49.727804 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0825 20:49:49.729235 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:49.731564 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0825 20:49:49.732994 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0825 20:49:49.734237 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0825 20:49:49.735382 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0825 20:49:49.736546 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0825 20:49:49.738340 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0825 20:49:49.739964 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0825 20:49:49.741084 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0825 20:49:49.742490 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0825 20:49:49.744019 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0825 20:49:49.745043 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0825 20:49:49.747585 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0825 20:49:49.749049 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0825 20:49:49.750330 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "I0825 20:49:49.751470 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0825 20:49:49.753274 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "I0825 20:49:49.754737 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0825 20:49:49.756184 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0825 20:49:49.757140 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "I0825 20:49:49.758142 4617412032 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "I0825 20:49:49.780210 4617412032 embedding.py:256] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0825 20:49:49.781554 4617412032 embedding.py:256] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0825 20:49:52.321995 4617412032 params.py:247] validation_dataset_reader.type = coref\n",
      "I0825 20:49:52.323589 4617412032 params.py:247] validation_dataset_reader.lazy = False\n",
      "I0825 20:49:52.324705 4617412032 params.py:247] validation_dataset_reader.cache_directory = None\n",
      "I0825 20:49:52.325716 4617412032 params.py:247] validation_dataset_reader.max_instances = None\n",
      "I0825 20:49:52.326928 4617412032 params.py:247] validation_dataset_reader.manual_distributed_sharding = False\n",
      "I0825 20:49:52.328820 4617412032 params.py:247] validation_dataset_reader.manual_multi_process_sharding = False\n",
      "I0825 20:49:52.330281 4617412032 params.py:247] validation_dataset_reader.max_span_width = 30\n",
      "I0825 20:49:52.332159 4617412032 params.py:247] validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0825 20:49:52.333601 4617412032 params.py:247] validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0825 20:49:52.335415 4617412032 params.py:247] validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0825 20:49:52.336690 4617412032 params.py:247] validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0825 20:49:52.338568 4617412032 params.py:247] validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0825 20:49:53.430948 4617412032 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0825 20:49:53.432651 4617412032 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0825 20:49:53.433748 4617412032 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0825 20:49:57.678939 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0825 20:49:57.680140 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0825 20:49:57.681221 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0825 20:49:57.682232 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0825 20:49:58.857796 4617412032 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:49:58.859386 4617412032 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0825 20:49:58.860863 4617412032 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0825 20:50:03.158282 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0825 20:50:03.159290 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0825 20:50:03.160386 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0825 20:50:03.161139 4617412032 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0825 20:50:03.196983 4617412032 params.py:247] validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0825 20:50:03.198138 4617412032 params.py:247] validation_dataset_reader.max_sentences = None\n",
      "I0825 20:50:03.199935 4617412032 params.py:247] validation_dataset_reader.remove_singleton_clusters = False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked', 'disclosed to', 'told'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified', 'spoke with'] ##  pronoun2 = ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    return ['He', 'She', 'They']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"about \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "\n",
    "action1 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p1[1]] = [action1] ### disclosed to\n",
    "verb_action[verb_list_p1[2]] = [action1] ### told\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action1] ### notified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spoke with \n",
    "### asked\n",
    "action2 = ['painting', 'dancing',\n",
    "           'fencing', 'gymnastics',\n",
    "           'independent cinema', 'woodworking',\n",
    "           'studying', 'horse racing',\n",
    "           'singing', 'kayaking', 'football',\n",
    "           'baseball', 'basketball', 'quizzing', \n",
    "           'gardening', 'blogging', 'board games', \n",
    "           'breadmaking', 'baking']\n",
    "verb_action[verb_list_p2[1]] = [action2]\n",
    "verb_action[verb_list_p1[0]] = [action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_verb = ['standing', 'speaking', 'talking', \n",
    "                  'passing by', 'sitting', 'sitting on the chair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    output = predictor.predict(document = sentence)\n",
    "    return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(oc1, oc2, verb, action, pronoun):\n",
    "\n",
    "\n",
    "    aux_verb = random.choice(auxiliary_verb)\n",
    "    input1 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[0] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "\n",
    "    input2 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[1] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "        \n",
    "    return input1, input2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/unique_input1_set.pickle', 'rb') as handle:\n",
    "    unique_input1_set = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'rb') as handle:\n",
    "    unique_input1_error_set = pickle.load(handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'rb') as handle:\n",
    "    occupation_pair_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_error.pickle', 'rb') as handle:\n",
    "    occupation1_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error.pickle', 'rb') as handle:\n",
    "    occupation2_error = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error.pickle', 'rb') as handle:\n",
    "    verb_error = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/Exploration/action_error.pickle', 'rb') as handle:\n",
    "    action_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'rb') as handle:\n",
    "    occupation_pair_count = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_count.pickle', 'rb') as handle:\n",
    "    occupation1_count = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count.pickle', 'rb') as handle:\n",
    "    occupation2_count = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count.pickle', 'rb') as handle:\n",
    "    verb_count = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/Exploration/action_count.pickle', 'rb') as handle:\n",
    "    action_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_dict(D):\n",
    "    return {k: v for k, v in sorted(D.items(), key=lambda item: item[1], reverse=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate_dict(error_dict, count_dict):\n",
    "    error_rate_dict = {}\n",
    "    for key in error_dict:\n",
    "        error_rate_dict[key] = error_dict[key]/count_dict[key]\n",
    "    return get_sorted_dict(error_rate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dict(error_dict, count_dict):\n",
    "    error_rate_dict = get_error_rate_dict(error_dict, count_dict)\n",
    "    \n",
    "    probability_dict = {}\n",
    "    error_rate_sum = sum(error_rate_dict.values())\n",
    "    for error_rate in error_rate_dict:\n",
    "        probability_dict[error_rate] = error_rate_dict[error_rate]/error_rate_sum\n",
    "    \n",
    "    return probability_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_random_choice(error_dict, count_dict, probablilities_dict = None):\n",
    "    if probablilities_dict == None:\n",
    "        probability_dict = get_probability_dict(error_dict, count_dict)\n",
    "    else:\n",
    "        probability_dict = probablilities_dict\n",
    "    \n",
    "    return list(probability_dict.keys())[np.random.choice(len(list(probability_dict.keys())), p=list(probability_dict.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set_exploitation = set()\n",
    "unique_input1_error_set_exploitation = set()\n",
    "\n",
    "occupation_pair_error_exploitation = {}\n",
    "\n",
    "occupation1_error_exploitation = {}\n",
    "\n",
    "occupation2_error_exploitation = {}\n",
    "\n",
    "verb_error_exploitation = {}\n",
    "\n",
    "action_error_exploitation = {}\n",
    "\n",
    "occupation_pair_count_exploitation = {}\n",
    "\n",
    "occupation1_count_exploitation = {}\n",
    "\n",
    "occupation2_count_exploitation = {}\n",
    "\n",
    "verb_count_exploitation = {}\n",
    "\n",
    "action_count_exploitation = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'farmer': 0.0625085390806401, 'carpenter': 0.06098714480742317, 'sheriff': 0.06060075801944954, 'lawyer': 0.05968181052942186, 'engineer': 0.05829855272864338, 'guard': 0.05816045903880056, 'driver': 0.05674982457266423, 'physician': 0.05202067252494221, 'janitor': 0.05169497334810186, 'laborer': 0.051234726078264764, 'mechanic': 0.05116047285206438, 'chief': 0.050904067180341184, 'manager': 0.048010572835353364, 'construction worker': 0.04485459832702527, 'CEO': 0.03371771498826515, 'supervisor': 0.0321131943811981, 'analyst': 0.03150458311644852, 'developer': 0.030072556611155408, 'accountant': 0.029800582584956326, 'technician': 0.02188874888754671, 'salesperson': 0.021555767395464775, 'cook': 0.020048371074103604, 'mover': 0.012431309037725427}\n",
      "\n",
      "{'farmer': 0.959349593495935, 'carpenter': 0.936, 'sheriff': 0.9300699300699301, 'lawyer': 0.9159663865546218, 'engineer': 0.8947368421052632, 'guard': 0.8926174496644296, 'driver': 0.8709677419354839, 'physician': 0.7983870967741935, 'janitor': 0.7933884297520661, 'laborer': 0.7863247863247863, 'mechanic': 0.7851851851851852, 'chief': 0.78125, 'manager': 0.7368421052631579, 'construction worker': 0.6884057971014492, 'CEO': 0.5174825174825175, 'supervisor': 0.4928571428571429, 'analyst': 0.4835164835164835, 'developer': 0.46153846153846156, 'accountant': 0.4573643410852713, 'technician': 0.3359375, 'salesperson': 0.3308270676691729, 'cook': 0.3076923076923077, 'mover': 0.19078947368421054}\n",
      "\n",
      "{'farmer': 6302, 'sheriff': 6013, 'lawyer': 5958, 'carpenter': 5946, 'guard': 5833, 'engineer': 5782, 'driver': 5721, 'physician': 5304, 'mechanic': 5240, 'janitor': 5141, 'chief': 5107, 'laborer': 5081, 'manager': 4836, 'construction worker': 4420, 'CEO': 3390, 'supervisor': 3224, 'analyst': 3133, 'developer': 3047, 'accountant': 2968, 'technician': 2202, 'salesperson': 2134, 'cook': 1996, 'mover': 1222}\n"
     ]
    }
   ],
   "source": [
    "oc1_probability = get_probability_dict(occupation1_error, occupation1_count)\n",
    "\n",
    "print(oc1_probability)\n",
    "print()\n",
    "\n",
    "error_rate_dict = get_error_rate_dict(occupation1_error, occupation1_count)\n",
    "print(error_rate_dict)\n",
    "print()\n",
    "\n",
    "output_dict = {}\n",
    "for i in range(100000):\n",
    "    oc1 = get_weighted_random_choice(occupation1_error, occupation1_count, probablilities_dict=oc1_probability)\n",
    "    update_dict(output_dict, oc1)\n",
    "print(get_sorted_dict(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_sentences(ITERS=3000):\n",
    "    err_count = 0\n",
    "\n",
    "    oc1_probability = get_probability_dict(occupation1_error, occupation1_count)\n",
    "    oc2_probability = get_probability_dict(occupation2_error, occupation2_count)\n",
    "\n",
    "    for i in range(ITERS):\n",
    "#         oc1 = random.choice(occupations_1)\n",
    "#         oc2 = random.choice(occupations_2)\n",
    "        oc1 = get_weighted_random_choice(occupation1_error, occupation1_count, probablilities_dict=oc1_probability)\n",
    "        oc2 = get_weighted_random_choice(occupation2_error, occupation2_count, probablilities_dict=oc2_probability)\n",
    "        verb = random.choice(list(verb_action.keys()))\n",
    "        action = random.choice(random.choice(verb_action[verb]))\n",
    "        pronoun = choose_pronoun_type(verb)\n",
    "        input1, input2 = generate_sentences(oc1, oc2, verb, action, pronoun)\n",
    "\n",
    "\n",
    "#         input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#                + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "        pred1, _ = predict_clusters(input1)\n",
    "        pred2, _ = predict_clusters(input2)\n",
    "#         pred3, _ = predict_clusters(input3)\n",
    "\n",
    "\n",
    "#         if(i % 30 == 0):\n",
    "#             print(\"Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "#             print(\"Unique inputs: \" + str(len(unique_input1_set_exploitation)))\n",
    "#             print(\"Iterations: \" + str(i))\n",
    "#             print(\"------------------------------\")\n",
    "\n",
    "        if input1 not in unique_input1_set:\n",
    "            unique_input1_set_exploitation.add(input1)\n",
    "\n",
    "        update_dict(occupation_pair_count_exploitation, (oc1, oc2))\n",
    "        update_dict(occupation1_count_exploitation, oc1)\n",
    "        update_dict(occupation2_count_exploitation, oc2)\n",
    "        update_dict(verb_count_exploitation, verb)\n",
    "        update_dict(action_count_exploitation, action)\n",
    "\n",
    "\n",
    "\n",
    "        if not (pred1 == pred2):\n",
    "#             if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#                 if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0]) ):\n",
    "    #         if(True):\n",
    "                    err_count += 1\n",
    "        \n",
    "                    \n",
    "                    if input1 not in unique_input1_error_set:\n",
    "                        unique_input1_error_set_exploitation.add(input1)\n",
    "\n",
    "    #                 print(pred1, pred2, pred3)\n",
    "    #                 print(input1)\n",
    "    #                 print(input2)\n",
    "    #                 print(input3)\n",
    "\n",
    "                    update_dict(occupation_pair_error_exploitation, (oc1, oc2))\n",
    "                    update_dict(occupation1_error_exploitation, oc1)\n",
    "                    update_dict(occupation2_error_exploitation, oc2)\n",
    "                    update_dict(verb_error_exploitation, verb)\n",
    "                    update_dict(action_error_exploitation, action)\n",
    "\n",
    "\n",
    "\n",
    "    print(err_count)\n",
    "    print(err_count/ITERS)\n",
    "    print(\"Final Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "    print(\"Final Unique inputs: \" + str(len(unique_input1_set_exploitation)))\n",
    "    \n",
    "    STABILITY_ERRS.append(unique_input1_error_set_exploitation)\n",
    "    STABILITY_TEST_CASES.append(unique_input1_set_exploitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "STABILITY_ERRS = []\n",
    "STABILITY_TEST_CASES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196\n",
      "0.732\n",
      "Final Unique errors: 2164\n",
      "Final Unique inputs: 2958\n",
      "2244\n",
      "0.748\n",
      "Final Unique errors: 2212\n",
      "Final Unique inputs: 2951\n",
      "2216\n",
      "0.7386666666666667\n",
      "Final Unique errors: 2167\n",
      "Final Unique inputs: 2937\n",
      "2251\n",
      "0.7503333333333333\n",
      "Final Unique errors: 2212\n",
      "Final Unique inputs: 2955\n",
      "2267\n",
      "0.7556666666666667\n",
      "Final Unique errors: 2230\n",
      "Final Unique inputs: 2952\n",
      "2200\n",
      "0.7333333333333333\n",
      "Final Unique errors: 2154\n",
      "Final Unique inputs: 2944\n",
      "2204\n",
      "0.7346666666666667\n",
      "Final Unique errors: 2171\n",
      "Final Unique inputs: 2949\n",
      "2229\n",
      "0.743\n",
      "Final Unique errors: 2177\n",
      "Final Unique inputs: 2935\n",
      "2226\n",
      "0.742\n",
      "Final Unique errors: 2181\n",
      "Final Unique inputs: 2939\n",
      "2244\n",
      "0.748\n",
      "Final Unique errors: 2210\n",
      "Final Unique inputs: 2953\n"
     ]
    }
   ],
   "source": [
    "STABILITY_ITERS = 10\n",
    "\n",
    "\n",
    "for i in range(STABILITY_ITERS):\n",
    "    unique_input1_set_exploitation = set()\n",
    "    unique_input1_error_set_exploitation = set()\n",
    "\n",
    "    occupation_pair_error_exploitation = {}\n",
    "\n",
    "    occupation1_error_exploitation = {}\n",
    "\n",
    "    occupation2_error_exploitation = {}\n",
    "\n",
    "    verb_error_exploitation = {}\n",
    "\n",
    "    action_error_exploitation = {}\n",
    "\n",
    "    occupation_pair_count_exploitation = {}\n",
    "\n",
    "    occupation1_count_exploitation = {}\n",
    "\n",
    "    occupation2_count_exploitation = {}\n",
    "\n",
    "    verb_count_exploitation = {}\n",
    "\n",
    "    action_count_exploitation = {}\n",
    "    \n",
    "    \n",
    "    generate_test_sentences(ITERS=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2164 2958\n",
      "2212 2951\n",
      "2167 2937\n",
      "2212 2955\n",
      "2230 2952\n",
      "2154 2944\n",
      "2171 2949\n",
      "2177 2935\n",
      "2181 2939\n",
      "2210 2953\n"
     ]
    }
   ],
   "source": [
    "for i, err_set in enumerate(STABILITY_ERRS):\n",
    "    print(len(err_set), len(STABILITY_TEST_CASES[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('saved_pickles/Exploitation/occupation_pair_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_count_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation1_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_count_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation2_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_count_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/verb_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_count_exploitation, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploitation/action_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_count_exploitation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('saved_pickles/Exploitation/occupation_pair_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_error_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation1_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_error_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/occupation2_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_error_exploitation, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploitation/verb_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_error_exploitation, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploitation/action_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_error_exploitation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
