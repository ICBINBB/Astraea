{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:08.850425 4726316480 file_utils.py:39] PyTorch version 1.5.1 available.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "I0825 20:41:13.962319 4726316480 file_utils.py:339] checking cache for https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz at /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0825 20:41:13.963419 4726316480 file_utils.py:340] waiting to acquire lock on /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0825 20:41:13.964954 4726316480 filelock.py:274] Lock 5787127144 acquired on /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174.lock\n",
      "I0825 20:41:13.965996 4726316480 file_utils.py:343] cache of https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz is up-to-date\n",
      "I0825 20:41:13.967118 4726316480 filelock.py:318] Lock 5787127144 released on /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174.lock\n",
      "I0825 20:41:13.968061 4726316480 archival.py:164] loading archive file https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz from cache at /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0825 20:41:13.969262 4726316480 archival.py:171] extracting archive file /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174 to temp dir /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp2o_t0bwm\n",
      "I0825 20:41:23.053957 4726316480 params.py:247] type = from_instances\n",
      "I0825 20:41:23.054785 4726316480 vocabulary.py:321] Loading token dictionary from /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp2o_t0bwm/vocabulary.\n",
      "I0825 20:41:23.055702 4726316480 filelock.py:274] Lock 5787012120 acquired on /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp2o_t0bwm/vocabulary/.lock\n",
      "I0825 20:41:23.056936 4726316480 filelock.py:318] Lock 5787012120 released on /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp2o_t0bwm/vocabulary/.lock\n",
      "I0825 20:41:23.057919 4726316480 params.py:247] model.type = coref\n",
      "I0825 20:41:23.058767 4726316480 params.py:247] model.regularizer = None\n",
      "I0825 20:41:23.059685 4726316480 params.py:247] model.text_field_embedder.type = basic\n",
      "I0825 20:41:23.060534 4726316480 params.py:247] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "I0825 20:41:23.061544 4726316480 params.py:247] model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0825 20:41:23.062164 4726316480 params.py:247] model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "I0825 20:41:23.062819 4726316480 params.py:247] model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
      "I0825 20:41:24.199638 4726316480 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0825 20:41:24.200695 4726316480 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0825 20:41:24.301734 4726316480 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/SpanBERT/spanbert-large-cased/pytorch_model.bin from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c8041a5fa85cc2725744bab27890d410f0f47dde0c9f1152de9a1b1e5df049e.d1ce6dff7f84348ad7c77a33a9a6e8751099db9c9d609ac7752e61804befe4da\n",
      "I0825 20:41:28.023856 4726316480 modeling_utils.py:741] Weights of BertModel not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "I0825 20:41:29.072314 4726316480 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0825 20:41:29.073657 4726316480 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0825 20:41:29.074729 4726316480 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0825 20:41:33.150076 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0825 20:41:33.150821 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0825 20:41:33.151534 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0825 20:41:33.152442 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0825 20:41:34.234314 4726316480 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0825 20:41:34.236088 4726316480 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:34.239635 4726316480 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0825 20:41:38.435771 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0825 20:41:38.437029 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0825 20:41:38.438260 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0825 20:41:38.439175 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0825 20:41:38.469308 4726316480 params.py:247] model.context_layer.type = pass_through\n",
      "I0825 20:41:38.470618 4726316480 params.py:247] model.context_layer.input_dim = 1024\n",
      "I0825 20:41:38.471772 4726316480 params.py:247] model.mention_feedforward.input_dim = 3092\n",
      "I0825 20:41:38.472681 4726316480 params.py:247] model.mention_feedforward.num_layers = 2\n",
      "I0825 20:41:38.473199 4726316480 params.py:247] model.mention_feedforward.hidden_dims = 1500\n",
      "I0825 20:41:38.474129 4726316480 params.py:247] model.mention_feedforward.activations = relu\n",
      "I0825 20:41:38.474781 4726316480 params.py:247] type = relu\n",
      "I0825 20:41:38.475698 4726316480 params.py:247] model.mention_feedforward.dropout = 0.3\n",
      "I0825 20:41:38.514966 4726316480 params.py:247] model.antecedent_feedforward.input_dim = 9296\n",
      "I0825 20:41:38.515711 4726316480 params.py:247] model.antecedent_feedforward.num_layers = 2\n",
      "I0825 20:41:38.516176 4726316480 params.py:247] model.antecedent_feedforward.hidden_dims = 1500\n",
      "I0825 20:41:38.516965 4726316480 params.py:247] model.antecedent_feedforward.activations = relu\n",
      "I0825 20:41:38.517596 4726316480 params.py:247] type = relu\n",
      "I0825 20:41:38.518474 4726316480 params.py:247] model.antecedent_feedforward.dropout = 0.3\n",
      "I0825 20:41:38.606413 4726316480 params.py:247] model.feature_size = 20\n",
      "I0825 20:41:38.607123 4726316480 params.py:247] model.max_span_width = 30\n",
      "I0825 20:41:38.607668 4726316480 params.py:247] model.spans_per_word = 0.4\n",
      "I0825 20:41:38.608372 4726316480 params.py:247] model.max_antecedents = 50\n",
      "I0825 20:41:38.609044 4726316480 params.py:247] model.coarse_to_fine = True\n",
      "I0825 20:41:38.609809 4726316480 params.py:247] model.inference_order = 2\n",
      "I0825 20:41:38.610252 4726316480 params.py:247] model.lexical_dropout = 0.2\n",
      "I0825 20:41:38.611436 4726316480 params.py:247] model.initializer.regexes.0.1.type = xavier_normal\n",
      "I0825 20:41:38.612143 4726316480 params.py:247] model.initializer.regexes.0.1.gain = 1.0\n",
      "I0825 20:41:38.612672 4726316480 params.py:247] model.initializer.regexes.1.1.type = xavier_normal\n",
      "I0825 20:41:38.613481 4726316480 params.py:247] model.initializer.regexes.1.1.gain = 1.0\n",
      "I0825 20:41:38.614089 4726316480 params.py:247] model.initializer.regexes.2.1.type = xavier_normal\n",
      "I0825 20:41:38.614804 4726316480 params.py:247] model.initializer.regexes.2.1.gain = 1.0\n",
      "I0825 20:41:38.615530 4726316480 params.py:247] model.initializer.regexes.3.1.type = xavier_normal\n",
      "I0825 20:41:38.616441 4726316480 params.py:247] model.initializer.regexes.3.1.gain = 1.0\n",
      "I0825 20:41:38.617161 4726316480 params.py:247] model.initializer.regexes.4.1.type = xavier_normal\n",
      "I0825 20:41:38.617992 4726316480 params.py:247] model.initializer.regexes.4.1.gain = 1.0\n",
      "I0825 20:41:38.618615 4726316480 params.py:247] model.initializer.regexes.5.1.type = xavier_normal\n",
      "I0825 20:41:38.619279 4726316480 params.py:247] model.initializer.regexes.5.1.gain = 1.0\n",
      "I0825 20:41:38.619782 4726316480 params.py:247] model.initializer.regexes.6.1.type = orthogonal\n",
      "I0825 20:41:38.620509 4726316480 params.py:247] model.initializer.regexes.6.1.gain = 1.0\n",
      "I0825 20:41:38.620965 4726316480 params.py:247] model.initializer.prevent_regexes = None\n",
      "I0825 20:41:38.677829 4726316480 initializers.py:471] Initializing parameters\n",
      "I0825 20:41:38.703881 4726316480 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0825 20:41:38.730456 4726316480 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0825 20:41:38.743545 4726316480 initializers.py:481] Initializing _mention_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0825 20:41:38.744263 4726316480 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0825 20:41:38.811289 4726316480 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0825 20:41:38.823001 4726316480 initializers.py:481] Initializing _antecedent_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0825 20:41:38.823782 4726316480 initializers.py:481] Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
      "I0825 20:41:38.824465 4726316480 initializers.py:481] Initializing _distance_embedding.weight using _distance_embedding.weight initializer\n",
      "I0825 20:41:38.825114 4726316480 initializers.py:481] Initializing _coarse2fine_scorer.weight using .*scorer.*weight initializer\n",
      "I0825 20:41:38.876048 4726316480 initializers.py:481] Initializing _span_updating_gated_sum._gate.weight using .*_span_updating_gated_sum.*weight initializer\n",
      "W0825 20:41:38.877089 4726316480 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "W0825 20:41:38.878006 4726316480 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
      "I0825 20:41:38.882608 4726316480 initializers.py:490] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0825 20:41:38.883960 4726316480 initializers.py:496]    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "I0825 20:41:38.884933 4726316480 initializers.py:496]    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "I0825 20:41:38.885612 4726316480 initializers.py:496]    _antecedent_scorer._module.bias\n",
      "I0825 20:41:38.886461 4726316480 initializers.py:496]    _attentive_span_extractor._global_attention._module.bias\n",
      "I0825 20:41:38.887120 4726316480 initializers.py:496]    _attentive_span_extractor._global_attention._module.weight\n",
      "I0825 20:41:38.888350 4726316480 initializers.py:496]    _coarse2fine_scorer.bias\n",
      "I0825 20:41:38.889219 4726316480 initializers.py:496]    _mention_feedforward._module._linear_layers.0.bias\n",
      "I0825 20:41:38.889947 4726316480 initializers.py:496]    _mention_feedforward._module._linear_layers.1.bias\n",
      "I0825 20:41:38.890972 4726316480 initializers.py:496]    _mention_scorer._module.bias\n",
      "I0825 20:41:38.894106 4726316480 initializers.py:496]    _span_updating_gated_sum._gate.bias\n",
      "I0825 20:41:38.896353 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "I0825 20:41:38.898265 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:38.900278 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "I0825 20:41:38.902194 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0825 20:41:38.903632 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "I0825 20:41:38.905844 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0825 20:41:38.907346 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I0825 20:41:38.908473 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0825 20:41:38.909717 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "I0825 20:41:38.911020 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0825 20:41:38.912221 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0825 20:41:38.913043 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0825 20:41:38.913790 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0825 20:41:38.914586 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0825 20:41:38.915130 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0825 20:41:38.915983 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0825 20:41:38.916674 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "I0825 20:41:38.917346 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "I0825 20:41:38.917994 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0825 20:41:38.918563 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0825 20:41:38.919108 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0825 20:41:38.919887 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0825 20:41:38.920607 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0825 20:41:38.921556 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0825 20:41:38.922553 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0825 20:41:38.923650 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0825 20:41:38.924685 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0825 20:41:38.925205 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0825 20:41:38.925895 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0825 20:41:38.926398 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0825 20:41:38.927083 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0825 20:41:38.927646 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0825 20:41:38.928458 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0825 20:41:38.929145 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0825 20:41:38.929955 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0825 20:41:38.930682 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0825 20:41:38.931941 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0825 20:41:38.932820 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0825 20:41:38.933473 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0825 20:41:38.934268 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0825 20:41:38.935015 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0825 20:41:38.935962 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0825 20:41:38.936856 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0825 20:41:38.937648 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0825 20:41:38.938393 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0825 20:41:38.939234 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0825 20:41:38.940677 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0825 20:41:38.941944 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:38.942736 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0825 20:41:38.943289 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0825 20:41:38.943882 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0825 20:41:38.944340 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0825 20:41:38.944816 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0825 20:41:38.945238 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0825 20:41:38.945799 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0825 20:41:38.946604 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0825 20:41:38.947131 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "I0825 20:41:38.947902 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0825 20:41:38.948427 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "I0825 20:41:38.949078 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0825 20:41:38.949732 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0825 20:41:38.950394 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0825 20:41:38.950859 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "I0825 20:41:38.951446 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "I0825 20:41:38.952048 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0825 20:41:38.952751 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0825 20:41:38.953476 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0825 20:41:38.954868 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0825 20:41:38.956423 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0825 20:41:38.957720 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "I0825 20:41:38.958601 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "I0825 20:41:38.959182 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "I0825 20:41:38.960026 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "I0825 20:41:38.960642 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "I0825 20:41:38.961480 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "I0825 20:41:38.962082 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "I0825 20:41:38.962784 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "I0825 20:41:38.963264 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "I0825 20:41:38.964145 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "I0825 20:41:38.964717 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "I0825 20:41:38.965314 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "I0825 20:41:38.965846 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "I0825 20:41:38.966583 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "I0825 20:41:38.967081 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "I0825 20:41:38.967823 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "I0825 20:41:38.968453 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "I0825 20:41:38.969147 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "I0825 20:41:38.969638 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "I0825 20:41:38.970339 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "I0825 20:41:38.971693 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "I0825 20:41:38.973096 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "I0825 20:41:38.974106 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "I0825 20:41:38.974802 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "I0825 20:41:38.977074 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:38.978402 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "I0825 20:41:38.979351 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "I0825 20:41:38.980518 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "I0825 20:41:38.981712 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "I0825 20:41:38.982492 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "I0825 20:41:38.983031 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "I0825 20:41:38.983721 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "I0825 20:41:38.984572 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "I0825 20:41:38.985445 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "I0825 20:41:38.986312 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "I0825 20:41:38.987054 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "I0825 20:41:38.988415 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "I0825 20:41:38.989318 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "I0825 20:41:38.990094 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "I0825 20:41:38.990917 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
      "I0825 20:41:38.991557 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
      "I0825 20:41:38.992497 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "I0825 20:41:38.993481 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "I0825 20:41:38.994319 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "I0825 20:41:38.995416 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "I0825 20:41:38.996664 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "I0825 20:41:38.997663 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "I0825 20:41:38.998249 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "I0825 20:41:38.998990 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "I0825 20:41:38.999537 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.000159 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "I0825 20:41:39.000705 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "I0825 20:41:39.001912 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "I0825 20:41:39.002578 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "I0825 20:41:39.003132 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "I0825 20:41:39.003795 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "I0825 20:41:39.004811 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "I0825 20:41:39.006150 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "I0825 20:41:39.006966 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "I0825 20:41:39.007591 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "I0825 20:41:39.008526 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "I0825 20:41:39.009093 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "I0825 20:41:39.009868 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "I0825 20:41:39.010394 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "I0825 20:41:39.011063 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.011539 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.012297 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "I0825 20:41:39.013080 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "I0825 20:41:39.013913 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "I0825 20:41:39.014650 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "I0825 20:41:39.016021 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:39.016684 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "I0825 20:41:39.017235 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "I0825 20:41:39.017938 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "I0825 20:41:39.018525 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "I0825 20:41:39.019217 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "I0825 20:41:39.019941 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "I0825 20:41:39.020830 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "I0825 20:41:39.021455 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "I0825 20:41:39.022821 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "I0825 20:41:39.023553 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.024107 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.024791 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "I0825 20:41:39.025381 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "I0825 20:41:39.026700 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "I0825 20:41:39.027611 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
      "I0825 20:41:39.028620 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
      "I0825 20:41:39.029945 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "I0825 20:41:39.031153 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "I0825 20:41:39.031980 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "I0825 20:41:39.032772 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "I0825 20:41:39.033456 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "I0825 20:41:39.034091 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "I0825 20:41:39.034785 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "I0825 20:41:39.035319 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "I0825 20:41:39.035920 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "I0825 20:41:39.036791 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.037645 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.038363 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "I0825 20:41:39.039295 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "I0825 20:41:39.040416 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "I0825 20:41:39.041249 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "I0825 20:41:39.042385 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "I0825 20:41:39.043342 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "I0825 20:41:39.043952 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "I0825 20:41:39.044988 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "I0825 20:41:39.045614 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "I0825 20:41:39.046262 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "I0825 20:41:39.047243 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "I0825 20:41:39.048048 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "I0825 20:41:39.049132 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "I0825 20:41:39.050605 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "I0825 20:41:39.051722 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.052587 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.053351 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "I0825 20:41:39.054205 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "I0825 20:41:39.054982 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:39.056453 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "I0825 20:41:39.057484 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "I0825 20:41:39.057962 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "I0825 20:41:39.058721 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "I0825 20:41:39.059256 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "I0825 20:41:39.060142 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "I0825 20:41:39.060769 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "I0825 20:41:39.061560 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "I0825 20:41:39.062053 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "I0825 20:41:39.062678 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "I0825 20:41:39.063210 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "I0825 20:41:39.064043 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.064834 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.065984 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0825 20:41:39.067503 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "I0825 20:41:39.068517 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "I0825 20:41:39.069253 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0825 20:41:39.070482 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0825 20:41:39.071726 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0825 20:41:39.073246 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0825 20:41:39.074090 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0825 20:41:39.074585 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0825 20:41:39.075145 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0825 20:41:39.075604 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0825 20:41:39.076210 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0825 20:41:39.076902 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0825 20:41:39.077624 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0825 20:41:39.078225 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.079043 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.079731 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "I0825 20:41:39.080581 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "I0825 20:41:39.082181 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "I0825 20:41:39.084007 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "I0825 20:41:39.084783 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "I0825 20:41:39.085509 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "I0825 20:41:39.086311 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "I0825 20:41:39.087032 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "I0825 20:41:39.088020 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "I0825 20:41:39.088831 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "I0825 20:41:39.089565 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "I0825 20:41:39.090348 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "I0825 20:41:39.092592 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "I0825 20:41:39.093740 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "I0825 20:41:39.094394 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.095106 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.095919 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:39.096570 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "I0825 20:41:39.097392 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "I0825 20:41:39.098063 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "I0825 20:41:39.098820 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "I0825 20:41:39.099493 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "I0825 20:41:39.100570 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "I0825 20:41:39.101227 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "I0825 20:41:39.101837 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "I0825 20:41:39.102589 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "I0825 20:41:39.103119 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "I0825 20:41:39.103938 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "I0825 20:41:39.104671 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "I0825 20:41:39.105587 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "I0825 20:41:39.106373 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.107226 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.107816 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
      "I0825 20:41:39.108598 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "I0825 20:41:39.109277 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "I0825 20:41:39.110075 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "I0825 20:41:39.110701 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "I0825 20:41:39.111481 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "I0825 20:41:39.112073 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "I0825 20:41:39.112941 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "I0825 20:41:39.113687 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "I0825 20:41:39.114635 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "I0825 20:41:39.115407 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "I0825 20:41:39.116102 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "I0825 20:41:39.116777 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "I0825 20:41:39.117619 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "I0825 20:41:39.118106 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.118853 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.119472 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "I0825 20:41:39.120314 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "I0825 20:41:39.120921 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "I0825 20:41:39.121803 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "I0825 20:41:39.122556 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "I0825 20:41:39.123351 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "I0825 20:41:39.124001 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "I0825 20:41:39.125000 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "I0825 20:41:39.125476 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "I0825 20:41:39.126251 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "I0825 20:41:39.126878 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "I0825 20:41:39.127642 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "I0825 20:41:39.128225 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "I0825 20:41:39.128933 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "I0825 20:41:39.129552 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:39.130386 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.130881 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0825 20:41:39.131706 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0825 20:41:39.132276 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0825 20:41:39.133144 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0825 20:41:39.133732 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0825 20:41:39.134412 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0825 20:41:39.135071 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0825 20:41:39.135952 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0825 20:41:39.136636 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0825 20:41:39.137554 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0825 20:41:39.138098 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0825 20:41:39.139014 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0825 20:41:39.139571 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0825 20:41:39.140314 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "I0825 20:41:39.140958 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.141783 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.142404 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0825 20:41:39.143136 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0825 20:41:39.143678 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0825 20:41:39.144524 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "I0825 20:41:39.145117 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0825 20:41:39.145857 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "I0825 20:41:39.146542 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0825 20:41:39.147402 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0825 20:41:39.148238 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0825 20:41:39.149070 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0825 20:41:39.149753 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0825 20:41:39.150544 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0825 20:41:39.151142 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0825 20:41:39.151847 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0825 20:41:39.152304 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.153097 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.153728 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0825 20:41:39.154464 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0825 20:41:39.155308 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0825 20:41:39.156176 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0825 20:41:39.157301 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0825 20:41:39.157907 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0825 20:41:39.158744 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0825 20:41:39.159525 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0825 20:41:39.160130 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0825 20:41:39.160789 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0825 20:41:39.161687 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0825 20:41:39.162373 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0825 20:41:39.163024 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0825 20:41:39.163722 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:39.164469 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.165210 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.165935 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0825 20:41:39.166658 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0825 20:41:39.167350 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0825 20:41:39.167982 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0825 20:41:39.168755 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0825 20:41:39.169457 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0825 20:41:39.170378 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0825 20:41:39.171020 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0825 20:41:39.171918 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0825 20:41:39.172656 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0825 20:41:39.173315 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0825 20:41:39.173988 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "I0825 20:41:39.174644 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0825 20:41:39.175207 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "I0825 20:41:39.175853 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.176429 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.177243 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0825 20:41:39.177957 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0825 20:41:39.178580 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0825 20:41:39.179067 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0825 20:41:39.179760 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0825 20:41:39.180653 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "I0825 20:41:39.182361 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0825 20:41:39.183109 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "I0825 20:41:39.183757 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0825 20:41:39.184566 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0825 20:41:39.185184 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0825 20:41:39.185846 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0825 20:41:39.186383 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0825 20:41:39.187174 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0825 20:41:39.187772 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.188720 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.189270 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0825 20:41:39.189936 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0825 20:41:39.190537 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0825 20:41:39.191141 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0825 20:41:39.191780 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0825 20:41:39.192389 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0825 20:41:39.193065 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0825 20:41:39.193664 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0825 20:41:39.194542 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0825 20:41:39.195306 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0825 20:41:39.195898 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0825 20:41:39.196713 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:39.197306 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0825 20:41:39.198092 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0825 20:41:39.198472 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0825 20:41:39.199060 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0825 20:41:39.199484 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0825 20:41:39.200015 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0825 20:41:39.200638 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0825 20:41:39.201284 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0825 20:41:39.201990 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0825 20:41:39.202538 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0825 20:41:39.203211 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0825 20:41:39.203973 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0825 20:41:39.204718 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0825 20:41:39.205291 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "I0825 20:41:39.205833 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0825 20:41:39.206396 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "I0825 20:41:39.207098 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0825 20:41:39.207774 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0825 20:41:39.208619 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "I0825 20:41:39.209249 4726316480 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "I0825 20:41:39.218184 4726316480 embedding.py:256] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0825 20:41:39.218845 4726316480 embedding.py:256] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0825 20:41:42.365535 4726316480 params.py:247] validation_dataset_reader.type = coref\n",
      "I0825 20:41:42.366605 4726316480 params.py:247] validation_dataset_reader.lazy = False\n",
      "I0825 20:41:42.367247 4726316480 params.py:247] validation_dataset_reader.cache_directory = None\n",
      "I0825 20:41:42.367996 4726316480 params.py:247] validation_dataset_reader.max_instances = None\n",
      "I0825 20:41:42.368645 4726316480 params.py:247] validation_dataset_reader.manual_distributed_sharding = False\n",
      "I0825 20:41:42.369627 4726316480 params.py:247] validation_dataset_reader.manual_multi_process_sharding = False\n",
      "I0825 20:41:42.370266 4726316480 params.py:247] validation_dataset_reader.max_span_width = 30\n",
      "I0825 20:41:42.371356 4726316480 params.py:247] validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0825 20:41:42.372402 4726316480 params.py:247] validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0825 20:41:42.373097 4726316480 params.py:247] validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0825 20:41:42.374000 4726316480 params.py:247] validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0825 20:41:42.374766 4726316480 params.py:247] validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0825 20:41:43.555923 4726316480 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0825 20:41:43.557266 4726316480 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0825 20:41:43.558312 4726316480 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0825 20:41:47.958437 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0825 20:41:47.959640 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0825 20:41:47.960726 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0825 20:41:47.961626 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0825 20:41:48.934342 4726316480 configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0825 20:41:48.935551 4726316480 configuration_utils.py:301] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0825 20:41:48.936498 4726316480 tokenization_utils.py:938] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0825 20:41:53.077062 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0825 20:41:53.077899 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0825 20:41:53.078470 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0825 20:41:53.079201 4726316480 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0825 20:41:53.104397 4726316480 params.py:247] validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0825 20:41:53.105165 4726316480 params.py:247] validation_dataset_reader.max_sentences = None\n",
      "I0825 20:41:53.105782 4726316480 params.py:247] validation_dataset_reader.remove_singleton_clusters = False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked', 'disclosed to', 'told'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified', 'spoke with'] ##  pronoun2 = ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    return ['He', 'She', 'They']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"librarian\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"about \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "\n",
    "action1 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p1[1]] = [action1] ### disclosed to\n",
    "verb_action[verb_list_p1[2]] = [action1] ### told\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action1] ### notified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spoke with \n",
    "### asked\n",
    "action2 = ['painting', 'dancing',\n",
    "           'fencing', 'gymnastics',\n",
    "           'independent cinema', 'woodworking',\n",
    "           'studying', 'horse racing',\n",
    "           'singing', 'kayaking', 'football',\n",
    "           'baseball', 'basketball', 'quizzing', \n",
    "           'gardening', 'blogging', 'board games', \n",
    "           'breadmaking', 'baking']\n",
    "verb_action[verb_list_p2[1]] = [action2]\n",
    "verb_action[verb_list_p1[0]] = [action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_verb = ['standing', 'speaking', 'talking', \n",
    "                  'passing by', 'sitting', 'sitting on the chair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    output = predictor.predict(document = sentence)\n",
    "    return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " {'top_spans': [[0, 1], [5, 5], [6, 6], [7, 8], [10, 11], [13, 13]],\n",
       "  'antecedent_indices': [[0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5]],\n",
       "  'predicted_antecedents': [-1, -1, -1, -1, -1, -1],\n",
       "  'document': ['The',\n",
       "   'developer',\n",
       "   'was',\n",
       "   'standing',\n",
       "   '.',\n",
       "   'She',\n",
       "   'notified',\n",
       "   'the',\n",
       "   'nurse',\n",
       "   'that',\n",
       "   'the',\n",
       "   'work',\n",
       "   'was',\n",
       "   'unsatisfactory',\n",
       "   '.'],\n",
       "  'clusters': []})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_clusters(\"The developer was standing. She notified the nurse that the work was unsatisfactory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(oc1, oc2, verb, action, pronoun):\n",
    "\n",
    "\n",
    "    aux_verb = random.choice(auxiliary_verb)\n",
    "    input1 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[0] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "\n",
    "    input2 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[1] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "        \n",
    "    return input1, input2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "occupation_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "occupation_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962\n",
      "0.654\n",
      "Final Unique errors: 1956\n",
      "Final Unique inputs: 2987\n",
      "1976\n",
      "0.6586666666666666\n",
      "Final Unique errors: 1963\n",
      "Final Unique inputs: 2985\n",
      "1928\n",
      "0.6426666666666667\n",
      "Final Unique errors: 1916\n",
      "Final Unique inputs: 2980\n",
      "1944\n",
      "0.648\n",
      "Final Unique errors: 1930\n",
      "Final Unique inputs: 2979\n",
      "1948\n",
      "0.6493333333333333\n",
      "Final Unique errors: 1938\n",
      "Final Unique inputs: 2979\n",
      "1971\n",
      "0.657\n",
      "Final Unique errors: 1960\n",
      "Final Unique inputs: 2986\n",
      "1960\n",
      "0.6533333333333333\n",
      "Final Unique errors: 1949\n",
      "Final Unique inputs: 2982\n",
      "2012\n",
      "0.6706666666666666\n",
      "Final Unique errors: 1997\n",
      "Final Unique inputs: 2978\n",
      "1959\n",
      "0.653\n",
      "Final Unique errors: 1951\n",
      "Final Unique inputs: 2990\n",
      "1974\n",
      "0.658\n",
      "Final Unique errors: 1963\n",
      "Final Unique inputs: 2984\n"
     ]
    }
   ],
   "source": [
    "STABILITY_ERRS = []\n",
    "STABILITY_TEST_CASES = []\n",
    "\n",
    "STABILITY_ITERS = 10\n",
    "\n",
    "\n",
    "for i in range(STABILITY_ITERS):\n",
    "    \n",
    "    unique_input1_set = set()\n",
    "    unique_input1_error_set = set()\n",
    "\n",
    "    occupation_pair_error = {}\n",
    "\n",
    "    occupation1_error = {}\n",
    "\n",
    "    occupation2_error = {}\n",
    "\n",
    "    verb_error = {}\n",
    "\n",
    "    action_error = {}\n",
    "\n",
    "    occupation_pair_count = {}\n",
    "\n",
    "    occupation1_count = {}\n",
    "\n",
    "    occupation2_count = {}\n",
    "\n",
    "    verb_count = {}\n",
    "\n",
    "    action_count = {}\n",
    "\n",
    "    err_count = 0\n",
    "    ITERS = 3000\n",
    "\n",
    "    RELAXED_ERROR = True\n",
    "\n",
    "\n",
    "    for i in range(ITERS):\n",
    "        oc1 = random.choice(occupations_1)\n",
    "        oc2 = random.choice(occupations_2)\n",
    "        verb = random.choice(list(verb_action.keys()))\n",
    "        action = random.choice(random.choice(verb_action[verb]))\n",
    "        pronoun = choose_pronoun_type(verb)\n",
    "\n",
    "        input1, input2 = generate_sentences(oc1, oc2, verb, action, pronoun)\n",
    "\n",
    "        pred1, _ = predict_clusters(input1)\n",
    "        pred2, _ = predict_clusters(input2)\n",
    "    #     pred3, _ = predict_clusters(input2)\n",
    "\n",
    "\n",
    "#         if(i % 30 == 0):\n",
    "#             print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "#             print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "#             print(\"------------------------------\")\n",
    "\n",
    "\n",
    "        unique_input1_set.add(input1)\n",
    "\n",
    "        update_dict(occupation_pair_count, (oc1, oc2))\n",
    "        update_dict(occupation1_count, oc1)\n",
    "        update_dict(occupation2_count, oc2)\n",
    "        update_dict(verb_count, verb)\n",
    "        update_dict(action_count, action)\n",
    "\n",
    "    #     print(pred1, pred2)\n",
    "    #     print(input1)\n",
    "    #     print(input2)\n",
    "    #     print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if not (pred1 == pred2):\n",
    "    #         if ((len(pred1) > 0 and len(pred2) > 0)):\n",
    "    #             if ((len(pred1[0]) == len(pred2[0]))):\n",
    "                    err_count += 1\n",
    "\n",
    "                    unique_input1_error_set.add(input1)\n",
    "\n",
    "    #                 print(pred1, pred2, pred3)\n",
    "    #                 print(input1)\n",
    "    #                 print(input2)\n",
    "    #                 print(input3)\n",
    "\n",
    "                    update_dict(occupation_pair_error, (oc1, oc2))\n",
    "                    update_dict(occupation1_error, oc1)\n",
    "                    update_dict(occupation2_error, oc2)\n",
    "                    update_dict(verb_error, verb)\n",
    "                    update_dict(action_error, action)\n",
    "\n",
    "\n",
    "\n",
    "    print(err_count)\n",
    "    print(err_count/ITERS)\n",
    "    print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "    print(\"Final Unique inputs: \" + str(len(unique_input1_set)))\n",
    "    STABILITY_ERRS.append(unique_input1_error_set)\n",
    "    STABILITY_TEST_CASES.append(unique_input1_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1956 2987\n",
      "1963 2985\n",
      "1916 2980\n",
      "1930 2979\n",
      "1938 2979\n",
      "1960 2986\n",
      "1949 2982\n",
      "1997 2978\n",
      "1951 2990\n",
      "1963 2984\n"
     ]
    }
   ],
   "source": [
    "for i, err_set in enumerate(STABILITY_ERRS):\n",
    "    print(len(err_set), len(STABILITY_TEST_CASES[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('saved_pickles/Exploration/unique_input1_set.pickle', 'wb') as handle:\n",
    "#     pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "#     pickle.dump(unique_input1_error_set, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation2_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/verb_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_count, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploration/action_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_count, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/occupation2_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/Exploration/verb_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_error, handle)\n",
    "\n",
    "# with open('saved_pickles/Exploration/action_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 if((oc1, oc2) in occupation_pair_error.keys()):\n",
    "#                     occupation_pair_error[(oc1, oc2)] += 1\n",
    "#                 else:\n",
    "#                     occupation_pair_error[(oc1, oc2)] = 1\n",
    "                                          \n",
    "#                 if(oc1 in occupation1_error.keys()):\n",
    "#                     occupation1_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation1_error[oc1] = 1\n",
    "                \n",
    "#                 if(oc2 in occupation2_error.keys()):\n",
    "#                     occupation2_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation2_error[oc1] = 1\n",
    "                                          \n",
    "#                 if(verb in verb_error.keys()):\n",
    "#                     verb_error[verb] += 1\n",
    "#                 else:\n",
    "#                     verb_error[verb] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_clusters(sentence):\n",
    "#     output = predictor.predict(document = sentence)\n",
    "#     return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_clusters(\"The developer was standing. She notified the nurse that the work was unsatisfactory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_count = 0\n",
    "# ITERS = 20\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(ITERS):\n",
    "#     oc1 = random.choice(occupations_1)\n",
    "#     oc2 = random.choice(occupations_2)\n",
    "#     verb = random.choice(list(verb_action.keys()))\n",
    "#     action = random.choice(random.choice(verb_action[verb]))\n",
    "#     in1 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     in2 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action) \n",
    "    \n",
    "#     in3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "#     pred1, _ = predict_clusters(in1)\n",
    "#     pred2, _ = predict_clusters(in2)\n",
    "#     pred3, _ = predict_clusters(in2)\n",
    "    \n",
    "#     if not (pred1 == pred2 and pred2 == pred3):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#             err_count += 1\n",
    "\n",
    "#             print(pred1, pred2, pred3)\n",
    "#             print(in1)\n",
    "#             print(in2)\n",
    "#             print(in3)\n",
    "#             print()\n",
    "    \n",
    "\n",
    "# print(err_count)\n",
    "# print(err_count/ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
