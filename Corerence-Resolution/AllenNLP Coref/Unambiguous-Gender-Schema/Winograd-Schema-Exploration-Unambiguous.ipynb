{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:39:50.491070 4665613760 archival.py:164] loading archive file https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz from cache at /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0630 02:39:50.499674 4665613760 archival.py:171] extracting archive file /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174 to temp dir /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmpsd0onzg7\n",
      "I0630 02:40:00.651583 4665613760 params.py:247] type = from_instances\n",
      "I0630 02:40:00.654124 4665613760 vocabulary.py:314] Loading token dictionary from /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmpsd0onzg7/vocabulary.\n",
      "I0630 02:40:00.660010 4665613760 params.py:247] model.type = coref\n",
      "I0630 02:40:00.662451 4665613760 params.py:247] model.regularizer = None\n",
      "I0630 02:40:00.663909 4665613760 params.py:247] model.text_field_embedder.type = basic\n",
      "I0630 02:40:00.665767 4665613760 params.py:247] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "I0630 02:40:00.666740 4665613760 params.py:247] model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0630 02:40:00.667329 4665613760 params.py:247] model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "I0630 02:40:01.928805 4665613760 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0630 02:40:01.932589 4665613760 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0630 02:40:03.133949 4665613760 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/pytorch_model.bin from cache at /Users/sakshiudeshi/.cache/torch/transformers/d707dadfcbbac6a5fc440f1e94db728b000a2816693f44a87092182199f2d52d.d1ce6dff7f84348ad7c77a33a9a6e8751099db9c9d609ac7752e61804befe4da\n",
      "I0630 02:40:08.505537 4665613760 modeling_utils.py:601] Weights of BertModel not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "I0630 02:40:09.558511 4665613760 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0630 02:40:09.560381 4665613760 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0630 02:40:09.561728 4665613760 tokenization_utils.py:420] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0630 02:40:14.255699 4665613760 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0630 02:40:14.257896 4665613760 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0630 02:40:14.259021 4665613760 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0630 02:40:14.260606 4665613760 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0630 02:40:14.961354 4665613760 params.py:247] model.context_layer.type = pass_through\n",
      "I0630 02:40:14.962368 4665613760 params.py:247] model.context_layer.input_dim = 1024\n",
      "I0630 02:40:14.963194 4665613760 params.py:247] model.mention_feedforward.input_dim = 3092\n",
      "I0630 02:40:14.963985 4665613760 params.py:247] model.mention_feedforward.num_layers = 2\n",
      "I0630 02:40:14.964565 4665613760 params.py:247] model.mention_feedforward.hidden_dims = 1500\n",
      "I0630 02:40:14.965519 4665613760 params.py:247] model.mention_feedforward.activations = relu\n",
      "I0630 02:40:14.966735 4665613760 params.py:247] type = relu\n",
      "I0630 02:40:14.967696 4665613760 params.py:247] model.mention_feedforward.dropout = 0.3\n",
      "I0630 02:40:15.011487 4665613760 params.py:247] model.antecedent_feedforward.input_dim = 9296\n",
      "I0630 02:40:15.012398 4665613760 params.py:247] model.antecedent_feedforward.num_layers = 2\n",
      "I0630 02:40:15.013042 4665613760 params.py:247] model.antecedent_feedforward.hidden_dims = 1500\n",
      "I0630 02:40:15.013803 4665613760 params.py:247] model.antecedent_feedforward.activations = relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.014489 4665613760 params.py:247] type = relu\n",
      "I0630 02:40:15.015440 4665613760 params.py:247] model.antecedent_feedforward.dropout = 0.3\n",
      "I0630 02:40:15.111077 4665613760 params.py:247] model.feature_size = 20\n",
      "I0630 02:40:15.111766 4665613760 params.py:247] model.max_span_width = 30\n",
      "I0630 02:40:15.112437 4665613760 params.py:247] model.spans_per_word = 0.4\n",
      "I0630 02:40:15.113102 4665613760 params.py:247] model.max_antecedents = 50\n",
      "I0630 02:40:15.113743 4665613760 params.py:247] model.coarse_to_fine = True\n",
      "I0630 02:40:15.114669 4665613760 params.py:247] model.inference_order = 2\n",
      "I0630 02:40:15.115227 4665613760 params.py:247] model.lexical_dropout = 0.2\n",
      "I0630 02:40:15.116711 4665613760 params.py:247] model.initializer.regexes.0.1.type = xavier_normal\n",
      "I0630 02:40:15.117777 4665613760 params.py:247] model.initializer.regexes.0.1.gain = 1.0\n",
      "I0630 02:40:15.118623 4665613760 params.py:247] model.initializer.regexes.1.1.type = xavier_normal\n",
      "I0630 02:40:15.119508 4665613760 params.py:247] model.initializer.regexes.1.1.gain = 1.0\n",
      "I0630 02:40:15.120774 4665613760 params.py:247] model.initializer.regexes.2.1.type = xavier_normal\n",
      "I0630 02:40:15.121664 4665613760 params.py:247] model.initializer.regexes.2.1.gain = 1.0\n",
      "I0630 02:40:15.122504 4665613760 params.py:247] model.initializer.regexes.3.1.type = xavier_normal\n",
      "I0630 02:40:15.123278 4665613760 params.py:247] model.initializer.regexes.3.1.gain = 1.0\n",
      "I0630 02:40:15.123914 4665613760 params.py:247] model.initializer.regexes.4.1.type = xavier_normal\n",
      "I0630 02:40:15.124701 4665613760 params.py:247] model.initializer.regexes.4.1.gain = 1.0\n",
      "I0630 02:40:15.125351 4665613760 params.py:247] model.initializer.regexes.5.1.type = xavier_normal\n",
      "I0630 02:40:15.126170 4665613760 params.py:247] model.initializer.regexes.5.1.gain = 1.0\n",
      "I0630 02:40:15.126808 4665613760 params.py:247] model.initializer.regexes.6.1.type = orthogonal\n",
      "I0630 02:40:15.127596 4665613760 params.py:247] model.initializer.regexes.6.1.gain = 1.0\n",
      "I0630 02:40:15.128105 4665613760 params.py:247] model.initializer.prevent_regexes = None\n",
      "I0630 02:40:15.193432 4665613760 initializers.py:471] Initializing parameters\n",
      "I0630 02:40:15.227572 4665613760 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0630 02:40:15.250934 4665613760 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0630 02:40:15.263791 4665613760 initializers.py:481] Initializing _mention_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0630 02:40:15.264651 4665613760 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0630 02:40:15.340538 4665613760 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0630 02:40:15.353191 4665613760 initializers.py:481] Initializing _antecedent_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0630 02:40:15.354360 4665613760 initializers.py:481] Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
      "I0630 02:40:15.355644 4665613760 initializers.py:481] Initializing _distance_embedding.weight using _distance_embedding.weight initializer\n",
      "I0630 02:40:15.357568 4665613760 initializers.py:481] Initializing _coarse2fine_scorer.weight using .*scorer.*weight initializer\n",
      "I0630 02:40:15.411193 4665613760 initializers.py:481] Initializing _span_updating_gated_sum._gate.weight using .*_span_updating_gated_sum.*weight initializer\n",
      "W0630 02:40:15.412041 4665613760 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
      "W0630 02:40:15.412738 4665613760 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "I0630 02:40:15.413553 4665613760 initializers.py:490] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0630 02:40:15.414310 4665613760 initializers.py:496]    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "I0630 02:40:15.414945 4665613760 initializers.py:496]    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "I0630 02:40:15.415796 4665613760 initializers.py:496]    _antecedent_scorer._module.bias\n",
      "I0630 02:40:15.416675 4665613760 initializers.py:496]    _attentive_span_extractor._global_attention._module.bias\n",
      "I0630 02:40:15.417272 4665613760 initializers.py:496]    _attentive_span_extractor._global_attention._module.weight\n",
      "I0630 02:40:15.418050 4665613760 initializers.py:496]    _coarse2fine_scorer.bias\n",
      "I0630 02:40:15.418529 4665613760 initializers.py:496]    _mention_feedforward._module._linear_layers.0.bias\n",
      "I0630 02:40:15.419372 4665613760 initializers.py:496]    _mention_feedforward._module._linear_layers.1.bias\n",
      "I0630 02:40:15.420078 4665613760 initializers.py:496]    _mention_scorer._module.bias\n",
      "I0630 02:40:15.421040 4665613760 initializers.py:496]    _span_updating_gated_sum._gate.bias\n",
      "I0630 02:40:15.422305 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "I0630 02:40:15.423067 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "I0630 02:40:15.423605 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "I0630 02:40:15.424403 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0630 02:40:15.425118 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "I0630 02:40:15.425999 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.426594 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.427297 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0630 02:40:15.427783 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "I0630 02:40:15.428493 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0630 02:40:15.429152 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0630 02:40:15.429875 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0630 02:40:15.430634 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0630 02:40:15.431341 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0630 02:40:15.432048 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0630 02:40:15.432871 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0630 02:40:15.433516 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "I0630 02:40:15.434213 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.434741 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0630 02:40:15.435473 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0630 02:40:15.436054 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0630 02:40:15.437167 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.438069 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.438799 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0630 02:40:15.439569 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0630 02:40:15.440143 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0630 02:40:15.440917 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0630 02:40:15.441462 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0630 02:40:15.442281 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0630 02:40:15.442960 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0630 02:40:15.443728 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0630 02:40:15.444398 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0630 02:40:15.445230 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0630 02:40:15.445850 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0630 02:40:15.446795 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0630 02:40:15.447644 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0630 02:40:15.448688 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0630 02:40:15.449276 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.449973 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.450606 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0630 02:40:15.451535 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0630 02:40:15.452280 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0630 02:40:15.453113 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0630 02:40:15.453747 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0630 02:40:15.455813 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0630 02:40:15.456695 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0630 02:40:15.457350 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0630 02:40:15.458778 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "I0630 02:40:15.459838 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0630 02:40:15.461531 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0630 02:40:15.462311 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0630 02:40:15.463034 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0630 02:40:15.463950 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0630 02:40:15.464658 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.465363 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.466134 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0630 02:40:15.467151 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "I0630 02:40:15.467803 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0630 02:40:15.468383 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "I0630 02:40:15.469349 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0630 02:40:15.470143 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0630 02:40:15.472540 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0630 02:40:15.473979 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "I0630 02:40:15.475621 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.477417 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0630 02:40:15.478288 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0630 02:40:15.478999 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0630 02:40:15.480612 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0630 02:40:15.481667 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0630 02:40:15.483361 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.484628 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.485227 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "I0630 02:40:15.485915 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "I0630 02:40:15.486901 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "I0630 02:40:15.488100 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "I0630 02:40:15.491948 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "I0630 02:40:15.494493 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "I0630 02:40:15.496042 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "I0630 02:40:15.497575 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "I0630 02:40:15.498601 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "I0630 02:40:15.499350 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "I0630 02:40:15.500745 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "I0630 02:40:15.501552 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "I0630 02:40:15.503408 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "I0630 02:40:15.507002 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "I0630 02:40:15.511068 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.513221 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.515587 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "I0630 02:40:15.516441 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "I0630 02:40:15.517158 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "I0630 02:40:15.519389 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "I0630 02:40:15.521306 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "I0630 02:40:15.522307 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "I0630 02:40:15.523616 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n",
      "I0630 02:40:15.524446 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "I0630 02:40:15.525264 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "I0630 02:40:15.526046 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "I0630 02:40:15.527300 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "I0630 02:40:15.527915 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "I0630 02:40:15.528626 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "I0630 02:40:15.529255 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "I0630 02:40:15.530056 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.530685 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.531394 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "I0630 02:40:15.531923 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "I0630 02:40:15.532732 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "I0630 02:40:15.533316 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "I0630 02:40:15.533997 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "I0630 02:40:15.534435 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
      "I0630 02:40:15.535032 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.535615 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "I0630 02:40:15.536262 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "I0630 02:40:15.536721 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "I0630 02:40:15.537533 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "I0630 02:40:15.538249 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "I0630 02:40:15.539096 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "I0630 02:40:15.539580 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "I0630 02:40:15.540405 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.540925 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.541635 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "I0630 02:40:15.542170 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "I0630 02:40:15.542907 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "I0630 02:40:15.543501 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "I0630 02:40:15.544154 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "I0630 02:40:15.544671 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "I0630 02:40:15.545296 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "I0630 02:40:15.545785 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "I0630 02:40:15.546665 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "I0630 02:40:15.547214 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "I0630 02:40:15.547868 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "I0630 02:40:15.548386 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "I0630 02:40:15.549158 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "I0630 02:40:15.549581 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "I0630 02:40:15.550253 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.550705 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.551375 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "I0630 02:40:15.551898 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "I0630 02:40:15.552655 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "I0630 02:40:15.553146 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "I0630 02:40:15.553785 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n",
      "I0630 02:40:15.554527 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "I0630 02:40:15.555379 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "I0630 02:40:15.556191 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "I0630 02:40:15.556905 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "I0630 02:40:15.557573 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "I0630 02:40:15.558238 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "I0630 02:40:15.558833 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "I0630 02:40:15.559603 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "I0630 02:40:15.560256 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "I0630 02:40:15.560908 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.561388 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.562139 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "I0630 02:40:15.562693 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "I0630 02:40:15.563488 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "I0630 02:40:15.564069 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
      "I0630 02:40:15.565140 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.565646 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "I0630 02:40:15.566231 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "I0630 02:40:15.567007 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "I0630 02:40:15.567581 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "I0630 02:40:15.568462 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "I0630 02:40:15.569223 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "I0630 02:40:15.570052 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "I0630 02:40:15.570781 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "I0630 02:40:15.571608 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "I0630 02:40:15.572355 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.573256 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.573850 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "I0630 02:40:15.574746 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "I0630 02:40:15.575401 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "I0630 02:40:15.576282 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "I0630 02:40:15.576965 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "I0630 02:40:15.577656 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "I0630 02:40:15.578201 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "I0630 02:40:15.578884 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "I0630 02:40:15.579503 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "I0630 02:40:15.580276 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "I0630 02:40:15.581164 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "I0630 02:40:15.581817 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "I0630 02:40:15.582629 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "I0630 02:40:15.583384 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "I0630 02:40:15.583845 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.584514 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.585005 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "I0630 02:40:15.585551 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "I0630 02:40:15.586266 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n",
      "I0630 02:40:15.587026 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "I0630 02:40:15.588020 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "I0630 02:40:15.589174 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "I0630 02:40:15.589936 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "I0630 02:40:15.591114 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "I0630 02:40:15.591700 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "I0630 02:40:15.592405 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "I0630 02:40:15.592913 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "I0630 02:40:15.593631 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "I0630 02:40:15.594100 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "I0630 02:40:15.594669 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "I0630 02:40:15.595329 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.596037 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.596868 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0630 02:40:15.597477 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "I0630 02:40:15.598026 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.598642 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0630 02:40:15.599194 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0630 02:40:15.599787 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0630 02:40:15.600318 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0630 02:40:15.601143 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0630 02:40:15.601706 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0630 02:40:15.602259 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0630 02:40:15.602898 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0630 02:40:15.603715 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0630 02:40:15.604677 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0630 02:40:15.605484 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0630 02:40:15.606118 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.606874 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.607486 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "I0630 02:40:15.608217 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "I0630 02:40:15.609035 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "I0630 02:40:15.609744 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "I0630 02:40:15.610368 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "I0630 02:40:15.610939 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "I0630 02:40:15.611455 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "I0630 02:40:15.612179 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "I0630 02:40:15.612681 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "I0630 02:40:15.613462 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "I0630 02:40:15.614117 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "I0630 02:40:15.614817 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "I0630 02:40:15.615375 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "I0630 02:40:15.616058 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "I0630 02:40:15.616617 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.617402 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.618096 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n",
      "I0630 02:40:15.618821 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "I0630 02:40:15.619536 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "I0630 02:40:15.620187 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "I0630 02:40:15.621050 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "I0630 02:40:15.621685 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "I0630 02:40:15.622191 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "I0630 02:40:15.622814 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "I0630 02:40:15.623328 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "I0630 02:40:15.624017 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "I0630 02:40:15.624512 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "I0630 02:40:15.625091 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "I0630 02:40:15.625480 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "I0630 02:40:15.625927 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "I0630 02:40:15.626437 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.626895 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.627321 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.627923 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "I0630 02:40:15.628472 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "I0630 02:40:15.629264 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "I0630 02:40:15.629813 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "I0630 02:40:15.630426 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "I0630 02:40:15.630892 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "I0630 02:40:15.631358 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "I0630 02:40:15.631772 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "I0630 02:40:15.632210 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "I0630 02:40:15.632741 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "I0630 02:40:15.633137 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "I0630 02:40:15.633611 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "I0630 02:40:15.634138 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "I0630 02:40:15.634846 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.635523 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.635921 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "I0630 02:40:15.636440 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "I0630 02:40:15.636874 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "I0630 02:40:15.637454 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "I0630 02:40:15.638213 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "I0630 02:40:15.638825 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "I0630 02:40:15.639873 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "I0630 02:40:15.640475 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "I0630 02:40:15.640936 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "I0630 02:40:15.641393 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "I0630 02:40:15.642167 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "I0630 02:40:15.642650 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "I0630 02:40:15.643392 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "I0630 02:40:15.643904 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "I0630 02:40:15.644570 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.644963 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.645499 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0630 02:40:15.646021 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0630 02:40:15.646449 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0630 02:40:15.647016 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0630 02:40:15.647790 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0630 02:40:15.648175 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0630 02:40:15.648617 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0630 02:40:15.649201 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0630 02:40:15.649583 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0630 02:40:15.650201 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0630 02:40:15.650828 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0630 02:40:15.651492 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0630 02:40:15.652502 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0630 02:40:15.652954 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "I0630 02:40:15.653482 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.654189 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.654927 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0630 02:40:15.655588 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0630 02:40:15.656161 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0630 02:40:15.656651 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "I0630 02:40:15.657102 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0630 02:40:15.657639 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "I0630 02:40:15.658188 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0630 02:40:15.658828 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0630 02:40:15.659333 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0630 02:40:15.659909 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0630 02:40:15.660367 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0630 02:40:15.661043 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0630 02:40:15.661455 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0630 02:40:15.661878 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0630 02:40:15.662466 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.662991 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.663746 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0630 02:40:15.664289 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0630 02:40:15.664906 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0630 02:40:15.665364 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0630 02:40:15.665850 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0630 02:40:15.666610 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0630 02:40:15.667076 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0630 02:40:15.667568 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0630 02:40:15.668012 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0630 02:40:15.668614 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0630 02:40:15.669093 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0630 02:40:15.669689 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0630 02:40:15.670169 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0630 02:40:15.670850 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "I0630 02:40:15.672048 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.672746 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.673317 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0630 02:40:15.673712 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0630 02:40:15.674158 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0630 02:40:15.674715 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0630 02:40:15.675394 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0630 02:40:15.676242 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0630 02:40:15.676720 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0630 02:40:15.677231 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0630 02:40:15.677731 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0630 02:40:15.678165 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0630 02:40:15.678677 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0630 02:40:15.679199 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "I0630 02:40:15.679781 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0630 02:40:15.680403 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.680875 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.681282 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.682022 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0630 02:40:15.682528 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0630 02:40:15.683198 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0630 02:40:15.683655 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0630 02:40:15.684178 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0630 02:40:15.684687 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "I0630 02:40:15.685148 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0630 02:40:15.685839 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "I0630 02:40:15.686244 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0630 02:40:15.686745 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0630 02:40:15.687246 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0630 02:40:15.687971 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0630 02:40:15.688745 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0630 02:40:15.689270 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0630 02:40:15.690169 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.691581 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.692270 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0630 02:40:15.693342 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0630 02:40:15.694010 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0630 02:40:15.694719 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0630 02:40:15.695258 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0630 02:40:15.696126 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0630 02:40:15.696841 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0630 02:40:15.697842 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0630 02:40:15.698281 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0630 02:40:15.698833 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0630 02:40:15.699411 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0630 02:40:15.699976 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "I0630 02:40:15.700506 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0630 02:40:15.701191 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0630 02:40:15.701730 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0630 02:40:15.702370 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0630 02:40:15.702938 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0630 02:40:15.704118 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0630 02:40:15.704983 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0630 02:40:15.705729 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0630 02:40:15.706248 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0630 02:40:15.706682 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0630 02:40:15.707125 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0630 02:40:15.707644 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0630 02:40:15.708130 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0630 02:40:15.708659 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "I0630 02:40:15.709522 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0630 02:40:15.710033 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0630 02:40:15.710572 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0630 02:40:15.747467 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0630 02:40:15.748168 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "I0630 02:40:15.749085 4665613760 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "I0630 02:40:15.759263 4665613760 embedding.py:252] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0630 02:40:15.760426 4665613760 embedding.py:252] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0630 02:40:20.295145 4665613760 params.py:247] validation_dataset_reader.type = coref\n",
      "I0630 02:40:20.296492 4665613760 params.py:247] validation_dataset_reader.lazy = False\n",
      "I0630 02:40:20.297413 4665613760 params.py:247] validation_dataset_reader.cache_directory = None\n",
      "I0630 02:40:20.298041 4665613760 params.py:247] validation_dataset_reader.max_instances = None\n",
      "I0630 02:40:20.299005 4665613760 params.py:247] validation_dataset_reader.max_span_width = 30\n",
      "I0630 02:40:20.299846 4665613760 params.py:247] validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0630 02:40:20.300822 4665613760 params.py:247] validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0630 02:40:20.301409 4665613760 params.py:247] validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0630 02:40:20.302362 4665613760 params.py:247] validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0630 02:40:20.303119 4665613760 params.py:247] validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0630 02:40:21.487050 4665613760 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0630 02:40:21.488402 4665613760 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0630 02:40:21.489575 4665613760 tokenization_utils.py:420] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0630 02:40:25.992532 4665613760 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0630 02:40:25.993487 4665613760 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0630 02:40:25.994170 4665613760 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0630 02:40:25.994984 4665613760 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0630 02:40:26.029384 4665613760 params.py:247] validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0630 02:40:26.030077 4665613760 params.py:247] validation_dataset_reader.max_sentences = None\n",
      "I0630 02:40:26.030622 4665613760 params.py:247] validation_dataset_reader.remove_singleton_clusters = False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked', 'disclosed to', 'told'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified', 'spoke with'] ##  pronoun2 = ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    return ['He', 'She', 'They']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"librarian\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"about \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "\n",
    "action1 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p1[1]] = [action1] ### disclosed to\n",
    "verb_action[verb_list_p1[2]] = [action1] ### told\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action1] ### notified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spoke with \n",
    "### asked\n",
    "action2 = ['painting', 'dancing',\n",
    "           'fencing', 'gymnastics',\n",
    "           'independent cinema', 'woodworking',\n",
    "           'studying', 'horse racing',\n",
    "           'singing', 'kayaking', 'football',\n",
    "           'baseball', 'basketball', 'quizzing', \n",
    "           'gardening', 'blogging', 'board games', \n",
    "           'breadmaking', 'baking']\n",
    "verb_action[verb_list_p2[1]] = [action2]\n",
    "verb_action[verb_list_p1[0]] = [action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_verb = ['standing', 'speaking', 'talking', \n",
    "                  'passing by', 'sitting', 'sitting on the chair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    output = predictor.predict(document = sentence)\n",
    "    return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(oc1, oc2, verb, action, pronoun):\n",
    "\n",
    "\n",
    "    aux_verb = random.choice(auxiliary_verb)\n",
    "    input1 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[0] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "\n",
    "    input2 = (\"The \" + oc1 + \" was \" + aux_verb + \". \" + pronoun[1] + \" \" \n",
    "              + verb + \" \" + \"the \" + oc2 + \" \" + filler_conjunction[verb] \n",
    "              + action + '.') \n",
    "        \n",
    "    return input1, input2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "occupation_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "occupation_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 0\n",
      "------------------------------\n",
      "Unique errors: 20\n",
      "Unique inputs: 30\n",
      "------------------------------\n",
      "Unique errors: 42\n",
      "Unique inputs: 60\n",
      "------------------------------\n",
      "Unique errors: 65\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 88\n",
      "Unique inputs: 120\n",
      "------------------------------\n",
      "Unique errors: 109\n",
      "Unique inputs: 150\n",
      "------------------------------\n",
      "Unique errors: 129\n",
      "Unique inputs: 180\n",
      "------------------------------\n",
      "Unique errors: 148\n",
      "Unique inputs: 210\n",
      "------------------------------\n",
      "Unique errors: 164\n",
      "Unique inputs: 240\n",
      "------------------------------\n",
      "Unique errors: 179\n",
      "Unique inputs: 270\n",
      "------------------------------\n",
      "Unique errors: 201\n",
      "Unique inputs: 300\n",
      "------------------------------\n",
      "Unique errors: 220\n",
      "Unique inputs: 330\n",
      "------------------------------\n",
      "Unique errors: 236\n",
      "Unique inputs: 360\n",
      "------------------------------\n",
      "Unique errors: 260\n",
      "Unique inputs: 390\n",
      "------------------------------\n",
      "Unique errors: 277\n",
      "Unique inputs: 420\n",
      "------------------------------\n",
      "Unique errors: 300\n",
      "Unique inputs: 450\n",
      "------------------------------\n",
      "Unique errors: 318\n",
      "Unique inputs: 480\n",
      "------------------------------\n",
      "Unique errors: 337\n",
      "Unique inputs: 510\n",
      "------------------------------\n",
      "Unique errors: 354\n",
      "Unique inputs: 539\n",
      "------------------------------\n",
      "Unique errors: 375\n",
      "Unique inputs: 569\n",
      "------------------------------\n",
      "Unique errors: 399\n",
      "Unique inputs: 599\n",
      "------------------------------\n",
      "Unique errors: 418\n",
      "Unique inputs: 629\n",
      "------------------------------\n",
      "Unique errors: 438\n",
      "Unique inputs: 659\n",
      "------------------------------\n",
      "Unique errors: 460\n",
      "Unique inputs: 689\n",
      "------------------------------\n",
      "Unique errors: 477\n",
      "Unique inputs: 719\n",
      "------------------------------\n",
      "Unique errors: 494\n",
      "Unique inputs: 749\n",
      "------------------------------\n",
      "Unique errors: 515\n",
      "Unique inputs: 779\n",
      "------------------------------\n",
      "Unique errors: 535\n",
      "Unique inputs: 809\n",
      "------------------------------\n",
      "Unique errors: 556\n",
      "Unique inputs: 839\n",
      "------------------------------\n",
      "Unique errors: 579\n",
      "Unique inputs: 869\n",
      "------------------------------\n",
      "Unique errors: 598\n",
      "Unique inputs: 899\n",
      "------------------------------\n",
      "Unique errors: 616\n",
      "Unique inputs: 929\n",
      "------------------------------\n",
      "Unique errors: 634\n",
      "Unique inputs: 959\n",
      "------------------------------\n",
      "Unique errors: 652\n",
      "Unique inputs: 989\n",
      "------------------------------\n",
      "Unique errors: 668\n",
      "Unique inputs: 1019\n",
      "------------------------------\n",
      "Unique errors: 686\n",
      "Unique inputs: 1048\n",
      "------------------------------\n",
      "Unique errors: 705\n",
      "Unique inputs: 1078\n",
      "------------------------------\n",
      "Unique errors: 721\n",
      "Unique inputs: 1108\n",
      "------------------------------\n",
      "Unique errors: 744\n",
      "Unique inputs: 1138\n",
      "------------------------------\n",
      "Unique errors: 762\n",
      "Unique inputs: 1168\n",
      "------------------------------\n",
      "Unique errors: 784\n",
      "Unique inputs: 1198\n",
      "------------------------------\n",
      "Unique errors: 806\n",
      "Unique inputs: 1228\n",
      "------------------------------\n",
      "Unique errors: 825\n",
      "Unique inputs: 1258\n",
      "------------------------------\n",
      "Unique errors: 845\n",
      "Unique inputs: 1288\n",
      "------------------------------\n",
      "Unique errors: 863\n",
      "Unique inputs: 1318\n",
      "------------------------------\n",
      "Unique errors: 883\n",
      "Unique inputs: 1348\n",
      "------------------------------\n",
      "Unique errors: 907\n",
      "Unique inputs: 1378\n",
      "------------------------------\n",
      "Unique errors: 923\n",
      "Unique inputs: 1408\n",
      "------------------------------\n",
      "Unique errors: 945\n",
      "Unique inputs: 1438\n",
      "------------------------------\n",
      "Unique errors: 966\n",
      "Unique inputs: 1468\n",
      "------------------------------\n",
      "Unique errors: 982\n",
      "Unique inputs: 1498\n",
      "------------------------------\n",
      "Unique errors: 1006\n",
      "Unique inputs: 1528\n",
      "------------------------------\n",
      "Unique errors: 1025\n",
      "Unique inputs: 1558\n",
      "------------------------------\n",
      "Unique errors: 1040\n",
      "Unique inputs: 1588\n",
      "------------------------------\n",
      "Unique errors: 1057\n",
      "Unique inputs: 1617\n",
      "------------------------------\n",
      "Unique errors: 1075\n",
      "Unique inputs: 1647\n",
      "------------------------------\n",
      "Unique errors: 1092\n",
      "Unique inputs: 1677\n",
      "------------------------------\n",
      "Unique errors: 1115\n",
      "Unique inputs: 1707\n",
      "------------------------------\n",
      "Unique errors: 1137\n",
      "Unique inputs: 1737\n",
      "------------------------------\n",
      "Unique errors: 1156\n",
      "Unique inputs: 1766\n",
      "------------------------------\n",
      "Unique errors: 1176\n",
      "Unique inputs: 1796\n",
      "------------------------------\n",
      "Unique errors: 1194\n",
      "Unique inputs: 1826\n",
      "------------------------------\n",
      "Unique errors: 1210\n",
      "Unique inputs: 1856\n",
      "------------------------------\n",
      "Unique errors: 1230\n",
      "Unique inputs: 1885\n",
      "------------------------------\n",
      "Unique errors: 1249\n",
      "Unique inputs: 1915\n",
      "------------------------------\n",
      "Unique errors: 1269\n",
      "Unique inputs: 1945\n",
      "------------------------------\n",
      "Unique errors: 1292\n",
      "Unique inputs: 1974\n",
      "------------------------------\n",
      "Unique errors: 1315\n",
      "Unique inputs: 2004\n",
      "------------------------------\n",
      "Unique errors: 1340\n",
      "Unique inputs: 2034\n",
      "------------------------------\n",
      "Unique errors: 1360\n",
      "Unique inputs: 2064\n",
      "------------------------------\n",
      "Unique errors: 1383\n",
      "Unique inputs: 2094\n",
      "------------------------------\n",
      "Unique errors: 1402\n",
      "Unique inputs: 2123\n",
      "------------------------------\n",
      "Unique errors: 1423\n",
      "Unique inputs: 2152\n",
      "------------------------------\n",
      "Unique errors: 1444\n",
      "Unique inputs: 2181\n",
      "------------------------------\n",
      "Unique errors: 1461\n",
      "Unique inputs: 2211\n",
      "------------------------------\n",
      "Unique errors: 1482\n",
      "Unique inputs: 2240\n",
      "------------------------------\n",
      "Unique errors: 1502\n",
      "Unique inputs: 2270\n",
      "------------------------------\n",
      "Unique errors: 1522\n",
      "Unique inputs: 2300\n",
      "------------------------------\n",
      "Unique errors: 1543\n",
      "Unique inputs: 2330\n",
      "------------------------------\n",
      "Unique errors: 1562\n",
      "Unique inputs: 2360\n",
      "------------------------------\n",
      "Unique errors: 1578\n",
      "Unique inputs: 2390\n",
      "------------------------------\n",
      "Unique errors: 1594\n",
      "Unique inputs: 2420\n",
      "------------------------------\n",
      "Unique errors: 1616\n",
      "Unique inputs: 2449\n",
      "------------------------------\n",
      "Unique errors: 1636\n",
      "Unique inputs: 2479\n",
      "------------------------------\n",
      "Unique errors: 1661\n",
      "Unique inputs: 2509\n",
      "------------------------------\n",
      "Unique errors: 1679\n",
      "Unique inputs: 2539\n",
      "------------------------------\n",
      "Unique errors: 1699\n",
      "Unique inputs: 2567\n",
      "------------------------------\n",
      "Unique errors: 1718\n",
      "Unique inputs: 2596\n",
      "------------------------------\n",
      "Unique errors: 1736\n",
      "Unique inputs: 2626\n",
      "------------------------------\n",
      "Unique errors: 1759\n",
      "Unique inputs: 2656\n",
      "------------------------------\n",
      "Unique errors: 1779\n",
      "Unique inputs: 2686\n",
      "------------------------------\n",
      "Unique errors: 1802\n",
      "Unique inputs: 2716\n",
      "------------------------------\n",
      "Unique errors: 1822\n",
      "Unique inputs: 2745\n",
      "------------------------------\n",
      "Unique errors: 1837\n",
      "Unique inputs: 2774\n",
      "------------------------------\n",
      "Unique errors: 1859\n",
      "Unique inputs: 2803\n",
      "------------------------------\n",
      "Unique errors: 1881\n",
      "Unique inputs: 2833\n",
      "------------------------------\n",
      "Unique errors: 1903\n",
      "Unique inputs: 2862\n",
      "------------------------------\n",
      "Unique errors: 1923\n",
      "Unique inputs: 2892\n",
      "------------------------------\n",
      "Unique errors: 1944\n",
      "Unique inputs: 2922\n",
      "------------------------------\n",
      "Unique errors: 1963\n",
      "Unique inputs: 2952\n",
      "------------------------------\n",
      "1993\n",
      "0.6643333333333333\n",
      "Final Unique errors: 1980\n",
      "Final Unique inputs: 2982\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 3000\n",
    "\n",
    "RELAXED_ERROR = True\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    oc1 = random.choice(occupations_1)\n",
    "    oc2 = random.choice(occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    \n",
    "    input1, input2 = generate_sentences(oc1, oc2, verb, action, pronoun)\n",
    "    \n",
    "    pred1, _ = predict_clusters(input1)\n",
    "    pred2, _ = predict_clusters(input2)\n",
    "#     pred3, _ = predict_clusters(input2)\n",
    "    \n",
    "    \n",
    "    if(i % 30 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add(input1)\n",
    "    \n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    \n",
    "#     print(pred1, pred2)\n",
    "#     print(input1)\n",
    "#     print(input2)\n",
    "#     print()\n",
    "\n",
    "                \n",
    "\n",
    "    \n",
    "\n",
    "    if not (pred1 == pred2):\n",
    "#         if ((len(pred1) > 0 and len(pred2) > 0)):\n",
    "#             if ((len(pred1[0]) == len(pred2[0]))):\n",
    "                err_count += 1\n",
    "                \n",
    "                unique_input1_error_set.add(input1)\n",
    "                \n",
    "#                 print(pred1, pred2, pred3)\n",
    "#                 print(input1)\n",
    "#                 print(input2)\n",
    "#                 print(input3)\n",
    "                \n",
    "                update_dict(occupation_pair_error, (oc1, oc2))\n",
    "                update_dict(occupation1_error, oc1)\n",
    "                update_dict(occupation2_error, oc2)\n",
    "                update_dict(verb_error, verb)\n",
    "                update_dict(action_error, action)\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The physician was standing. He disclosed to the housekeeper that the cookies were baked.', 'The technician was talking. He told the counselor that the concert was appreciated.', 'The guard was sitting on the chair. He told the assistant that the delivery was made.', 'The developer was sitting on the chair. He spoke with the teacher about fencing.', 'The farmer was sitting on the chair. He asked the assistant about basketball.']\n",
      "\n",
      "['The physician was standing. He disclosed to the housekeeper that the cookies were baked.', 'The technician was talking. He told the counselor that the concert was appreciated.', 'The guard was sitting on the chair. He told the assistant that the delivery was made.', 'The farmer was sitting on the chair. He asked the assistant about basketball.', 'The engineer was passing by. He spoke with the teacher about football.', 'The farmer was passing by. He asked the clerk about dancing.', 'The accountant was talking. He asked the paralegal about independent cinema.', 'The driver was sitting on the chair. He spoke with the paralegal about horse racing.', 'The farmer was sitting on the chair. He spoke with the baker about studying.', 'The farmer was talking. He asked the tailor about studying.']\n"
     ]
    }
   ],
   "source": [
    "# print(occupation_pair_count)\n",
    "# print(occupation1_count)\n",
    "# print(occupation2_count)\n",
    "# print(verb_count)\n",
    "# print(action_count)\n",
    "print(list(unique_input1_set)[0:5])\n",
    "print()\n",
    "print(list(unique_input1_error_set)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('saved_pickles/Exploration/unique_input1_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_error_set, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_count, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_count, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_error, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 if((oc1, oc2) in occupation_pair_error.keys()):\n",
    "#                     occupation_pair_error[(oc1, oc2)] += 1\n",
    "#                 else:\n",
    "#                     occupation_pair_error[(oc1, oc2)] = 1\n",
    "                                          \n",
    "#                 if(oc1 in occupation1_error.keys()):\n",
    "#                     occupation1_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation1_error[oc1] = 1\n",
    "                \n",
    "#                 if(oc2 in occupation2_error.keys()):\n",
    "#                     occupation2_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation2_error[oc1] = 1\n",
    "                                          \n",
    "#                 if(verb in verb_error.keys()):\n",
    "#                     verb_error[verb] += 1\n",
    "#                 else:\n",
    "#                     verb_error[verb] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_clusters(sentence):\n",
    "#     output = predictor.predict(document = sentence)\n",
    "#     return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " {'top_spans': [[0, 1], [5, 5], [6, 6], [7, 8], [10, 11], [13, 13]],\n",
       "  'antecedent_indices': [[0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5],\n",
       "   [0, 1, 2, 3, 4, 5]],\n",
       "  'predicted_antecedents': [-1, -1, -1, -1, -1, -1],\n",
       "  'document': ['The',\n",
       "   'developer',\n",
       "   'was',\n",
       "   'standing',\n",
       "   '.',\n",
       "   'She',\n",
       "   'notified',\n",
       "   'the',\n",
       "   'nurse',\n",
       "   'that',\n",
       "   'the',\n",
       "   'work',\n",
       "   'was',\n",
       "   'unsatisfactory',\n",
       "   '.'],\n",
       "  'clusters': []})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_clusters(\"The developer was standing. She notified the nurse that the work was unsatisfactory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_count = 0\n",
    "# ITERS = 20\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(ITERS):\n",
    "#     oc1 = random.choice(occupations_1)\n",
    "#     oc2 = random.choice(occupations_2)\n",
    "#     verb = random.choice(list(verb_action.keys()))\n",
    "#     action = random.choice(random.choice(verb_action[verb]))\n",
    "#     in1 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     in2 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action) \n",
    "    \n",
    "#     in3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "#     pred1, _ = predict_clusters(in1)\n",
    "#     pred2, _ = predict_clusters(in2)\n",
    "#     pred3, _ = predict_clusters(in2)\n",
    "    \n",
    "#     if not (pred1 == pred2 and pred2 == pred3):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#             err_count += 1\n",
    "\n",
    "#             print(pred1, pred2, pred3)\n",
    "#             print(in1)\n",
    "#             print(in2)\n",
    "#             print(in3)\n",
    "#             print()\n",
    "    \n",
    "\n",
    "# print(err_count)\n",
    "# print(err_count/ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
