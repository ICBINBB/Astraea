{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:18.243547 4686319040 archival.py:164] loading archive file https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz from cache at /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0619 16:50:18.246519 4686319040 archival.py:171] extracting archive file /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174 to temp dir /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmpdo37bp77\n",
      "I0619 16:50:29.319860 4686319040 params.py:247] type = from_instances\n",
      "I0619 16:50:29.321023 4686319040 vocabulary.py:314] Loading token dictionary from /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmpdo37bp77/vocabulary.\n",
      "I0619 16:50:29.324124 4686319040 params.py:247] model.type = coref\n",
      "I0619 16:50:29.326133 4686319040 params.py:247] model.regularizer = None\n",
      "I0619 16:50:29.327081 4686319040 params.py:247] model.text_field_embedder.type = basic\n",
      "I0619 16:50:29.328428 4686319040 params.py:247] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "I0619 16:50:29.329649 4686319040 params.py:247] model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0619 16:50:29.332423 4686319040 params.py:247] model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "I0619 16:50:30.386964 4686319040 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0619 16:50:30.388569 4686319040 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0619 16:50:31.545029 4686319040 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/pytorch_model.bin from cache at /Users/sakshiudeshi/.cache/torch/transformers/d707dadfcbbac6a5fc440f1e94db728b000a2816693f44a87092182199f2d52d.d1ce6dff7f84348ad7c77a33a9a6e8751099db9c9d609ac7752e61804befe4da\n",
      "I0619 16:50:36.881474 4686319040 modeling_utils.py:601] Weights of BertModel not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "I0619 16:50:37.928474 4686319040 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0619 16:50:37.929380 4686319040 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0619 16:50:37.930134 4686319040 tokenization_utils.py:420] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0619 16:50:42.265742 4686319040 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0619 16:50:42.266522 4686319040 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0619 16:50:42.267272 4686319040 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0619 16:50:42.268042 4686319040 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0619 16:50:42.389832 4686319040 params.py:247] model.context_layer.type = pass_through\n",
      "I0619 16:50:42.390814 4686319040 params.py:247] model.context_layer.input_dim = 1024\n",
      "I0619 16:50:42.391940 4686319040 params.py:247] model.mention_feedforward.input_dim = 3092\n",
      "I0619 16:50:42.392790 4686319040 params.py:247] model.mention_feedforward.num_layers = 2\n",
      "I0619 16:50:42.393332 4686319040 params.py:247] model.mention_feedforward.hidden_dims = 1500\n",
      "I0619 16:50:42.394124 4686319040 params.py:247] model.mention_feedforward.activations = relu\n",
      "I0619 16:50:42.395258 4686319040 params.py:247] type = relu\n",
      "I0619 16:50:42.396605 4686319040 params.py:247] model.mention_feedforward.dropout = 0.3\n",
      "I0619 16:50:42.442676 4686319040 params.py:247] model.antecedent_feedforward.input_dim = 9296\n",
      "I0619 16:50:42.443468 4686319040 params.py:247] model.antecedent_feedforward.num_layers = 2\n",
      "I0619 16:50:42.444150 4686319040 params.py:247] model.antecedent_feedforward.hidden_dims = 1500\n",
      "I0619 16:50:42.444889 4686319040 params.py:247] model.antecedent_feedforward.activations = relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:42.445754 4686319040 params.py:247] type = relu\n",
      "I0619 16:50:42.447352 4686319040 params.py:247] model.antecedent_feedforward.dropout = 0.3\n",
      "I0619 16:50:42.546466 4686319040 params.py:247] model.feature_size = 20\n",
      "I0619 16:50:42.547563 4686319040 params.py:247] model.max_span_width = 30\n",
      "I0619 16:50:42.548222 4686319040 params.py:247] model.spans_per_word = 0.4\n",
      "I0619 16:50:42.549205 4686319040 params.py:247] model.max_antecedents = 50\n",
      "I0619 16:50:42.549854 4686319040 params.py:247] model.coarse_to_fine = True\n",
      "I0619 16:50:42.550580 4686319040 params.py:247] model.inference_order = 2\n",
      "I0619 16:50:42.551362 4686319040 params.py:247] model.lexical_dropout = 0.2\n",
      "I0619 16:50:42.552927 4686319040 params.py:247] model.initializer.regexes.0.1.type = xavier_normal\n",
      "I0619 16:50:42.553997 4686319040 params.py:247] model.initializer.regexes.0.1.gain = 1.0\n",
      "I0619 16:50:42.555132 4686319040 params.py:247] model.initializer.regexes.1.1.type = xavier_normal\n",
      "I0619 16:50:42.556149 4686319040 params.py:247] model.initializer.regexes.1.1.gain = 1.0\n",
      "I0619 16:50:42.556771 4686319040 params.py:247] model.initializer.regexes.2.1.type = xavier_normal\n",
      "I0619 16:50:42.557618 4686319040 params.py:247] model.initializer.regexes.2.1.gain = 1.0\n",
      "I0619 16:50:42.558635 4686319040 params.py:247] model.initializer.regexes.3.1.type = xavier_normal\n",
      "I0619 16:50:42.559918 4686319040 params.py:247] model.initializer.regexes.3.1.gain = 1.0\n",
      "I0619 16:50:42.561381 4686319040 params.py:247] model.initializer.regexes.4.1.type = xavier_normal\n",
      "I0619 16:50:42.562579 4686319040 params.py:247] model.initializer.regexes.4.1.gain = 1.0\n",
      "I0619 16:50:42.563823 4686319040 params.py:247] model.initializer.regexes.5.1.type = xavier_normal\n",
      "I0619 16:50:42.564948 4686319040 params.py:247] model.initializer.regexes.5.1.gain = 1.0\n",
      "I0619 16:50:42.565945 4686319040 params.py:247] model.initializer.regexes.6.1.type = orthogonal\n",
      "I0619 16:50:42.566702 4686319040 params.py:247] model.initializer.regexes.6.1.gain = 1.0\n",
      "I0619 16:50:42.567708 4686319040 params.py:247] model.initializer.prevent_regexes = None\n",
      "I0619 16:50:42.630110 4686319040 initializers.py:471] Initializing parameters\n",
      "I0619 16:50:42.657844 4686319040 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0619 16:50:42.684267 4686319040 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0619 16:50:42.700314 4686319040 initializers.py:481] Initializing _mention_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0619 16:50:42.701090 4686319040 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0619 16:50:42.781024 4686319040 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0619 16:50:42.794373 4686319040 initializers.py:481] Initializing _antecedent_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0619 16:50:42.795278 4686319040 initializers.py:481] Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
      "I0619 16:50:42.795921 4686319040 initializers.py:481] Initializing _distance_embedding.weight using _distance_embedding.weight initializer\n",
      "I0619 16:50:42.796864 4686319040 initializers.py:481] Initializing _coarse2fine_scorer.weight using .*scorer.*weight initializer\n",
      "I0619 16:50:42.849771 4686319040 initializers.py:481] Initializing _span_updating_gated_sum._gate.weight using .*_span_updating_gated_sum.*weight initializer\n",
      "W0619 16:50:42.850610 4686319040 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
      "W0619 16:50:42.851403 4686319040 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "I0619 16:50:42.852380 4686319040 initializers.py:490] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0619 16:50:42.853392 4686319040 initializers.py:496]    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "I0619 16:50:42.854241 4686319040 initializers.py:496]    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "I0619 16:50:42.855059 4686319040 initializers.py:496]    _antecedent_scorer._module.bias\n",
      "I0619 16:50:42.855802 4686319040 initializers.py:496]    _attentive_span_extractor._global_attention._module.bias\n",
      "I0619 16:50:42.856482 4686319040 initializers.py:496]    _attentive_span_extractor._global_attention._module.weight\n",
      "I0619 16:50:42.857234 4686319040 initializers.py:496]    _coarse2fine_scorer.bias\n",
      "I0619 16:50:42.857980 4686319040 initializers.py:496]    _mention_feedforward._module._linear_layers.0.bias\n",
      "I0619 16:50:42.858810 4686319040 initializers.py:496]    _mention_feedforward._module._linear_layers.1.bias\n",
      "I0619 16:50:42.859498 4686319040 initializers.py:496]    _mention_scorer._module.bias\n",
      "I0619 16:50:42.860011 4686319040 initializers.py:496]    _span_updating_gated_sum._gate.bias\n",
      "I0619 16:50:42.860981 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "I0619 16:50:42.863392 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "I0619 16:50:42.865128 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "I0619 16:50:42.866005 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0619 16:50:42.867117 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "I0619 16:50:42.868020 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.868869 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.869569 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0619 16:50:42.870285 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "I0619 16:50:42.871252 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0619 16:50:42.872132 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0619 16:50:42.873082 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0619 16:50:42.873553 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0619 16:50:42.874288 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0619 16:50:42.874913 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0619 16:50:42.875457 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0619 16:50:42.876008 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "I0619 16:50:42.876790 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:42.877315 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0619 16:50:42.877995 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0619 16:50:42.878535 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0619 16:50:42.879405 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.880681 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.881403 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0619 16:50:42.882059 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0619 16:50:42.882928 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0619 16:50:42.883519 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0619 16:50:42.884294 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0619 16:50:42.884792 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0619 16:50:42.885333 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0619 16:50:42.885949 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0619 16:50:42.886937 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0619 16:50:42.887712 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0619 16:50:42.888520 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0619 16:50:42.889178 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0619 16:50:42.890047 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0619 16:50:42.890547 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0619 16:50:42.891514 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.893339 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.894479 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0619 16:50:42.895996 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0619 16:50:42.897305 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0619 16:50:42.898380 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0619 16:50:42.899106 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0619 16:50:42.899781 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0619 16:50:42.900487 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0619 16:50:42.901063 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0619 16:50:42.901717 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "I0619 16:50:42.902327 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0619 16:50:42.903138 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0619 16:50:42.903847 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0619 16:50:42.904676 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0619 16:50:42.905278 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0619 16:50:42.906205 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.907667 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.908478 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0619 16:50:42.909322 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "I0619 16:50:42.909978 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0619 16:50:42.910568 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "I0619 16:50:42.911611 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0619 16:50:42.912533 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0619 16:50:42.913818 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0619 16:50:42.914731 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "I0619 16:50:42.915544 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:42.916455 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0619 16:50:42.917186 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0619 16:50:42.917978 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0619 16:50:42.918653 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0619 16:50:42.919456 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0619 16:50:42.920135 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.921308 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.922146 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "I0619 16:50:42.922806 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "I0619 16:50:42.923341 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "I0619 16:50:42.923837 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "I0619 16:50:42.924280 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "I0619 16:50:42.924914 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "I0619 16:50:42.925511 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "I0619 16:50:42.926208 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "I0619 16:50:42.926681 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "I0619 16:50:42.927417 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "I0619 16:50:42.927927 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "I0619 16:50:42.928856 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "I0619 16:50:42.929701 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "I0619 16:50:42.930982 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "I0619 16:50:42.931695 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.932686 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.933940 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "I0619 16:50:42.934821 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "I0619 16:50:42.935590 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "I0619 16:50:42.936517 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "I0619 16:50:42.937879 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "I0619 16:50:42.938508 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "I0619 16:50:42.939151 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n",
      "I0619 16:50:42.939966 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "I0619 16:50:42.940549 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "I0619 16:50:42.941504 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "I0619 16:50:42.942028 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "I0619 16:50:42.942891 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "I0619 16:50:42.943476 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "I0619 16:50:42.944267 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "I0619 16:50:42.945091 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.946082 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.947546 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "I0619 16:50:42.949398 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "I0619 16:50:42.950176 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "I0619 16:50:42.950734 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "I0619 16:50:42.951477 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "I0619 16:50:42.952256 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
      "I0619 16:50:42.952931 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:42.953657 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "I0619 16:50:42.954453 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "I0619 16:50:42.955192 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "I0619 16:50:42.955837 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "I0619 16:50:42.956459 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "I0619 16:50:42.957265 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "I0619 16:50:42.957859 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "I0619 16:50:42.958490 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.959059 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.959698 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "I0619 16:50:42.960395 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "I0619 16:50:42.961281 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "I0619 16:50:42.962434 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "I0619 16:50:42.963218 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "I0619 16:50:42.965167 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "I0619 16:50:42.967515 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "I0619 16:50:42.968280 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "I0619 16:50:42.969078 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "I0619 16:50:42.969999 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "I0619 16:50:42.970866 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "I0619 16:50:42.971924 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "I0619 16:50:42.972630 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "I0619 16:50:42.973432 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "I0619 16:50:42.974195 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.974982 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.975607 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "I0619 16:50:42.976291 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "I0619 16:50:42.976799 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "I0619 16:50:42.977501 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "I0619 16:50:42.978055 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n",
      "I0619 16:50:42.978848 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "I0619 16:50:42.979754 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "I0619 16:50:42.981199 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "I0619 16:50:42.981968 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "I0619 16:50:42.982770 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "I0619 16:50:42.983510 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "I0619 16:50:42.984101 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "I0619 16:50:42.984753 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "I0619 16:50:42.985339 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "I0619 16:50:42.986142 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "I0619 16:50:42.987522 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "I0619 16:50:42.989363 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "I0619 16:50:42.990063 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "I0619 16:50:42.990612 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "I0619 16:50:42.991418 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
      "I0619 16:50:42.992372 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:42.993302 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "I0619 16:50:42.993758 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "I0619 16:50:42.994692 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "I0619 16:50:42.995365 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "I0619 16:50:42.997692 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "I0619 16:50:42.999348 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "I0619 16:50:43.000143 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "I0619 16:50:43.000961 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "I0619 16:50:43.001842 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "I0619 16:50:43.002725 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.003680 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.004436 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "I0619 16:50:43.005235 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "I0619 16:50:43.005872 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "I0619 16:50:43.006502 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "I0619 16:50:43.007183 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "I0619 16:50:43.007811 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "I0619 16:50:43.008275 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "I0619 16:50:43.008995 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "I0619 16:50:43.009584 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "I0619 16:50:43.010363 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "I0619 16:50:43.011132 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "I0619 16:50:43.012068 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "I0619 16:50:43.012894 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "I0619 16:50:43.014240 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "I0619 16:50:43.015014 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.015713 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.016596 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "I0619 16:50:43.017236 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "I0619 16:50:43.017905 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n",
      "I0619 16:50:43.018486 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "I0619 16:50:43.019109 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "I0619 16:50:43.019596 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "I0619 16:50:43.020613 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "I0619 16:50:43.021846 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "I0619 16:50:43.022842 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "I0619 16:50:43.023357 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "I0619 16:50:43.024064 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "I0619 16:50:43.024568 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "I0619 16:50:43.025218 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "I0619 16:50:43.025660 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "I0619 16:50:43.027220 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.031018 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.032359 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0619 16:50:43.033428 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "I0619 16:50:43.033912 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:43.034733 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0619 16:50:43.035421 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0619 16:50:43.036382 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0619 16:50:43.037103 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0619 16:50:43.037917 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0619 16:50:43.038502 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0619 16:50:43.039163 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0619 16:50:43.039700 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0619 16:50:43.040460 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0619 16:50:43.040995 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0619 16:50:43.041615 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0619 16:50:43.042154 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.042814 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.043404 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "I0619 16:50:43.044092 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "I0619 16:50:43.044728 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "I0619 16:50:43.045445 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "I0619 16:50:43.046181 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "I0619 16:50:43.047425 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "I0619 16:50:43.048293 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "I0619 16:50:43.048882 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "I0619 16:50:43.049576 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "I0619 16:50:43.050105 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "I0619 16:50:43.050732 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "I0619 16:50:43.051430 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "I0619 16:50:43.052340 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "I0619 16:50:43.053038 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "I0619 16:50:43.053973 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.054615 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.055434 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n",
      "I0619 16:50:43.056031 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "I0619 16:50:43.056777 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "I0619 16:50:43.058245 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "I0619 16:50:43.059127 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "I0619 16:50:43.060220 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "I0619 16:50:43.061105 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "I0619 16:50:43.062182 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "I0619 16:50:43.063102 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "I0619 16:50:43.064446 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "I0619 16:50:43.065341 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "I0619 16:50:43.066122 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "I0619 16:50:43.066992 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "I0619 16:50:43.068250 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "I0619 16:50:43.069060 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.069807 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.070631 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:43.071428 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "I0619 16:50:43.072304 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "I0619 16:50:43.072808 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "I0619 16:50:43.073441 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "I0619 16:50:43.073941 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "I0619 16:50:43.074614 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "I0619 16:50:43.075361 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "I0619 16:50:43.076047 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "I0619 16:50:43.076712 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "I0619 16:50:43.077447 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "I0619 16:50:43.078135 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "I0619 16:50:43.079055 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "I0619 16:50:43.079925 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "I0619 16:50:43.081321 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.081943 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.082479 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "I0619 16:50:43.083171 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "I0619 16:50:43.083738 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "I0619 16:50:43.084424 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "I0619 16:50:43.085068 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "I0619 16:50:43.085713 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "I0619 16:50:43.086437 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "I0619 16:50:43.087383 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "I0619 16:50:43.088379 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "I0619 16:50:43.089253 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "I0619 16:50:43.089847 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "I0619 16:50:43.090528 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "I0619 16:50:43.091003 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "I0619 16:50:43.091871 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "I0619 16:50:43.092462 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.093214 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.093764 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0619 16:50:43.094433 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0619 16:50:43.094954 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0619 16:50:43.095689 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0619 16:50:43.096373 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0619 16:50:43.097381 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0619 16:50:43.098044 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0619 16:50:43.098779 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0619 16:50:43.099694 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0619 16:50:43.100769 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0619 16:50:43.101809 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0619 16:50:43.102575 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0619 16:50:43.103366 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0619 16:50:43.104432 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "I0619 16:50:43.104954 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:43.105597 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.106265 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0619 16:50:43.107831 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0619 16:50:43.109333 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0619 16:50:43.110267 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "I0619 16:50:43.110969 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0619 16:50:43.112152 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "I0619 16:50:43.113391 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0619 16:50:43.114403 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0619 16:50:43.115312 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0619 16:50:43.116043 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0619 16:50:43.116592 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0619 16:50:43.117307 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0619 16:50:43.117842 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0619 16:50:43.118484 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0619 16:50:43.118953 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.119610 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.120146 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0619 16:50:43.121050 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0619 16:50:43.121612 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0619 16:50:43.122463 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0619 16:50:43.123011 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0619 16:50:43.123604 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0619 16:50:43.124063 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0619 16:50:43.124567 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0619 16:50:43.125071 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0619 16:50:43.125519 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0619 16:50:43.126307 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0619 16:50:43.126920 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0619 16:50:43.127699 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0619 16:50:43.128358 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "I0619 16:50:43.129238 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.130620 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.133777 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0619 16:50:43.134574 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0619 16:50:43.135435 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0619 16:50:43.136343 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0619 16:50:43.137067 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0619 16:50:43.137997 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0619 16:50:43.138757 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0619 16:50:43.139588 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0619 16:50:43.140316 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0619 16:50:43.141113 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0619 16:50:43.141627 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0619 16:50:43.142376 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "I0619 16:50:43.143105 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0619 16:50:43.143764 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:43.144307 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.145159 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.145815 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0619 16:50:43.147336 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0619 16:50:43.148683 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0619 16:50:43.149698 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0619 16:50:43.150197 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0619 16:50:43.151048 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "I0619 16:50:43.151684 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0619 16:50:43.152776 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "I0619 16:50:43.153621 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0619 16:50:43.154371 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0619 16:50:43.155272 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0619 16:50:43.155873 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0619 16:50:43.156759 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0619 16:50:43.157464 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0619 16:50:43.158153 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.158648 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.159554 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0619 16:50:43.160196 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0619 16:50:43.161102 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0619 16:50:43.162431 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0619 16:50:43.165145 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0619 16:50:43.166117 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0619 16:50:43.166834 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0619 16:50:43.167878 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0619 16:50:43.168603 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0619 16:50:43.169111 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0619 16:50:43.169725 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0619 16:50:43.170537 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "I0619 16:50:43.171267 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0619 16:50:43.171988 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0619 16:50:43.172782 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0619 16:50:43.173274 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0619 16:50:43.173864 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0619 16:50:43.174439 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0619 16:50:43.175112 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0619 16:50:43.175611 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0619 16:50:43.176353 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0619 16:50:43.176817 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0619 16:50:43.177599 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0619 16:50:43.178244 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0619 16:50:43.179029 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0619 16:50:43.179813 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "I0619 16:50:43.181053 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0619 16:50:43.181826 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0619 16:50:43.182615 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0619 16:50:43.183284 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0619 16:50:43.183937 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "I0619 16:50:43.184830 4686319040 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "I0619 16:50:43.200576 4686319040 embedding.py:252] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0619 16:50:43.201509 4686319040 embedding.py:252] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0619 16:50:45.657497 4686319040 params.py:247] validation_dataset_reader.type = coref\n",
      "I0619 16:50:45.658672 4686319040 params.py:247] validation_dataset_reader.lazy = False\n",
      "I0619 16:50:45.659420 4686319040 params.py:247] validation_dataset_reader.cache_directory = None\n",
      "I0619 16:50:45.659990 4686319040 params.py:247] validation_dataset_reader.max_instances = None\n",
      "I0619 16:50:45.660969 4686319040 params.py:247] validation_dataset_reader.max_span_width = 30\n",
      "I0619 16:50:45.662326 4686319040 params.py:247] validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0619 16:50:45.663541 4686319040 params.py:247] validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0619 16:50:45.664454 4686319040 params.py:247] validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0619 16:50:45.665552 4686319040 params.py:247] validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0619 16:50:45.666639 4686319040 params.py:247] validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0619 16:50:46.756920 4686319040 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0619 16:50:46.757754 4686319040 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0619 16:50:46.758261 4686319040 tokenization_utils.py:420] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0619 16:50:51.575155 4686319040 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0619 16:50:51.575913 4686319040 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0619 16:50:51.576354 4686319040 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0619 16:50:51.576911 4686319040 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0619 16:50:52.099416 4686319040 params.py:247] validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0619 16:50:52.100346 4686319040 params.py:247] validation_dataset_reader.max_sentences = None\n",
      "I0619 16:50:52.100994 4686319040 params.py:247] validation_dataset_reader.remove_singleton_clusters = False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'constructor',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselors',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'sewer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked the', 'disclosed to the', 'told the'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified the', 'spoke with the'] ##  pronoun2 = ['his', 'her', 'their']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    if (verb in verb_list_p1):\n",
    "        return ['he', 'she', 'they']\n",
    "    if (verb in verb_list_p2):\n",
    "        return ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"teacher\", \"librarian\", \"nurse\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"if \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "action1 = [\"can eat meat\", \"can play a ukelele\", \"can do 100 push ups\", \n",
    "           \"can generate the report\",\n",
    "           \"can bake cookies\", \"can bake bread\"]\n",
    "\n",
    "action2 = [\"could eat a sandwich\", \"could pay with cash\", \"could return the paperwork\", \n",
    "           \"could help with tax preparation\", \"could estimate how long the wait would be\",\n",
    "           \"could help with the delivery\", \"could bake a cake\", \"could sign the document\",\n",
    "           \"could bake cookies\", \"could bake bread\"]\n",
    "verb_action[verb_list_p1[0]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action[verb_list_p1[1]] = [action1, action2]\n",
    "verb_action[verb_list_p1[2]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "action3 = ['work was unsatisfactory', 'performance was not up to the mark',\n",
    "           'application was approved', 'application was not approved',\n",
    "           'application was under review', 'work was appreciated', \n",
    "           'behaviour was unacceptable', 'project scope had been finalised']\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "action4 = ['professional goals', 'personal goals',\n",
    "           'struggles with addiction', \n",
    "           'dislike for Korean cinema', 'woodworking hobby',\n",
    "           'desire to study', 'love for coffee',\n",
    "           'frustrations with work']\n",
    "verb_action[verb_list_p2[1]] = [action4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    output = predictor.predict(document = sentence)\n",
    "    return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "occupation_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "occupation_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 43\n",
      "Unique inputs: 119\n",
      "------------------------------\n",
      "[[[0, 1], [6, 6]]] [[[3, 4], [6, 6]]] [[[3, 4], [6, 6]]]\n",
      "The lawyer told the paralegal that he can generate the report\n",
      "The lawyer told the paralegal that she can generate the report\n",
      "The lawyer told the paralegal that they can generate the report\n",
      "[[[0, 1], [6, 6]]] [[[3, 4], [6, 6]]] [[[3, 4], [6, 6]]]\n",
      "The mover told the teacher that he can bake cookies\n",
      "The mover told the teacher that she can bake cookies\n",
      "The mover told the teacher that they can bake cookies\n",
      "[[[3, 4], [6, 6]]] [[[0, 1], [6, 6]]] [[[0, 1], [6, 6]]]\n",
      "The manager asked the auditor if he could sign the document\n",
      "The manager asked the auditor if she could sign the document\n",
      "The manager asked the auditor if they could sign the document\n",
      "[[[0, 1], [6, 6]]] [[[3, 4], [6, 6]]] [[[3, 4], [6, 6]]]\n",
      "The accountant told the paralegal that he can do 100 push ups\n",
      "The accountant told the paralegal that she can do 100 push ups\n",
      "The accountant told the paralegal that they can do 100 push ups\n",
      "4\n",
      "0.13333333333333333\n",
      "Final Unique errors: 47\n",
      "Final Unique inputs: 149\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    oc1 = random.choice(occupations_1)\n",
    "    oc2 = random.choice(occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    input1 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "    input2 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action) \n",
    "    \n",
    "    input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "    pred1, _ = predict_clusters(input1)\n",
    "    pred2, _ = predict_clusters(input2)\n",
    "    pred3, _ = predict_clusters(input2)\n",
    "    \n",
    "    \n",
    "    if(i % 30 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add(input1)\n",
    "    \n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "\n",
    "    \n",
    "\n",
    "    if not (pred1 == pred2 and pred2 == pred3):\n",
    "        if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "            if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0]) ):\n",
    "#         if(True):\n",
    "                err_count += 1\n",
    "                \n",
    "                unique_input1_error_set.add(input1)\n",
    "                \n",
    "                print(pred1, pred2, pred3)\n",
    "                print(input1)\n",
    "                print(input2)\n",
    "                print(input3)\n",
    "                \n",
    "                update_dict(occupation_pair_error, (oc1, oc2))\n",
    "                update_dict(occupation1_error, oc1)\n",
    "                update_dict(occupation2_error, oc2)\n",
    "                update_dict(verb_error, verb)\n",
    "                update_dict(action_error, action)\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('mover', 'librarian'): 15, ('manager', 'attendant'): 4, ('accountant', 'attendant'): 6, ('accountant', 'accountant'): 11, ('physician', 'accountant'): 5, ('physician', 'librarian'): 14, ('mechanic', 'baker'): 5, ('mechanic', 'housekeeper'): 2, ('technician', 'nurse'): 7, ('lawyer', 'cleaner'): 8, ('supervisor', 'nurse'): 9, ('carpenter', 'baker'): 3, ('cook', 'housekeeper'): 9, ('physician', 'secretary'): 7, ('janitor', 'accountant'): 10, ('engineer', 'counselors'): 5, ('supervisor', 'assistant'): 7, ('constructor', 'sewer'): 5, ('physician', 'assistant'): 8, ('mechanic', 'librarian'): 15, ('technician', 'assistant'): 12, ('janitor', 'paralegal'): 6, ('mover', 'housekeeper'): 9, ('CEO', 'librarian'): 16, ('driver', 'auditor'): 10, ('engineer', 'hairdresser'): 9, ('chief', 'cleaner'): 9, ('physician', 'clerk'): 6, ('carpenter', 'accountant'): 7, ('mechanic', 'sewer'): 6, ('laborer', 'designer'): 6, ('sheriff', 'cleaner'): 7, ('analyst', 'hairdresser'): 7, ('guard', 'cleaner'): 5, ('analyst', 'cashier'): 3, ('driver', 'librarian'): 11, ('cook', 'sewer'): 5, ('CEO', 'accountant'): 6, ('guard', 'receptionist'): 8, ('physician', 'teacher'): 14, ('accountant', 'editor'): 9, ('driver', 'nurse'): 7, ('carpenter', 'hairdresser'): 4, ('farmer', 'sewer'): 6, ('carpenter', 'secretary'): 9, ('cook', 'designer'): 5, ('analyst', 'nurse'): 14, ('technician', 'editor'): 7, ('lawyer', 'sewer'): 6, ('analyst', 'editor'): 3, ('farmer', 'housekeeper'): 5, ('driver', 'teacher'): 11, ('mechanic', 'nurse'): 11, ('engineer', 'sewer'): 9, ('cook', 'assistant'): 6, ('technician', 'librarian'): 14, ('chief', 'clerk'): 9, ('farmer', 'nurse'): 8, ('CEO', 'teacher'): 15, ('engineer', 'editor'): 8, ('laborer', 'teacher'): 8, ('carpenter', 'designer'): 8, ('laborer', 'editor'): 6, ('farmer', 'librarian'): 11, ('janitor', 'attendant'): 6, ('CEO', 'clerk'): 4, ('farmer', 'hairdresser'): 7, ('CEO', 'sewer'): 4, ('manager', 'nurse'): 11, ('cook', 'writer'): 13, ('carpenter', 'librarian'): 13, ('CEO', 'paralegal'): 3, ('sheriff', 'hairdresser'): 7, ('laborer', 'paralegal'): 6, ('analyst', 'accountant'): 8, ('supervisor', 'cleaner'): 3, ('lawyer', 'attendant'): 5, ('supervisor', 'auditor'): 5, ('CEO', 'baker'): 7, ('laborer', 'nurse'): 15, ('analyst', 'housekeeper'): 10, ('laborer', 'cleaner'): 5, ('farmer', 'cleaner'): 13, ('guard', 'counselors'): 9, ('cook', 'clerk'): 2, ('mover', 'cashier'): 6, ('supervisor', 'clerk'): 4, ('mechanic', 'accountant'): 9, ('supervisor', 'writer'): 7, ('supervisor', 'accountant'): 3, ('CEO', 'cashier'): 2, ('supervisor', 'receptionist'): 2, ('sheriff', 'attendant'): 5, ('technician', 'auditor'): 8, ('salesperson', 'attendant'): 8, ('laborer', 'baker'): 7, ('manager', 'housekeeper'): 8, ('developer', 'teacher'): 15, ('mechanic', 'paralegal'): 9, ('physician', 'hairdresser'): 7, ('driver', 'baker'): 6, ('lawyer', 'secretary'): 7, ('carpenter', 'writer'): 5, ('driver', 'editor'): 8, ('technician', 'teacher'): 10, ('CEO', 'hairdresser'): 3, ('engineer', 'writer'): 8, ('physician', 'auditor'): 6, ('manager', 'teacher'): 6, ('sheriff', 'teacher'): 9, ('janitor', 'teacher'): 15, ('supervisor', 'teacher'): 10, ('physician', 'cashier'): 5, ('janitor', 'receptionist'): 8, ('developer', 'editor'): 9, ('carpenter', 'assistant'): 8, ('carpenter', 'sewer'): 6, ('salesperson', 'auditor'): 5, ('salesperson', 'counselors'): 4, ('engineer', 'secretary'): 7, ('chief', 'hairdresser'): 4, ('manager', 'writer'): 3, ('guard', 'assistant'): 4, ('janitor', 'writer'): 6, ('engineer', 'nurse'): 7, ('CEO', 'housekeeper'): 7, ('janitor', 'housekeeper'): 7, ('engineer', 'clerk'): 9, ('janitor', 'editor'): 6, ('farmer', 'secretary'): 10, ('engineer', 'designer'): 7, ('chief', 'secretary'): 4, ('manager', 'secretary'): 3, ('CEO', 'cleaner'): 3, ('carpenter', 'attendant'): 6, ('engineer', 'cashier'): 8, ('developer', 'cleaner'): 8, ('developer', 'nurse'): 8, ('janitor', 'librarian'): 13, ('cook', 'attendant'): 5, ('guard', 'accountant'): 4, ('physician', 'receptionist'): 7, ('chief', 'writer'): 5, ('mechanic', 'editor'): 8, ('mover', 'receptionist'): 4, ('accountant', 'librarian'): 8, ('chief', 'nurse'): 9, ('mover', 'cleaner'): 4, ('supervisor', 'sewer'): 6, ('physician', 'nurse'): 9, ('sheriff', 'counselors'): 5, ('developer', 'counselors'): 4, ('mover', 'nurse'): 10, ('developer', 'hairdresser'): 6, ('laborer', 'assistant'): 3, ('chief', 'attendant'): 9, ('guard', 'teacher'): 14, ('chief', 'cashier'): 6, ('developer', 'auditor'): 3, ('developer', 'clerk'): 7, ('guard', 'secretary'): 8, ('physician', 'housekeeper'): 5, ('accountant', 'secretary'): 3, ('cook', 'librarian'): 11, ('farmer', 'cashier'): 4, ('constructor', 'attendant'): 6, ('supervisor', 'editor'): 7, ('sheriff', 'auditor'): 3, ('carpenter', 'cleaner'): 6, ('accountant', 'designer'): 8, ('salesperson', 'clerk'): 6, ('guard', 'writer'): 8, ('analyst', 'secretary'): 6, ('mechanic', 'assistant'): 8, ('chief', 'librarian'): 13, ('supervisor', 'hairdresser'): 2, ('supervisor', 'baker'): 7, ('cook', 'secretary'): 5, ('guard', 'hairdresser'): 6, ('salesperson', 'hairdresser'): 5, ('farmer', 'accountant'): 8, ('farmer', 'receptionist'): 9, ('laborer', 'sewer'): 8, ('mover', 'hairdresser'): 5, ('accountant', 'nurse'): 13, ('CEO', 'assistant'): 6, ('laborer', 'housekeeper'): 4, ('mover', 'teacher'): 16, ('carpenter', 'receptionist'): 9, ('manager', 'cleaner'): 5, ('lawyer', 'librarian'): 7, ('laborer', 'librarian'): 15, ('manager', 'baker'): 10, ('analyst', 'designer'): 10, ('technician', 'paralegal'): 7, ('carpenter', 'paralegal'): 5, ('mechanic', 'cleaner'): 3, ('cook', 'accountant'): 3, ('CEO', 'auditor'): 5, ('technician', 'clerk'): 4, ('farmer', 'assistant'): 7, ('cook', 'receptionist'): 4, ('driver', 'cleaner'): 10, ('analyst', 'receptionist'): 9, ('accountant', 'housekeeper'): 2, ('accountant', 'clerk'): 4, ('carpenter', 'teacher'): 5, ('constructor', 'teacher'): 8, ('mover', 'sewer'): 10, ('guard', 'cashier'): 6, ('driver', 'secretary'): 6, ('salesperson', 'sewer'): 5, ('chief', 'paralegal'): 5, ('technician', 'cleaner'): 14, ('lawyer', 'nurse'): 12, ('analyst', 'teacher'): 13, ('CEO', 'secretary'): 7, ('supervisor', 'designer'): 5, ('mover', 'accountant'): 11, ('salesperson', 'assistant'): 9, ('salesperson', 'teacher'): 13, ('cook', 'auditor'): 7, ('carpenter', 'housekeeper'): 13, ('carpenter', 'cashier'): 5, ('lawyer', 'paralegal'): 5, ('manager', 'accountant'): 9, ('analyst', 'counselors'): 6, ('engineer', 'housekeeper'): 7, ('accountant', 'paralegal'): 7, ('janitor', 'hairdresser'): 1, ('technician', 'counselors'): 4, ('physician', 'baker'): 6, ('sheriff', 'librarian'): 9, ('sheriff', 'paralegal'): 10, ('constructor', 'housekeeper'): 7, ('janitor', 'assistant'): 7, ('chief', 'teacher'): 13, ('mechanic', 'counselors'): 1, ('driver', 'clerk'): 8, ('physician', 'paralegal'): 6, ('janitor', 'designer'): 5, ('guard', 'designer'): 8, ('salesperson', 'housekeeper'): 3, ('manager', 'auditor'): 6, ('accountant', 'auditor'): 4, ('lawyer', 'cashier'): 5, ('engineer', 'attendant'): 3, ('farmer', 'attendant'): 6, ('laborer', 'attendant'): 1, ('lawyer', 'clerk'): 7, ('supervisor', 'paralegal'): 6, ('lawyer', 'designer'): 3, ('engineer', 'teacher'): 14, ('chief', 'housekeeper'): 7, ('salesperson', 'secretary'): 9, ('salesperson', 'librarian'): 7, ('sheriff', 'housekeeper'): 2, ('engineer', 'librarian'): 12, ('manager', 'editor'): 5, ('lawyer', 'writer'): 8, ('driver', 'writer'): 7, ('mover', 'baker'): 5, ('analyst', 'assistant'): 6, ('janitor', 'auditor'): 3, ('janitor', 'cashier'): 4, ('driver', 'hairdresser'): 9, ('CEO', 'writer'): 5, ('developer', 'assistant'): 5, ('accountant', 'sewer'): 5, ('driver', 'paralegal'): 5, ('mover', 'auditor'): 4, ('laborer', 'secretary'): 3, ('chief', 'auditor'): 9, ('mechanic', 'teacher'): 10, ('carpenter', 'nurse'): 14, ('engineer', 'auditor'): 4, ('analyst', 'librarian'): 10, ('guard', 'editor'): 5, ('laborer', 'receptionist'): 6, ('physician', 'designer'): 5, ('mechanic', 'designer'): 3, ('constructor', 'writer'): 5, ('salesperson', 'nurse'): 10, ('salesperson', 'designer'): 7, ('mechanic', 'cashier'): 6, ('laborer', 'cashier'): 9, ('cook', 'cleaner'): 6, ('analyst', 'paralegal'): 5, ('guard', 'librarian'): 13, ('mover', 'writer'): 9, ('analyst', 'cleaner'): 4, ('chief', 'sewer'): 11, ('constructor', 'cashier'): 9, ('developer', 'librarian'): 8, ('lawyer', 'baker'): 5, ('accountant', 'cashier'): 7, ('sheriff', 'clerk'): 5, ('mover', 'counselors'): 4, ('lawyer', 'assistant'): 4, ('technician', 'baker'): 5, ('chief', 'receptionist'): 3, ('salesperson', 'receptionist'): 4, ('lawyer', 'receptionist'): 9, ('carpenter', 'editor'): 5, ('CEO', 'editor'): 3, ('analyst', 'clerk'): 6, ('driver', 'assistant'): 6, ('physician', 'cleaner'): 10, ('mechanic', 'clerk'): 4, ('lawyer', 'hairdresser'): 7, ('driver', 'receptionist'): 5, ('chief', 'counselors'): 7, ('mechanic', 'receptionist'): 6, ('manager', 'paralegal'): 5, ('constructor', 'hairdresser'): 4, ('technician', 'writer'): 6, ('mover', 'editor'): 8, ('farmer', 'teacher'): 16, ('constructor', 'paralegal'): 8, ('guard', 'baker'): 4, ('engineer', 'baker'): 5, ('developer', 'baker'): 11, ('sheriff', 'assistant'): 9, ('accountant', 'baker'): 4, ('technician', 'accountant'): 3, ('developer', 'writer'): 4, ('constructor', 'nurse'): 8, ('accountant', 'assistant'): 3, ('cook', 'paralegal'): 11, ('CEO', 'nurse'): 15, ('accountant', 'writer'): 3, ('manager', 'designer'): 5, ('sheriff', 'editor'): 7, ('supervisor', 'librarian'): 16, ('lawyer', 'counselors'): 5, ('laborer', 'clerk'): 6, ('janitor', 'baker'): 3, ('sheriff', 'secretary'): 5, ('technician', 'receptionist'): 9, ('constructor', 'librarian'): 7, ('mechanic', 'auditor'): 6, ('analyst', 'writer'): 7, ('technician', 'secretary'): 2, ('analyst', 'attendant'): 8, ('manager', 'receptionist'): 4, ('engineer', 'cleaner'): 5, ('guard', 'housekeeper'): 4, ('technician', 'attendant'): 5, ('physician', 'sewer'): 4, ('farmer', 'paralegal'): 4, ('lawyer', 'editor'): 5, ('mechanic', 'attendant'): 7, ('janitor', 'secretary'): 4, ('manager', 'hairdresser'): 5, ('salesperson', 'writer'): 5, ('laborer', 'auditor'): 7, ('cook', 'nurse'): 3, ('technician', 'sewer'): 3, ('sheriff', 'cashier'): 5, ('constructor', 'editor'): 4, ('sheriff', 'accountant'): 5, ('salesperson', 'accountant'): 6, ('guard', 'attendant'): 4, ('mover', 'paralegal'): 6, ('engineer', 'receptionist'): 3, ('sheriff', 'baker'): 6, ('cook', 'editor'): 6, ('sheriff', 'nurse'): 9, ('manager', 'counselors'): 2, ('mover', 'attendant'): 8, ('supervisor', 'secretary'): 8, ('constructor', 'auditor'): 2, ('technician', 'hairdresser'): 4, ('manager', 'librarian'): 9, ('cook', 'hairdresser'): 5, ('janitor', 'sewer'): 9, ('farmer', 'writer'): 4, ('farmer', 'auditor'): 6, ('carpenter', 'counselors'): 6, ('guard', 'nurse'): 7, ('cook', 'teacher'): 6, ('cook', 'cashier'): 3, ('technician', 'housekeeper'): 7, ('accountant', 'counselors'): 9, ('driver', 'cashier'): 4, ('accountant', 'cleaner'): 5, ('salesperson', 'baker'): 3, ('analyst', 'auditor'): 6, ('sheriff', 'designer'): 5, ('constructor', 'accountant'): 5, ('constructor', 'receptionist'): 5, ('driver', 'designer'): 6, ('developer', 'accountant'): 7, ('developer', 'sewer'): 5, ('technician', 'cashier'): 8, ('constructor', 'cleaner'): 4, ('accountant', 'receptionist'): 9, ('accountant', 'teacher'): 9, ('driver', 'accountant'): 5, ('engineer', 'accountant'): 2, ('janitor', 'nurse'): 9, ('supervisor', 'cashier'): 7, ('driver', 'housekeeper'): 8, ('chief', 'editor'): 7, ('janitor', 'cleaner'): 5, ('developer', 'paralegal'): 2, ('lawyer', 'auditor'): 7, ('laborer', 'hairdresser'): 4, ('manager', 'sewer'): 3, ('engineer', 'assistant'): 4, ('constructor', 'counselors'): 3, ('CEO', 'designer'): 6, ('driver', 'sewer'): 3, ('farmer', 'clerk'): 5, ('farmer', 'counselors'): 3, ('supervisor', 'attendant'): 3, ('physician', 'writer'): 5, ('janitor', 'clerk'): 5, ('lawyer', 'teacher'): 8, ('chief', 'baker'): 5, ('manager', 'assistant'): 6, ('CEO', 'receptionist'): 4, ('physician', 'counselors'): 4, ('constructor', 'designer'): 5, ('farmer', 'designer'): 4, ('developer', 'secretary'): 5, ('physician', 'attendant'): 5, ('sheriff', 'writer'): 3, ('guard', 'clerk'): 2, ('guard', 'paralegal'): 4, ('mechanic', 'hairdresser'): 7, ('supervisor', 'housekeeper'): 3, ('mover', 'secretary'): 3, ('developer', 'attendant'): 6, ('salesperson', 'cleaner'): 4, ('mechanic', 'secretary'): 4, ('mover', 'clerk'): 4, ('salesperson', 'cashier'): 8, ('guard', 'auditor'): 2, ('manager', 'clerk'): 4, ('constructor', 'baker'): 3, ('cook', 'baker'): 6, ('engineer', 'paralegal'): 2, ('driver', 'attendant'): 3, ('mover', 'designer'): 3, ('constructor', 'secretary'): 2, ('farmer', 'baker'): 5, ('CEO', 'counselors'): 4, ('lawyer', 'housekeeper'): 4, ('developer', 'receptionist'): 2, ('laborer', 'counselors'): 2, ('constructor', 'assistant'): 4, ('sheriff', 'sewer'): 5, ('analyst', 'baker'): 2, ('janitor', 'counselors'): 3, ('CEO', 'attendant'): 3, ('farmer', 'editor'): 5, ('salesperson', 'editor'): 3, ('cook', 'counselors'): 3, ('analyst', 'sewer'): 2, ('carpenter', 'auditor'): 6, ('technician', 'designer'): 1, ('developer', 'cashier'): 1, ('driver', 'counselors'): 1, ('sheriff', 'receptionist'): 3, ('laborer', 'writer'): 7, ('mechanic', 'writer'): 3, ('salesperson', 'paralegal'): 3, ('developer', 'designer'): 7, ('supervisor', 'counselors'): 5, ('lawyer', 'accountant'): 2, ('chief', 'designer'): 1, ('guard', 'sewer'): 1, ('carpenter', 'clerk'): 2, ('physician', 'editor'): 3, ('developer', 'housekeeper'): 1, ('chief', 'accountant'): 1, ('manager', 'cashier'): 2, ('laborer', 'accountant'): 1, ('accountant', 'hairdresser'): 1, ('chief', 'assistant'): 2}\n",
      "{'mover': 144, 'manager': 115, 'accountant': 130, 'physician': 141, 'mechanic': 133, 'technician': 140, 'lawyer': 129, 'supervisor': 125, 'carpenter': 145, 'cook': 124, 'janitor': 135, 'engineer': 138, 'constructor': 104, 'CEO': 128, 'driver': 139, 'chief': 139, 'laborer': 129, 'sheriff': 124, 'analyst': 145, 'guard': 126, 'farmer': 146, 'salesperson': 127, 'developer': 124}\n",
      "{'librarian': 267, 'attendant': 122, 'accountant': 131, 'baker': 123, 'housekeeper': 134, 'nurse': 225, 'cleaner': 146, 'secretary': 127, 'counselors': 99, 'assistant': 134, 'sewer': 127, 'paralegal': 130, 'auditor': 124, 'hairdresser': 119, 'clerk': 113, 'designer': 123, 'cashier': 123, 'receptionist': 132, 'teacher': 258, 'editor': 137, 'writer': 136}\n",
      "{'notified the': 616, 'spoke with the': 634, 'disclosed to the': 587, 'told the': 609, 'asked the': 584}\n",
      "{'work was appreciated': 84, 'professional goals': 99, 'could estimate how long the wait would be': 102, 'can eat meat': 217, 'dislike for Korean cinema': 87, 'woodworking hobby': 86, 'can play a ukelele': 200, 'struggles with addiction': 96, 'can do 100 push ups': 222, 'application was approved': 88, 'could bake a cake': 108, 'desire to study': 104, 'could pay with cash': 124, 'project scope had been finalised': 83, 'could help with tax preparation': 111, 'application was under review': 73, 'personal goals': 78, 'could eat a sandwich': 95, 'could return the paperwork': 121, 'love for coffee': 84, 'work was unsatisfactory': 75, 'could help with the delivery': 99, 'performance was not up to the mark': 61, 'could sign the document': 117, 'can generate the report': 264, 'application was not approved': 70, 'behaviour was unacceptable': 82}\n"
     ]
    }
   ],
   "source": [
    "print(occupation_pair_count)\n",
    "print(occupation1_count)\n",
    "print(occupation2_count)\n",
    "print(verb_count)\n",
    "print(action_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('saved_pickles/unique_input1_set.pickle', 'wb') as handle:\n",
    "#     pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "# with open('saved_pickles/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "#     pickle.dump(unique_input1_error_set, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('saved_pickles/occupation_pair_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/occupation1_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/occupation2_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "# with open('saved_pickles/verb_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_count, handle)\n",
    "\n",
    "# with open('saved_pickles/action_count.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_count, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('saved_pickles/occupation_pair_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation_pair_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/occupation1_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/occupation2_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "# with open('saved_pickles/verb_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(verb_error, handle)\n",
    "\n",
    "# with open('saved_pickles/action_error.pickle', 'wb') as handle:\n",
    "#     pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 if((oc1, oc2) in occupation_pair_error.keys()):\n",
    "#                     occupation_pair_error[(oc1, oc2)] += 1\n",
    "#                 else:\n",
    "#                     occupation_pair_error[(oc1, oc2)] = 1\n",
    "                                          \n",
    "#                 if(oc1 in occupation1_error.keys()):\n",
    "#                     occupation1_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation1_error[oc1] = 1\n",
    "                \n",
    "#                 if(oc2 in occupation2_error.keys()):\n",
    "#                     occupation2_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation2_error[oc1] = 1\n",
    "                                          \n",
    "#                 if(verb in verb_error.keys()):\n",
    "#                     verb_error[verb] += 1\n",
    "#                 else:\n",
    "#                     verb_error[verb] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_clusters(sentence):\n",
    "#     output = predictor.predict(document = sentence)\n",
    "#     return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[[0, 1], [7, 7]]],\n",
       " {'top_spans': [[0, 1], [4, 5], [7, 7], [7, 10]],\n",
       "  'antecedent_indices': [[0, 1, 2, 3],\n",
       "   [0, 1, 2, 3],\n",
       "   [0, 1, 2, 3],\n",
       "   [0, 1, 2, 3]],\n",
       "  'predicted_antecedents': [-1, -1, 0, -1],\n",
       "  'document': ['The',\n",
       "   'guard',\n",
       "   'spoke',\n",
       "   'with',\n",
       "   'the',\n",
       "   'librarian',\n",
       "   'about',\n",
       "   'his',\n",
       "   'struggles',\n",
       "   'with',\n",
       "   'addiction'],\n",
       "  'clusters': [[[0, 1], [7, 7]]]})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_clusters(\"The guard spoke with the librarian about his struggles with addiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_count = 0\n",
    "# ITERS = 20\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(ITERS):\n",
    "#     oc1 = random.choice(occupations_1)\n",
    "#     oc2 = random.choice(occupations_2)\n",
    "#     verb = random.choice(list(verb_action.keys()))\n",
    "#     action = random.choice(random.choice(verb_action[verb]))\n",
    "#     in1 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     in2 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action) \n",
    "    \n",
    "#     in3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "#     pred1, _ = predict_clusters(in1)\n",
    "#     pred2, _ = predict_clusters(in2)\n",
    "#     pred3, _ = predict_clusters(in2)\n",
    "    \n",
    "#     if not (pred1 == pred2 and pred2 == pred3):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#             err_count += 1\n",
    "\n",
    "#             print(pred1, pred2, pred3)\n",
    "#             print(in1)\n",
    "#             print(in2)\n",
    "#             print(in3)\n",
    "#             print()\n",
    "    \n",
    "\n",
    "# print(err_count)\n",
    "# print(err_count/ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
