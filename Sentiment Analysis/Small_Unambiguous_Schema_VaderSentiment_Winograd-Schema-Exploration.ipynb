{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Vader Sentiment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema (based on EEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['feels', 'feel', 'made me feel', 'was'] \n",
    "verb_list_p2 = ['saw', 'found himself/herself in a', 'told us all about the recent', 'talked to', 'goes to'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list = ['is' , 'was', 'talked to', 'goes to', 'feels', \\\n",
    "             'feel', 'made me feel', 'found himself/herself in', \\\n",
    "             'told us all about the recent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_female = ['this woman','this girl','my sister','my daughter', 'my wife',\\\n",
    "                 'my girlfriend', 'my mother', 'my aunt', 'my mom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_male = ['this man', 'this boy', 'my brother', 'my son', 'my husband', \\\n",
    "               'my boyfriend', 'my father', 'my uncle', 'my dad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_pronoun_female = [\"He\"]\n",
    "subj_pronoun_male = [\"She\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_pronoun_female = [\"her\"]\n",
    "obj_pronoun_male = [\"him\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_male_biased = ['the supervisor','the janitor','the cook','the mover','the laborer','the construction worker','the chief','the developer','the carpenter','the manager','the lawyer','the farmer','the driver','the salesperson','the physician','the guard','the analyst','the mechanic','the sheriff','the CEO','the technician','the accountant','the engineer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_female_biased = ['the cashier','the teacher','the nurse','the assistant','the secretary','the auditor','the cleaner','the receptionist','the clerk','the counselor','the designer','the hairdresser','the attendant','the writer','the housekeeper','the baker','the accountant','the editor','the librarian','the tailor','the teacher','the the librarian','the the nurse','the the paralegal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', \\\n",
    "                        'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', \\\n",
    "                       'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', \\\n",
    "                       'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', \\\n",
    "                     'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', \\\n",
    "                     'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', \\\n",
    "                     'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from EEC\n",
    "African_American_Female_Names = ['Ebony', 'Jasmine', 'Lakisha', 'Latisha', 'Latoya', 'Nichelle', 'Shaniqua', 'Shereen', 'Tanisha', 'Tia']\n",
    "African_American_Male_Names = ['Alonzo', 'Alphonse', 'Darnell', 'Jamel', 'Jerome', 'Lamar', 'Leroy', 'Malik', 'Terrence', 'Torrance']\n",
    "European_American_Female_Names = ['Amanda', 'Betsy', 'Courtney', 'Ellen', 'Heather', 'Katie', 'Kristin', 'Melanie', 'Nancy', 'Stephanie']\n",
    "European_American_Male_Names = ['Adam', 'Alan', 'Andrew', 'Frank', 'Harry', 'Jack', 'Josh', 'Justin', 'Roger', 'Ryan']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_male_names = European_American_Male_Names + African_American_Male_Names\n",
    "gen_female_names = European_American_Female_Names + African_American_Female_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_american_names = African_American_Female_Names + African_American_Male_Names\n",
    "european_american_names = European_American_Female_Names + European_American_Male_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_person_male_all = subj_pronoun_male + person_male # + occupations_male_biased\n",
    "subj_person_female_all = subj_pronoun_female + person_female # + occupations_female_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_person_male = obj_pronoun_male + person_male\n",
    "obj_person_female = obj_pronoun_female + person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_states = [\"angry\", \"anxious\", \"ecstatic\", \"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                   \"excited\", \"devastated\", \"enraged\", \"fearful\", \"glad\", \"disappointed\",\\\n",
    "                   \"furious\", \"scared\", \"happy\", \"miserable\", \"irritated\", \"terrified\",\\\n",
    "                   \"relieved\", \"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emotional_states = [\"ecstatic\", \"excited\", \"glad\", \"happy\", \"relieved\"]\n",
    "\n",
    "negative_emotional_states = [\"angry\", \"anxious\",\"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                             \"devastated\", \"enraged\", \"fearful\", \"disappointed\",\\\n",
    "                             \"furious\", \"scared\", \"miserable\", \"irritated\", \"terrified\", \"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_situations = [\"annoying\", \"dreadful\", \"amazing\", \"depressing\", \\\n",
    "                       \"displeasing\", \"horrible\", \"funny\", \"gloomy\", \\\n",
    "                       \"irritating\", \"shocking\", \"great\", \"grim\", \\\n",
    "                       \"outrageous\", \"terrifying\", \"hilarious\", \"heartbreaking\", \\\n",
    "                       \"vexing\", \"threatening\", \"wonderful\", \"serious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emotional_situations = [\"amazing\", \"funny\", \"great\", \"hilarious\",\"wonderful\"]\n",
    "\n",
    "negative_emotional_situations = [\"annoying\", \"dreadful\", \"depressing\", \"displeasing\", \"horrible\",\\\n",
    "                                \"gloomy\", \"irritating\", \"shocking\", \"grim\", \"outrageous\", \"terrifying\", \"heartbreaking\",\\\n",
    "                                \"vexing\",  \"threatening\", \"serious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_subjs = [\"I made\", \"The situation makes\", \"The conversation with\"]\n",
    "verb_feel_list = [\"feel\", \"made me feel\", \"found himself/herself in a/an\", \"told us all about the recent\", \"was\", \\\n",
    "                  \"found herself in a/an\", \"found himself in a/an\"]\n",
    "end_noun = ['situation', 'events']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Exploration Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice =  random.choice([0, 1, 2])\n",
    "\n",
    "dice = 0\n",
    "\n",
    "#Noun choice\n",
    "if dice == 0:\n",
    "#     person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "#     subj_person_male = subj_person_male_all[person_choice]\n",
    "#     subj_person_female = subj_person_female_all[person_choice]\n",
    "    person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "    subj_person_male = person_male[person_choice]\n",
    "    subj_person_female = person_female[person_choice]    \n",
    "elif dice == 1:\n",
    "    subj_person_male = random.choice(subj_person_male_all)\n",
    "    subj_person_female = random.choice(subj_person_female_all)\n",
    "else:\n",
    "    subj_person_male = random.choice(occupations_male_biased)\n",
    "    subj_person_female = random.choice(occupations_female_biased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_state = random.choice(emotional_states)\n",
    "emotional_situation = random.choice(emotional_situations)\n",
    "\n",
    "verb1 = random.choice(verb_list_p1)\n",
    "verb_feel = random.choice(verb_feel_list)\n",
    "\n",
    "neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "neutral_subj_2 = neutral_subjs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made', 'The situation makes']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_subjs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# struct_1_female = \" \".join([subj_person_female,verb1,emotional_state + \".\"])\n",
    "# struct_1_male = \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "\n",
    "# struct_2_female =  \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "# struct_2_male =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_4_female =  \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "# struct_4_male =  \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])\n",
    "\n",
    "# struct_5_female =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "# struct_5_male =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "\n",
    "# struct_6_female =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "# struct_6_male =  \" \".join([neutral_subj_2, subj_person_male, verb_feel_list[4], emotional_situation + \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "struct_8_female =  \" \".join([neutral_subj_1, obj_pronoun_female[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "struct_8_male =  \" \".join([neutral_subj_1, obj_pronoun_male[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "struct_7_female =  \" \".join([neutral_subj_2, obj_pronoun_female[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "struct_7_male =  \" \".join([neutral_subj_2, obj_pronoun_male[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "I made her feel sad.\n",
      "I made him feel sad.\n",
      "------------------------------\n",
      "The conversation with her was irritating.\n",
      "The conversation with him was irritating.\n"
     ]
    }
   ],
   "source": [
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_8_female)\n",
    "print(struct_8_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_7_female)\n",
    "print(struct_7_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(struct_1_female)\n",
    "# print(struct_1_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_2_female)\n",
    "# print(struct_2_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_3_female)\n",
    "# print(struct_3_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_5_female)\n",
    "# print(struct_5_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_6_female)\n",
    "# print(struct_6_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_schema_oracle(inp):\n",
    "    res = 0\n",
    "    token_list = inp.rstrip(\".\").split()\n",
    "    for token in token_list:\n",
    "        if (token in positive_emotional_situations) or \\\n",
    "            (token in positive_emotional_states):\n",
    "            res = 1\n",
    "            break\n",
    "        elif (token in negative_emotional_situations) or \\\n",
    "            (token in negative_emotional_states):\n",
    "            res = -1\n",
    "            break           \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fairness(input1, input2, unique_input1_error_set, err_count):\n",
    "    pred1 = analyzer.polarity_scores(input1)\n",
    "    pred2 = analyzer.polarity_scores(input2)\n",
    "#     if (len(pred1) > 0 and len(pred2) > 0):\n",
    "    \n",
    "    verdict1 = run_schema_oracle(input1)\n",
    "    verdict2 = run_schema_oracle(input2)\n",
    "    assert(verdict1 == verdict2), \"ERROR: bug in run_schema_oracle() for inputs {}, {}\".format(input1, input2)\n",
    "    \n",
    "    if not ((np.sign(pred1['compound']) == np.sign(verdict1)) == (np.sign(pred2['compound'])  == np.sign(verdict2))): \n",
    "        err_count += 1\n",
    "        unique_input1_error_set.add(input1 + \"; \" + input2)          \n",
    "#             unique_input2_error_set.add(input2)\n",
    "#         print(\"verdicts: \", verdict1, verdict2)\n",
    "#         print(\"prediction: \", pred1, pred2)\n",
    "#         print(\"inp1: \", input1)\n",
    "#         print(\"inp2: \",input2)\n",
    "    return unique_input1_error_set, err_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prediction(input1, input2, unique_input1_error_set, err_count):\n",
    "    pred1 = analyzer.polarity_scores(input1)\n",
    "    pred2 = analyzer.polarity_scores(input2)\n",
    "#     if (len(pred1) > 0 and len(pred2) > 0):\n",
    "    \n",
    "    verdict1 = run_schema_oracle(input1)\n",
    "    verdict2 = run_schema_oracle(input2)\n",
    "    assert(verdict1 == verdict2), \"ERROR: bug in run_schema_oracle() for inputs {}, {}\".format(input1, input2)\n",
    "    \n",
    "    if not ((np.sign(pred1['compound']) == np.sign(verdict1)) and (np.sign(pred2['compound'])  == np.sign(verdict2))): \n",
    "        err_count += 1\n",
    "        unique_input1_error_set.add(input1 + \"; \" + input2)          \n",
    "#             unique_input2_error_set.add(input2)\n",
    "#         print(\"verdicts: \", verdict1, verdict2)\n",
    "#         print(\"prediction: \", pred1, pred2)\n",
    "#         print(\"inp1: \", input1)\n",
    "#         print(\"inp2: \",input2)\n",
    "    return unique_input1_error_set, err_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#List Data from EEC and (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = gen_female_names + ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', 'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', 'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', 'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = gen_male_names + ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', 'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', 'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', 'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_choice(choice):\n",
    "    if choice == 0:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = subj_person_male_all[person_choice]\n",
    "        subj_person_female = subj_person_female_all[person_choice]\n",
    "    elif choice == 1:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = random.choice(subj_person_male_all)\n",
    "        subj_person_female = random.choice(subj_person_female_all)\n",
    "    elif choice == 2:\n",
    "        subj_person_male = random.choice(occupations_male_biased)\n",
    "        subj_person_female = random.choice(occupations_female_biased)\n",
    "    elif choice == 3:\n",
    "        subj_person_male = random.choice(male_biased_names)\n",
    "        subj_person_female = random.choice(female_biased_names)\n",
    "    elif choice == 4:\n",
    "        subj_person_male = random.choice(gen_male_names)\n",
    "        subj_person_female = random.choice(gen_female_names)\n",
    "    elif choice == 5:\n",
    "        subj_person_male = random.choice(european_american_names)\n",
    "        subj_person_female = random.choice(european_american_names)\n",
    "    \n",
    "    return subj_person_male, subj_person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokens(choice):\n",
    "    resList = []\n",
    "    \n",
    "    subj_person_male, subj_person_female = subj_choice(choice)\n",
    "    \n",
    "    resList.append(subj_person_male)\n",
    "    resList.append(subj_person_female)\n",
    "\n",
    "    emotional_state = random.choice(emotional_states)\n",
    "    emotional_situation = random.choice(emotional_situations)\n",
    "    \n",
    "    resList.append(emotional_state)\n",
    "    resList.append(emotional_situation)\n",
    "\n",
    "    verb1 = random.choice(verb_list_p1)\n",
    "    verb_feel = random.choice(verb_feel_list)\n",
    "    \n",
    "    resList.append(verb1)\n",
    "    resList.append(verb_feel)\n",
    "\n",
    "    neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "    neutral_subj_2 = neutral_subjs[2]\n",
    "    \n",
    "    resList.append(neutral_subj_1)\n",
    "    resList.append(neutral_subj_2)\n",
    "    \n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gender_specific_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([subj_person_female, verb1, emotional_state + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])      \n",
    "\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ]) \n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])       \n",
    "\n",
    "    elif schema_no == 3:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])   \n",
    "    \n",
    "    elif schema_no == 4:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])         \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neutral_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "#     if schema_no == 0:\n",
    "#         res_str_1 =   \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "#     elif schema_no == 1:\n",
    "#         res_str_1 =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_2,  subj_person_male, verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([neutral_subj_1, random.choice([obj_pronoun_female[0], subj_person_female]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([neutral_subj_1, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_female[0],subj_person_female]), verb_feel_list[4], emotional_situation + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_pronoun = [\"I\", \"me\"]\n",
    "neutral_sent_verb = [\"saw\", \"talked to\"]\n",
    "end_sentence = [\"in the market\", \"yesterday\", \"goes to the school in our neighborhood\", \"has two children\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the market', 'yesterday']\n",
      "['goes to the school in our neighborhood', 'has two children']\n"
     ]
    }
   ],
   "source": [
    "print(end_sentence[:2])\n",
    "print(end_sentence[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_neutral_sentences(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    neutral_verb = random.choice(neutral_sent_verb)\n",
    "    end_sentence_1 = random.choice(end_sentence[:2])\n",
    "    end_sentence_2 = random.choice(end_sentence[2:4])\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "    \n",
    "    if schema_no == 0:\n",
    "        res_str_1 = \" \".join([subj_person_female, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 1:\n",
    "        res_str_1 = \" \".join([neutral_pronoun[0], neutral_verb, subj_person_female, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_pronoun[0], neutral_verb, subj_person_male, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([ subj_person_female, end_sentence_2 + \".\"])\n",
    "        res_str_2 =  \" \".join([ subj_person_male, end_sentence_2 + \".\"])\n",
    "    \n",
    "    return res_str_1, res_str_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Direct Gender Bias - Subjective (Pro)Noun Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Direct gender noun comparisons (e.g. My boyfriend/My girlfriend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  0 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique input1s: 1679\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2726\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 1680\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2759\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 1680\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2760\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 1680\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2760\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 1680\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2760\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique input1s: 1680\n",
      "Final Unique input2s: 1680\n",
      "Final Unique input pairs: 2760\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Random gender noun comparisons (e.g. My boyfriend/My mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  1 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique input1s: 1859\n",
      "Unique input2s: 1859\n",
      "Unique input pairs: 11574\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 1860\n",
      "Unique input2s: 1860\n",
      "Unique input pairs: 14791\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 1860\n",
      "Unique input2s: 1860\n",
      "Unique input pairs: 16535\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 1860\n",
      "Unique input2s: 1860\n",
      "Unique input pairs: 17463\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 1860\n",
      "Unique input2s: 1860\n",
      "Unique input pairs: 18069\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique input1s: 1860\n",
      "Final Unique input2s: 1860\n",
      "Final Unique input pairs: 18495\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Indirect Gender Bias, i.e. Occupational Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  2 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 37\n",
      "Unique input1s: 4028\n",
      "Unique input2s: 4054\n",
      "Unique input pairs: 20044\n",
      "------------------------------\n",
      "Unique errors: 60\n",
      "Unique input1s: 4187\n",
      "Unique input2s: 4185\n",
      "Unique input pairs: 33071\n",
      "------------------------------\n",
      "Unique errors: 81\n",
      "Unique input1s: 4199\n",
      "Unique input2s: 4199\n",
      "Unique input pairs: 42453\n",
      "------------------------------\n",
      "Unique errors: 93\n",
      "Unique input1s: 4200\n",
      "Unique input2s: 4200\n",
      "Unique input pairs: 49711\n",
      "------------------------------\n",
      "Unique errors: 102\n",
      "Unique input1s: 4200\n",
      "Unique input2s: 4200\n",
      "Unique input pairs: 55414\n",
      "------------------------------\n",
      "322\n",
      "0.0015333333333333334\n",
      "Final Unique errors: 106\n",
      "Final Unique input1s: 4200\n",
      "Final Unique input2s: 4200\n",
      "Final Unique input pairs: 60144\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the cleaner told us all about the recent gloomy events.; the salesperson told us all about the recent gloomy events.', 'The conversation with the cleaner was serious.; The conversation with the laborer was serious.', 'The conversation with the cleaner was serious.; The conversation with the accountant was serious.', 'the cleaner told us all about the recent serious events.; the carpenter told us all about the recent serious events.', 'the cleaner told us all about the recent gloomy events.; the carpenter told us all about the recent gloomy events.', 'the cleaner found herself in a/an serious situation.; the cook found himself in a/an serious situation.', 'the cleaner told us all about the recent serious events.; the engineer told us all about the recent serious events.', 'the cleaner found herself in a/an gloomy situation.; the guard found himself in a/an gloomy situation.', 'the cleaner told us all about the recent gloomy events.; the mover told us all about the recent gloomy events.', 'The conversation with the cleaner was serious.; The conversation with the salesperson was serious.', 'the cleaner found herself in a/an serious situation.; the janitor found himself in a/an serious situation.', 'the cleaner told us all about the recent serious events.; the salesperson told us all about the recent serious events.', 'the cleaner found herself in a/an gloomy situation.; the CEO found himself in a/an gloomy situation.', 'The conversation with the cleaner was serious.; The conversation with the chief was serious.', 'the cleaner found herself in a/an gloomy situation.; the farmer found himself in a/an gloomy situation.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Gender Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  3 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 226\n",
      "Unique input1s: 7129\n",
      "Unique input2s: 7314\n",
      "Unique input pairs: 23648\n",
      "------------------------------\n",
      "Unique errors: 446\n",
      "Unique input1s: 8146\n",
      "Unique input2s: 8336\n",
      "Unique input pairs: 43486\n",
      "------------------------------\n",
      "Unique errors: 665\n",
      "Unique input1s: 8414\n",
      "Unique input2s: 8605\n",
      "Unique input pairs: 61084\n",
      "------------------------------\n",
      "Unique errors: 850\n",
      "Unique input1s: 8490\n",
      "Unique input2s: 8673\n",
      "Unique input pairs: 76908\n",
      "------------------------------\n",
      "Unique errors: 1014\n",
      "Unique input1s: 8513\n",
      "Unique input2s: 8692\n",
      "Unique input pairs: 91396\n",
      "------------------------------\n",
      "2048\n",
      "0.009752380952380952\n",
      "Final Unique errors: 1167\n",
      "Final Unique input1s: 8519\n",
      "Final Unique input2s: 8696\n",
      "Final Unique input pairs: 104795\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tia found herself in a/an dreadful situation.; James found himself in a/an dreadful situation.', 'Tia told us all about the recent serious events.; Alan told us all about the recent serious events.', 'Tia made me feel irritated.; Joshua made me feel irritated.', 'The conversation with Tia was shocking.; The conversation with Frank was shocking.', 'Tia made me feel discouraged.; John  made me feel discouraged.', 'Tia told us all about the recent serious events.; Joshua told us all about the recent serious events.', 'Tia feel annoyed.; Justin feel annoyed.', 'Tia made me feel anxious.; Roger made me feel anxious.', 'Tia was annoyed.; Lamar was annoyed.', 'Tia feels disappointed.; Kevin feels disappointed.', 'Tia told us all about the recent outrageous events.; Donald told us all about the recent outrageous events.', 'Tia told us all about the recent annoying events.; Michael  told us all about the recent annoying events.', 'Tia made me feel discouraged.; Mark made me feel discouraged.', 'Tia made me feel depressed.; Roger made me feel depressed.', 'Tia made me feel disappointed.; Robert  made me feel disappointed.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Racial Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique input1s: 3575\n",
      "Unique input2s: 3585\n",
      "Unique input pairs: 19056\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 3656\n",
      "Unique input2s: 3656\n",
      "Unique input pairs: 30052\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 3660\n",
      "Unique input2s: 3659\n",
      "Unique input pairs: 37743\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 3660\n",
      "Unique input2s: 3660\n",
      "Unique input pairs: 43413\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 3660\n",
      "Unique input2s: 3660\n",
      "Unique input pairs: 47632\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique input1s: 3660\n",
      "Final Unique input2s: 3660\n",
      "Final Unique input pairs: 51143\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Neutral (Sentiment) Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice = 0 #5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique input1s: 72\n",
      "Unique input2s: 73\n",
      "Unique input pairs: 81\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 87\n",
      "Unique input2s: 85\n",
      "Unique input pairs: 107\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 90\n",
      "Unique input2s: 89\n",
      "Unique input pairs: 116\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 90\n",
      "Unique input2s: 90\n",
      "Unique input pairs: 120\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 90\n",
      "Unique input2s: 90\n",
      "Unique input pairs: 123\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 98\n",
      "Unique input2s: 97\n",
      "Unique input pairs: 257\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 100\n",
      "Unique input2s: 99\n",
      "Unique input pairs: 376\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 100\n",
      "Unique input2s: 100\n",
      "Unique input pairs: 481\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 100\n",
      "Unique input2s: 100\n",
      "Unique input pairs: 562\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 100\n",
      "Unique input2s: 100\n",
      "Unique input pairs: 631\n",
      "------------------------------\n",
      "Unique errors: 9\n",
      "Unique input1s: 208\n",
      "Unique input2s: 213\n",
      "Unique input pairs: 844\n",
      "------------------------------\n",
      "Unique errors: 21\n",
      "Unique input1s: 262\n",
      "Unique input2s: 270\n",
      "Unique input pairs: 991\n",
      "------------------------------\n",
      "Unique errors: 27\n",
      "Unique input1s: 297\n",
      "Unique input2s: 308\n",
      "Unique input pairs: 1138\n",
      "------------------------------\n",
      "Unique errors: 35\n",
      "Unique input1s: 314\n",
      "Unique input2s: 316\n",
      "Unique input pairs: 1279\n",
      "------------------------------\n",
      "Unique errors: 35\n",
      "Unique input1s: 323\n",
      "Unique input2s: 324\n",
      "Unique input pairs: 1411\n",
      "------------------------------\n",
      "Unique errors: 44\n",
      "Unique input1s: 449\n",
      "Unique input2s: 451\n",
      "Unique input pairs: 1692\n",
      "------------------------------\n",
      "Unique errors: 44\n",
      "Unique input1s: 550\n",
      "Unique input2s: 547\n",
      "Unique input pairs: 1842\n",
      "------------------------------\n",
      "Unique errors: 44\n",
      "Unique input1s: 603\n",
      "Unique input2s: 606\n",
      "Unique input pairs: 1988\n",
      "------------------------------\n",
      "Unique errors: 47\n",
      "Unique input1s: 651\n",
      "Unique input2s: 658\n",
      "Unique input pairs: 2135\n",
      "------------------------------\n",
      "Unique errors: 50\n",
      "Unique input1s: 687\n",
      "Unique input2s: 690\n",
      "Unique input pairs: 2277\n",
      "------------------------------\n",
      "Unique errors: 57\n",
      "Unique input1s: 736\n",
      "Unique input2s: 726\n",
      "Unique input pairs: 2559\n",
      "------------------------------\n",
      "Unique errors: 60\n",
      "Unique input1s: 744\n",
      "Unique input2s: 732\n",
      "Unique input pairs: 2696\n",
      "------------------------------\n",
      "Unique errors: 60\n",
      "Unique input1s: 747\n",
      "Unique input2s: 737\n",
      "Unique input pairs: 2824\n",
      "------------------------------\n",
      "Unique errors: 69\n",
      "Unique input1s: 750\n",
      "Unique input2s: 740\n",
      "Unique input pairs: 2954\n",
      "------------------------------\n",
      "Unique errors: 83\n",
      "Unique input1s: 752\n",
      "Unique input2s: 742\n",
      "Unique input pairs: 3084\n",
      "------------------------------\n",
      "Unique errors: 87\n",
      "Unique input1s: 806\n",
      "Unique input2s: 798\n",
      "Unique input pairs: 3340\n",
      "------------------------------\n",
      "Unique errors: 87\n",
      "Unique input1s: 831\n",
      "Unique input2s: 819\n",
      "Unique input pairs: 3470\n",
      "------------------------------\n",
      "Unique errors: 87\n",
      "Unique input1s: 843\n",
      "Unique input2s: 826\n",
      "Unique input pairs: 3601\n",
      "------------------------------\n",
      "Unique errors: 87\n",
      "Unique input1s: 847\n",
      "Unique input2s: 830\n",
      "Unique input pairs: 3717\n",
      "------------------------------\n",
      "Unique errors: 87\n",
      "Unique input1s: 850\n",
      "Unique input2s: 835\n",
      "Unique input pairs: 3832\n",
      "------------------------------\n",
      "93\n",
      "0.017222222222222222\n",
      "Final Unique errors: 87\n",
      "Final Unique input1s: 851\n",
      "Final Unique input2s: 837\n",
      "Final Unique input pairs: 3942\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 300 #00 #00 #0\n",
    "num_iter = 50 #00 #10000 #20 #0\n",
    "\n",
    "for noun_choice in [0, 1, 2, 3, 4, 5]:\n",
    "    for i in range(ITERS):\n",
    "        tokens = select_tokens(noun_choice)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 0)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 1)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 2)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        if (i > 0) and  (i % num_iter == 0):\n",
    "            print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "            print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "            print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "            print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "            print(\"------------------------------\")\n",
    "        #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(6 * 3 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I saw the cleaner in the market.; I saw the manager in the market.', 'the cleaner goes to the school in our neighborhood.; the cook goes to the school in our neighborhood.', 'I saw Tia yesterday.; I saw Jack yesterday.', 'Tia goes to the school in our neighborhood.; Adam goes to the school in our neighborhood.', 'I saw Tia in the market.; I saw Alphonse in the market.', 'the cleaner has two children.; the CEO has two children.', 'the cleaner has two children.; the manager has two children.', 'Tia has two children.; Lamar has two children.', 'Tia has two children.; Adam has two children.', 'I talked to Tia yesterday.; I talked to Alphonse yesterday.', 'the cleaner saw me in the market.; the manager saw me in the market.', 'I saw Tia yesterday.; I saw Darnell yesterday.', 'Tia saw me yesterday.; Darnell talked to me yesterday.', 'I talked to the cleaner yesterday.; I talked to the CEO yesterday.', 'the cleaner talked to me yesterday.; the manager talked to me yesterday.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
