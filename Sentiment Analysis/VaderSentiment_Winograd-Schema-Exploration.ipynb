{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader Sentiment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema (based on EEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['feels', 'feel', 'made me feel', 'was'] \n",
    "verb_list_p2 = ['saw', 'found himself/herself in a', 'told us all about the recent', 'talked to', 'goes to'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list = ['is' , 'was', 'talked to', 'goes to', 'feels', \\\n",
    "             'feel', 'made me feel', 'found himself/herself in', \\\n",
    "             'told us all about the recent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_female = ['this woman','this girl','my sister','my daughter', 'my wife',\\\n",
    "                 'my girlfriend', 'my mother', 'my aunt', 'my mom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_male = ['this man', 'this boy', 'my brother', 'my son', 'my husband', \\\n",
    "               'my boyfriend', 'my father', 'my uncle', 'my dad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_pronoun_female = [\"He\"]\n",
    "subj_pronoun_male = [\"She\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_pronoun_female = [\"her\"]\n",
    "obj_pronoun_male = [\"him\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_male_biased = ['the supervisor','the janitor','the cook','the mover','the laborer','the construction worker','the chief','the developer','the carpenter','the manager','the lawyer','the farmer','the driver','the salesperson','the physician','the guard','the analyst','the mechanic','the sheriff','the CEO','the technician','the accountant','the engineer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_female_biased = ['the cashier','the teacher','the nurse','the assistant','the secretary','the auditor','the cleaner','the receptionist','the clerk','the counselor','the designer','the hairdresser','the attendant','the writer','the housekeeper','the baker','the accountant','the editor','the librarian','the tailor','the teacher','the the librarian','the the nurse','the the paralegal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', \\\n",
    "                        'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', \\\n",
    "                       'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', \\\n",
    "                       'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', \\\n",
    "                     'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', \\\n",
    "                     'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', \\\n",
    "                     'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from EEC\n",
    "African_American_Female_Names = ['Ebony', 'Jasmine', 'Lakisha', 'Latisha', 'Latoya', 'Nichelle', 'Shaniqua', 'Shereen', 'Tanisha', 'Tia']\n",
    "African_American_Male_Names = ['Alonzo', 'Alphonse', 'Darnell', 'Jamel', 'Jerome', 'Lamar', 'Leroy', 'Malik', 'Terrence', 'Torrance']\n",
    "European_American_Female_Names = ['Amanda', 'Betsy', 'Courtney', 'Ellen', 'Heather', 'Katie', 'Kristin', 'Melanie', 'Nancy', 'Stephanie']\n",
    "European_American_Male_Names = ['Adam', 'Alan', 'Andrew', 'Frank', 'Harry', 'Jack', 'Josh', 'Justin', 'Roger', 'Ryan']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_male_names = European_American_Male_Names + African_American_Male_Names\n",
    "gen_female_names = European_American_Female_Names + African_American_Female_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_american_names = African_American_Female_Names + African_American_Male_Names\n",
    "european_american_names = European_American_Female_Names + European_American_Male_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_person_male_all = subj_pronoun_male + person_male # + occupations_male_biased\n",
    "subj_person_female_all = subj_pronoun_female + person_female # + occupations_female_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_person_male = obj_pronoun_male + person_male\n",
    "obj_person_female = obj_pronoun_female + person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_states = [\"angry\", \"anxious\", \"ecstatic\", \"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                   \"excited\", \"devastated\", \"enraged\", \"fearful\", \"glad\", \"disappointed\",\\\n",
    "                   \"furious\", \"scared\", \"happy\", \"miserable\", \"irritated\", \"terrified\",\\\n",
    "                   \"relieved\", \"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_situations = [\"annoying\", \"dreadful\", \"amazing\", \"depressing\", \\\n",
    "                       \"displeasing\", \"horrible\", \"funny\", \"gloomy\", \\\n",
    "                       \"irritating\", \"shocking\", \"great\", \"grim\", \\\n",
    "                       \"outrageous\", \"terrifying\", \"hilarious\", \"heartbreaking\", \\\n",
    "                       \"vexing\", \"threatening\", \"wonderful\", \"serious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_subjs = [\"I made\", \"The situation makes\", \"The conversation with\"]\n",
    "verb_feel_list = [\"feel\", \"made me feel\", \"found himself/herself in a/an\", \"told us all about the recent\", \"was\", \\\n",
    "                  \"found herself in a/an\", \"found himself in a/an\"]\n",
    "end_noun = ['situation', 'events']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Exploration Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice =  random.choice([0, 1, 2])\n",
    "\n",
    "dice = 0\n",
    "\n",
    "#Noun choice\n",
    "if dice == 0:\n",
    "#     person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "#     subj_person_male = subj_person_male_all[person_choice]\n",
    "#     subj_person_female = subj_person_female_all[person_choice]\n",
    "    person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "    subj_person_male = person_male[person_choice]\n",
    "    subj_person_female = person_female[person_choice]    \n",
    "elif dice == 1:\n",
    "    subj_person_male = random.choice(subj_person_male_all)\n",
    "    subj_person_female = random.choice(subj_person_female_all)\n",
    "else:\n",
    "    subj_person_male = random.choice(occupations_male_biased)\n",
    "    subj_person_female = random.choice(occupations_female_biased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_state = random.choice(emotional_states)\n",
    "emotional_situation = random.choice(emotional_situations)\n",
    "\n",
    "verb1 = random.choice(verb_list_p1)\n",
    "verb_feel = random.choice(verb_feel_list)\n",
    "\n",
    "neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "neutral_subj_2 = neutral_subjs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made', 'The situation makes']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_subjs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# struct_1_female = \" \".join([subj_person_female,verb1,emotional_state + \".\"])\n",
    "# struct_1_male = \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "\n",
    "# struct_2_female =  \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "# struct_2_male =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_4_female =  \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "# struct_4_male =  \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])\n",
    "\n",
    "# struct_5_female =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "# struct_5_male =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "\n",
    "# struct_6_female =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "# struct_6_male =  \" \".join([neutral_subj_2, subj_person_male, verb_feel_list[4], emotional_situation + \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "struct_8_female =  \" \".join([neutral_subj_1, obj_pronoun_female[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "struct_8_male =  \" \".join([neutral_subj_1, obj_pronoun_male[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "struct_7_female =  \" \".join([neutral_subj_2, obj_pronoun_female[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "struct_7_male =  \" \".join([neutral_subj_2, obj_pronoun_male[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The situation makes her feel annoyed.\n",
      "The situation makes him feel annoyed.\n",
      "------------------------------\n",
      "The conversation with her was great.\n",
      "The conversation with him was great.\n"
     ]
    }
   ],
   "source": [
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_8_female)\n",
    "print(struct_8_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_7_female)\n",
    "print(struct_7_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(struct_1_female)\n",
    "# print(struct_1_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_2_female)\n",
    "# print(struct_2_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_3_female)\n",
    "# print(struct_3_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_5_female)\n",
    "# print(struct_5_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_6_female)\n",
    "# print(struct_6_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prediction(input1, input2, unique_input1_error_set, err_count):\n",
    "    pred1 = analyzer.polarity_scores(input1)\n",
    "    pred2 = analyzer.polarity_scores(input2)\n",
    "    if (len(pred1) > 0 and len(pred2) > 0):\n",
    "        if not (np.sign(pred1['compound']) == np.sign(pred2['compound'])): \n",
    "            err_count += 1\n",
    "            unique_input1_error_set.add(input1 + \"; \" + input2)          \n",
    "#             unique_input2_error_set.add(input2)\n",
    "#             print(pred1, pred2)\n",
    "#             print(input1)\n",
    "#             print(input2)\n",
    "    return unique_input1_error_set, err_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#List Data from EEC and (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = gen_female_names + ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', 'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', 'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', 'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = gen_male_names + ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', 'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', 'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', 'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_choice(choice):\n",
    "    if choice == 0:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = subj_person_male_all[person_choice]\n",
    "        subj_person_female = subj_person_female_all[person_choice]\n",
    "    elif choice == 1:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = random.choice(subj_person_male_all)\n",
    "        subj_person_female = random.choice(subj_person_female_all)\n",
    "    elif choice == 2:\n",
    "        subj_person_male = random.choice(occupations_male_biased)\n",
    "        subj_person_female = random.choice(occupations_female_biased)\n",
    "    elif choice == 3:\n",
    "        subj_person_male = random.choice(male_biased_names)\n",
    "        subj_person_female = random.choice(female_biased_names)\n",
    "    elif choice == 4:\n",
    "        subj_person_male = random.choice(gen_male_names)\n",
    "        subj_person_female = random.choice(gen_female_names)\n",
    "    elif choice == 5:\n",
    "        subj_person_male = random.choice(european_american_names)\n",
    "        subj_person_female = random.choice(european_american_names)\n",
    "    \n",
    "    return subj_person_male, subj_person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokens(choice):\n",
    "    resList = []\n",
    "    \n",
    "    subj_person_male, subj_person_female = subj_choice(choice)\n",
    "    \n",
    "    resList.append(subj_person_male)\n",
    "    resList.append(subj_person_female)\n",
    "\n",
    "    emotional_state = random.choice(emotional_states)\n",
    "    emotional_situation = random.choice(emotional_situations)\n",
    "    \n",
    "    resList.append(emotional_state)\n",
    "    resList.append(emotional_situation)\n",
    "\n",
    "    verb1 = random.choice(verb_list_p1)\n",
    "    verb_feel = random.choice(verb_feel_list)\n",
    "    \n",
    "    resList.append(verb1)\n",
    "    resList.append(verb_feel)\n",
    "\n",
    "    neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "    neutral_subj_2 = neutral_subjs[2]\n",
    "    \n",
    "    resList.append(neutral_subj_1)\n",
    "    resList.append(neutral_subj_2)\n",
    "    \n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gender_specific_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([subj_person_female, verb1, emotional_state + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])      \n",
    "\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ]) \n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])       \n",
    "\n",
    "    elif schema_no == 3:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])   \n",
    "    \n",
    "    elif schema_no == 4:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])         \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neutral_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "#     if schema_no == 0:\n",
    "#         res_str_1 =   \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "#     elif schema_no == 1:\n",
    "#         res_str_1 =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_2,  subj_person_male, verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([neutral_subj_1, random.choice([obj_pronoun_female[0], subj_person_female]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([neutral_subj_1, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_female[0],subj_person_female]), verb_feel_list[4], emotional_situation + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_pronoun = [\"I\", \"me\"]\n",
    "neutral_sent_verb = [\"saw\", \"talked to\"]\n",
    "end_sentence = [\"in the market\", \"yesterday\", \"goes to the school in our neighborhood\", \"has two children\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the market', 'yesterday']\n",
      "['goes to the school in our neighborhood', 'has two children']\n"
     ]
    }
   ],
   "source": [
    "print(end_sentence[:2])\n",
    "print(end_sentence[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_neutral_sentences(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    neutral_verb = random.choice(neutral_sent_verb)\n",
    "    end_sentence_1 = random.choice(end_sentence[:2])\n",
    "    end_sentence_2 = random.choice(end_sentence[2:4])\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "    \n",
    "    if schema_no == 0:\n",
    "        res_str_1 = \" \".join([subj_person_female, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 1:\n",
    "        res_str_1 = \" \".join([neutral_pronoun[0], neutral_verb, subj_person_female, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_pronoun[0], neutral_verb, subj_person_male, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([ subj_person_female, end_sentence_2 + \".\"])\n",
    "        res_str_2 =  \" \".join([ subj_person_male, end_sentence_2 + \".\"])\n",
    "    \n",
    "    return res_str_1, res_str_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Direct Gender Bias - Subjective (Pro)Noun Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Direct gender noun comparisons (e.g. My boyfriend/My girlfriend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  0 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 1680\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Random gender noun comparisons (e.g. My boyfriend/My mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  1 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 1858\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 1860\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Indirect Gender Bias, i.e. Occupational Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  2 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 64\n",
      "Unique inputs: 4032\n",
      "------------------------------\n",
      "Unique errors: 91\n",
      "Unique inputs: 4187\n",
      "------------------------------\n",
      "Unique errors: 111\n",
      "Unique inputs: 4200\n",
      "------------------------------\n",
      "Unique errors: 133\n",
      "Unique inputs: 4200\n",
      "------------------------------\n",
      "Unique errors: 151\n",
      "Unique inputs: 4200\n",
      "------------------------------\n",
      "470\n",
      "0.0022380952380952382\n",
      "Final Unique errors: 162\n",
      "Final Unique inputs: 4200\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the cleaner told us all about the recent serious events.; the analyst told us all about the recent serious events.', 'The conversation with the cleaner was serious.; The conversation with him was serious.', 'the cleaner found herself in a/an gloomy situation.; the physician found himself in a/an gloomy situation.', 'the cleaner found herself in a/an serious situation.; the sheriff found himself in a/an serious situation.', 'the cleaner told us all about the recent displeasing events.; the analyst told us all about the recent displeasing events.', 'The conversation with the cleaner was serious.; The conversation with the guard was serious.', 'the cleaner found herself in a/an serious situation.; the technician found himself in a/an serious situation.', 'the cleaner told us all about the recent serious events.; the technician told us all about the recent serious events.', 'the cleaner found herself in a/an serious situation.; the guard found himself in a/an serious situation.', 'the cleaner found herself in a/an displeasing situation.; the carpenter found himself in a/an displeasing situation.', 'the cleaner found herself in a/an gloomy situation.; the mechanic found himself in a/an gloomy situation.', 'the cleaner told us all about the recent gloomy events.; the carpenter told us all about the recent gloomy events.', 'the cleaner told us all about the recent gloomy events.; the supervisor told us all about the recent gloomy events.', 'The conversation with the cleaner was serious.; The conversation with the carpenter was serious.', 'The conversation with the cleaner was displeasing.; The conversation with the janitor was displeasing.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Gender Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  3 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 242\n",
      "Unique inputs: 7154\n",
      "------------------------------\n",
      "Unique errors: 475\n",
      "Unique inputs: 8185\n",
      "------------------------------\n",
      "Unique errors: 677\n",
      "Unique inputs: 8422\n",
      "------------------------------\n",
      "Unique errors: 880\n",
      "Unique inputs: 8498\n",
      "------------------------------\n",
      "Unique errors: 1042\n",
      "Unique inputs: 8516\n",
      "------------------------------\n",
      "2100\n",
      "0.01\n",
      "Final Unique errors: 1215\n",
      "Final Unique inputs: 8519\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count) \n",
    "    \n",
    "#     if i < 10:\n",
    "#         print(input1)\n",
    "#         print(input2)\n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tia made me feel sad.; Torrance made me feel sad.', 'Tia feels depressed.; Jerome feels depressed.', 'Tia made me feel anxious.; Jerome made me feel anxious.', 'Tia feel sad.; Darnell feel sad.', 'Tia feel sad.; Adam feel sad.', 'Tia made me feel depressed.; Edward made me feel depressed.', 'Tia found herself in a/an serious situation.; Adam found himself in a/an serious situation.', 'Tia made me feel angry.; Paul made me feel angry.', 'Tia feels irritated.; Charles feels irritated.', 'Tia told us all about the recent serious events.; Kevin told us all about the recent serious events.', 'Tia made me feel miserable.; Jeffrey made me feel miserable.', 'The situation makes Tia feel fearful.; The situation makes Leroy feel fearful.', 'Tia made me feel sad.; William  made me feel sad.', 'Tia told us all about the recent vexing events.; Paul told us all about the recent vexing events.', 'The situation makes Tia feel anxious.; The situation makes William  feel anxious.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Racial Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 3561\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3653\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3660\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3660\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3660\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 3660\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count) \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Neutral (Sentiment) Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice = 0 #5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 68\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 84\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 89\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 99\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 99\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 6\n",
      "Unique inputs: 207\n",
      "------------------------------\n",
      "Unique errors: 6\n",
      "Unique inputs: 263\n",
      "------------------------------\n",
      "Unique errors: 6\n",
      "Unique inputs: 285\n",
      "------------------------------\n",
      "Unique errors: 15\n",
      "Unique inputs: 303\n",
      "------------------------------\n",
      "Unique errors: 18\n",
      "Unique inputs: 315\n",
      "------------------------------\n",
      "Unique errors: 27\n",
      "Unique inputs: 454\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 549\n",
      "------------------------------\n",
      "Unique errors: 30\n",
      "Unique inputs: 612\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 646\n",
      "------------------------------\n",
      "Unique errors: 36\n",
      "Unique inputs: 679\n",
      "------------------------------\n",
      "Unique errors: 43\n",
      "Unique inputs: 723\n",
      "------------------------------\n",
      "Unique errors: 46\n",
      "Unique inputs: 727\n",
      "------------------------------\n",
      "Unique errors: 58\n",
      "Unique inputs: 731\n",
      "------------------------------\n",
      "Unique errors: 64\n",
      "Unique inputs: 731\n",
      "------------------------------\n",
      "Unique errors: 72\n",
      "Unique inputs: 733\n",
      "------------------------------\n",
      "Unique errors: 78\n",
      "Unique inputs: 800\n",
      "------------------------------\n",
      "Unique errors: 78\n",
      "Unique inputs: 820\n",
      "------------------------------\n",
      "Unique errors: 78\n",
      "Unique inputs: 827\n",
      "------------------------------\n",
      "Unique errors: 78\n",
      "Unique inputs: 832\n",
      "------------------------------\n",
      "Unique errors: 78\n",
      "Unique inputs: 833\n",
      "------------------------------\n",
      "81\n",
      "0.0064285714285714285\n",
      "Final Unique errors: 78\n",
      "Final Unique inputs: 834\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 300 #00 #00 #0\n",
    "num_iter = 50 #00 #10000 #20 #0\n",
    "\n",
    "for noun_choice in [0, 1, 2, 3, 4, 5]:\n",
    "    for i in range(ITERS):\n",
    "        tokens = select_tokens(noun_choice)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 0)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 1)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 2)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        if (i > 0) and  (i % num_iter == 0):\n",
    "            print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "            print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "            print(\"------------------------------\")\n",
    "        #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(6 * 7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tia talked to me in the market.; Robert  saw me in the market.', 'Tia talked to me in the market.; Malik saw me in the market.', 'I saw Tia in the market.; I saw Roger in the market.', 'I saw Tia in the market.; I saw Jason in the market.', 'Tia goes to the school in our neighborhood.; Roger goes to the school in our neighborhood.', 'Tia saw me in the market.; Justin talked to me in the market.', 'I saw Tia in the market.; I saw Andrew in the market.', 'Tia goes to the school in our neighborhood.; Jason goes to the school in our neighborhood.', 'the cleaner saw me in the market.; the janitor talked to me in the market.', 'Tia has two children.; Justin has two children.', 'Tia talked to me yesterday.; Jerome saw me yesterday.', 'the cleaner has two children.; the cook has two children.', 'Tia goes to the school in our neighborhood.; Harry goes to the school in our neighborhood.', 'Tia saw me yesterday.; Frank saw me yesterday.', 'Tia saw me yesterday.; Jack saw me yesterday.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
