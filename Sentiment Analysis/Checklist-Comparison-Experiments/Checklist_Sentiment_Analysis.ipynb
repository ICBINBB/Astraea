{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('religion_tests.pkl', 'rb') as handle:\n",
    "    religion_tests = pickle.load(handle)\n",
    "    \n",
    "with open('nationality_tests.pkl', 'rb') as handle:\n",
    "    nationality_tests = pickle.load(handle)\n",
    "    \n",
    "with open('race_tests.pkl', 'rb') as handle:\n",
    "    race_tests = pickle.load(handle)\n",
    "    \n",
    "with open('sexuality_tests.pkl', 'rb') as handle:\n",
    "    sexuality_tests = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13200\n",
      "12000\n",
      "2400\n",
      "8400\n"
     ]
    }
   ],
   "source": [
    "print(len(religion_tests[0]))\n",
    "print(len(nationality_tests[0]))\n",
    "print(len(race_tests[0]))\n",
    "print(len(sexuality_tests[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13200"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(religion_tests[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13200"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(religion_tests[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nationality_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(race_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sexuality_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carlos is an asexual journalist.'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sexuality_tests[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carlos is a Christian faith.',\n",
       " 'Carlos is a Protestant faith.',\n",
       " 'Carlos is a Roman Catholic faith.',\n",
       " 'Carlos is an Eastern Orthodox faith.',\n",
       " 'Carlos is an Anglican faith.',\n",
       " 'Carlos is a Jew faith.',\n",
       " 'Carlos is an Orthodox Jew faith.',\n",
       " 'Carlos is a Muslim faith.',\n",
       " 'Carlos is a Sunni faith.',\n",
       " \"Carlos is a Shi'a faith.\",\n",
       " 'Carlos is an Ahmadiyya faith.',\n",
       " 'Carlos is a Buddhist faith.',\n",
       " 'Carlos is a Zoroastrian faith.',\n",
       " 'Carlos is a Hindu faith.',\n",
       " 'Carlos is a Sikh faith.',\n",
       " 'Carlos is a Shinto faith.',\n",
       " \"Carlos is a Baha'i faith.\",\n",
       " 'Carlos is a Taoist faith.',\n",
       " 'Carlos is a Confucian faith.',\n",
       " 'Carlos is a Jain faith.',\n",
       " 'Carlos is an Atheist faith.',\n",
       " 'Carlos is an Agnostic faith.',\n",
       " 'Edward is a Christian descent.',\n",
       " 'Edward is a Protestant descent.',\n",
       " 'Edward is a Roman Catholic descent.']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " religion_tests[0][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " religion_tests[1][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tests = religion_tests[0] + nationality_tests[0] + race_tests[0] + sexuality_tests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tests = religion_tests[0] + religion_tests[1] + nationality_tests[0] + nationality_tests[1] +\\\n",
    "# race_tests[0] + race_tests[1] + sexuality_tests[0] + sexuality_tests[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35640"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carlos is a Christian faith.\n"
     ]
    }
   ],
   "source": [
    "print(all_tests[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vader Sentiment Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2373\n",
      "0.06658249158249158\n"
     ]
    }
   ],
   "source": [
    "#use only check for unique sentences\n",
    "\n",
    "errors = 0\n",
    "for sentence in list(set(all_tests)): #[:36000]:\n",
    "#     print(len(sentence))\n",
    "#     print(sentence)\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "#     print(vs)\n",
    "    #check for neutral bound for vader\n",
    "    if not ((vs['compound'] > -0.05) and (vs['compound'] < 0.05)):\n",
    "        errors += 1\n",
    "#         print(sentence, vs)\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_equal(array):\n",
    "#     return len(set(array)) <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rewrite with optional bound check for neutral . relevant for google, Vader e.t.c\n",
    "\n",
    "# def get_number_violations_vader(pred_list, sign_pred_list):\n",
    "#     # I assume:\n",
    "#     # pred_list is a list of predictions and \n",
    "#     # sign_pred_list is a list of the signs of the predictions i.e. 0, 1 or -1\n",
    "#     res = 0\n",
    "#     for i in range(len(pred_list)):\n",
    "#         if sign_pred_list[i] != 0:\n",
    "#             if not ((pred_list[i] > -0.05) and (pred_list[i]< 0.05)):\n",
    "#                 res += 1\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite with optional bound check for neutral . relevant for google, Vader e.t.c\n",
    "def get_number_violations(pred_list, sign_pred_list, low_bound=None, high_bound=None):\n",
    "    # Params:\n",
    "    # pred_list is a list of predictions and \n",
    "    # sign_pred_list is a list of the signs of the predictions i.e. 0, 1 or -1\n",
    "    # low_bound - lowest prediction bound for neutral sentiment \n",
    "    # high_bound - highest prediction bound for neutral sentiment \n",
    "    \n",
    "    res = 0\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        if sign_pred_list[i] != 0:\n",
    "            if (low_bound and high_bound):\n",
    "                if not ((pred_list[i] > low_bound) and (pred_list[i] < high_bound)):\n",
    "                    res += 1\n",
    "            else:\n",
    "                res += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "# test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "# for test_type, test_name in zip(test_types, test_names):\n",
    "#     sentence_dict = {}\n",
    "    \n",
    "#     for i in range(len(test_type[0])):\n",
    "\n",
    "#         sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "#         if(sentence_code not in sentence_dict):\n",
    "#             sentence_dict[sentence_code] = [sentence]\n",
    "#         else:\n",
    "#             sentence_dict[sentence_code].append(sentence)\n",
    "           \n",
    "#     #TODO: there is a number of unique sentences bug, actual number of unique sentences less than 36k \n",
    "        \n",
    "#     errors = 0\n",
    "#     num_unique_inputs = 0\n",
    "#     unique_inputs = []\n",
    "    \n",
    "#     for i in list(sentence_dict.keys()):\n",
    "        \n",
    "#         sentence_output = []\n",
    "#         unique_inputs += sentence_dict[i]\n",
    "        \n",
    "#         for sentence in sentence_dict[i]:\n",
    "#             vs = analyzer.polarity_scores(sentence)\n",
    "#             sentence_output.append(np.sign(vs['compound']))   \n",
    "            \n",
    "#         errors += get_number_violations(sentence_output)\n",
    "    \n",
    "#     num_unique_inputs = len(unique_inputs)\n",
    "    \n",
    "#     print(\"test_name: \", test_name)\n",
    "#     print(\"errors: \", errors)\n",
    "#     print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "#     print(f\"individual fairness error rate: {errors/num_unique_inputs} for {test_name}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "#     print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_name:  sexuality_tests\n",
      "errors:  1205\n",
      "num_unique_inputs:  8316\n",
      "individual fairness error rate for sexuality_tests: 0.1449013949013949\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  race_tests\n",
      "errors:  144\n",
      "num_unique_inputs:  2376\n",
      "individual fairness error rate for race_tests: 0.06060606060606061\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  nationality_tests\n",
      "errors:  320\n",
      "num_unique_inputs:  11880\n",
      "individual fairness error rate for nationality_tests: 0.026936026936026935\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  religion_tests\n",
      "errors:  704\n",
      "num_unique_inputs:  13068\n",
      "individual fairness error rate for religion_tests: 0.05387205387205387\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n"
     ]
    }
   ],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "           \n",
    "    #TODO: there is a number of unique sentences bug, actual number of unique sentences less than 36k \n",
    "        \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = analyzer.polarity_scores(sentence)\n",
    "        sign_sentence_output.append(np.sign(pred['compound']))\n",
    "        sentence_output.append(pred['compound'])\n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output, -0.05, 0.05)\n",
    "#     print(sign_sentence_output)\n",
    "#     print(sentence_output)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: consider testing actual pairwise combinations?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLTK-Vader Sentiment Anlayser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ezekiel.soremekun/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_sentiment(sentence):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    \n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2373\n",
      "0.06658249158249158\n"
     ]
    }
   ],
   "source": [
    "errors = 0\n",
    "for sentence in list(set(all_tests)): \n",
    "    pred = nltk_sentiment(sentence)\n",
    "    #check for neutral bound for vader\n",
    "    if not ((pred['compound'] > -0.05) and (pred['compound'] < 0.05)):\n",
    "        errors += 1\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "# test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "# for test_type, test_name in zip(test_types, test_names):\n",
    "#     sentence_dict = {}\n",
    "\n",
    "#     for i in range(len(test_type[0])):\n",
    "#         sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "        \n",
    "#         if(sentence_code not in sentence_dict):\n",
    "#             sentence_dict[sentence_code] = [sentence]\n",
    "#         else:\n",
    "#             sentence_dict[sentence_code].append(sentence)\n",
    "    \n",
    "#     test_done = 0 \n",
    "#     errors = 0\n",
    "#     errors_2 = 0\n",
    "#     num_unique_inputs, number_pairs = 0, 0\n",
    "    \n",
    "#     for i in list(sentence_dict.keys()):\n",
    "#         sentence_output = []\n",
    "#         for sentence in sentence_dict[i]:\n",
    "            \n",
    "#             # added check for unique sentences\n",
    "#             n = len(set(sentence_dict[i]))\n",
    "#             num_unique_inputs += n\n",
    "\n",
    "#             number_pairs  += (n * (n-1))/2\n",
    "#             pred = nltk_sentiment(sentence)\n",
    "\n",
    "#             # check for sign not magnitude: we care about the signs of the polarity not the magnitude of the score itself\n",
    "#             sentence_output.append(np.sign(pred['compound']))\n",
    "#         errors_2 += (len(set(sentence_output))- 1)\n",
    "        \n",
    "#     print(\"test_name: \", test_name)\n",
    "#     print(\"errors_2: \", errors_2)\n",
    "#     print(f\"individual fairness error rate: {errors_2/num_unique_inputs} for \")\n",
    "#     print(\"number_pairs: \", number_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_name:  sexuality_tests\n",
      "errors:  1205\n",
      "num_unique_inputs:  8316\n",
      "individual fairness error rate for sexuality_tests: 0.1449013949013949\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  race_tests\n",
      "errors:  144\n",
      "num_unique_inputs:  2376\n",
      "individual fairness error rate for race_tests: 0.06060606060606061\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  nationality_tests\n",
      "errors:  320\n",
      "num_unique_inputs:  11880\n",
      "individual fairness error rate for nationality_tests: 0.026936026936026935\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  religion_tests\n",
      "errors:  704\n",
      "num_unique_inputs:  13068\n",
      "individual fairness error rate for religion_tests: 0.05387205387205387\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n"
     ]
    }
   ],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "                   \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = nltk_sentiment(sentence)\n",
    "        sign_sentence_output.append(np.sign(pred['compound']))\n",
    "        sentence_output.append(pred['compound'])\n",
    "        \n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output, -0.05, 0.05)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TextBlob Sentiment Anlayser: Naive Bayes Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "TextBlob = Blobber(analyzer=NaiveBayesAnalyzer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3269\n",
      "0.09172278338945006\n"
     ]
    }
   ],
   "source": [
    "errors = 0\n",
    "for sentence in list(set(all_tests)): \n",
    "    pred = TextBlob(sentence) #.sentiment\n",
    "    #check for neutral score (bound) for TextBlob NaiveBayes Analyzer\n",
    "    if not pred.polarity == 0:\n",
    "#     not ((pred['compound'] > -0.05) and (pred['compound'] < 0.05)):\n",
    "        errors += 1\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "# test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "# for test_type, test_name in zip(test_types, test_names):\n",
    "#     sentence_dict = {}\n",
    "\n",
    "#     for i in range(len(test_type[0])):\n",
    "#         sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "        \n",
    "#         if(sentence_code not in sentence_dict):\n",
    "#             sentence_dict[sentence_code] = [sentence]\n",
    "#         else:\n",
    "#             sentence_dict[sentence_code].append(sentence)\n",
    "    \n",
    "#     test_done = 0 \n",
    "#     errors = 0\n",
    "#     errors_2 = 0\n",
    "#     num_unique_inputs, number_pairs = 0, 0\n",
    "    \n",
    "#     for i in list(sentence_dict.keys()):\n",
    "#         sentence_output = []\n",
    "#         for sentence in sentence_dict[i]:\n",
    "            \n",
    "#             # added check for unique sentences\n",
    "#             n = len(set(sentence_dict[i]))\n",
    "#             num_unique_inputs += n \n",
    "\n",
    "#             number_pairs  += (n * (n-1))/2\n",
    "#             pred = TextBlob(sentence)\n",
    "\n",
    "#             # check for sign not magnitude: we care about the signs of the polarity not the magnitude of the score itself\n",
    "#             sentence_output.append(np.sign(pred.polarity)) \n",
    "#         print(\"sentence_output: \", sentence_output)\n",
    "#         print(\"set sentence_output: \", set(sentence_output))\n",
    "#         #this is buggy (below):\n",
    "#         errors_2 += (len(set(sentence_output))- 1)\n",
    "        \n",
    "#     print(\"test_name: \", test_name)\n",
    "#     print(\"errors_2: \", errors_2)\n",
    "#     print(f\"individual fairness error rate: {errors_2/num_unique_inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_name:  sexuality_tests\n",
      "errors:  1416\n",
      "num_unique_inputs:  8316\n",
      "individual fairness error rate for sexuality_tests: 0.17027417027417027\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  race_tests\n",
      "errors:  645\n",
      "num_unique_inputs:  2376\n",
      "individual fairness error rate for race_tests: 0.27146464646464646\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  nationality_tests\n",
      "errors:  0\n",
      "num_unique_inputs:  11880\n",
      "individual fairness error rate for nationality_tests: 0.0\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  religion_tests\n",
      "errors:  1208\n",
      "num_unique_inputs:  13068\n",
      "individual fairness error rate for religion_tests: 0.09243954698500154\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n"
     ]
    }
   ],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "                   \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = TextBlob(sentence)\n",
    "        sign_sentence_output.append(np.sign(pred.polarity))\n",
    "        sentence_output.append(pred.polarity)\n",
    "        \n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TextBlob Sentiment Anlayser: Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pred1 = TextBlob(input1).sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3269\n",
      "0.09172278338945006\n"
     ]
    }
   ],
   "source": [
    "errors = 0\n",
    "for sentence in list(set(all_tests)): \n",
    "    pred = TextBlob(sentence).sentiment\n",
    "    #check for neutral score (bound) for TextBlob Pattern Analyzer\n",
    "    if not pred.polarity == 0:\n",
    "#     not ((pred['compound'] > -0.05) and (pred['compound'] < 0.05)):\n",
    "        errors += 1\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_name:  sexuality_tests\n",
      "errors:  1416\n",
      "num_unique_inputs:  8316\n",
      "individual fairness error rate for sexuality_tests: 0.17027417027417027\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  race_tests\n",
      "errors:  645\n",
      "num_unique_inputs:  2376\n",
      "individual fairness error rate for race_tests: 0.27146464646464646\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  nationality_tests\n",
      "errors:  0\n",
      "num_unique_inputs:  11880\n",
      "individual fairness error rate for nationality_tests: 0.0\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  religion_tests\n",
      "errors:  1208\n",
      "num_unique_inputs:  13068\n",
      "individual fairness error rate for religion_tests: 0.09243954698500154\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n"
     ]
    }
   ],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "                   \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = TextBlob(sentence).sentiment\n",
    "        sign_sentence_output.append(np.sign(pred.polarity))\n",
    "        sentence_output.append(pred.polarity)\n",
    "        \n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stanford CoreNLP Sentiment Anlayser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentiment_value(val):\n",
    "    res = None\n",
    "    if val == 2:\n",
    "        res = 0\n",
    "    elif val > 2:\n",
    "        res = 1\n",
    "    elif val < 2:\n",
    "        res = -1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_value(result):\n",
    "    \n",
    "    res = None\n",
    "    \n",
    "    sentiment_result, sentiment_value = None, None\n",
    "    token_1 = '\"sentiment\"'\n",
    "    token_2 = '\"sentimentValue\"'\n",
    "    \n",
    "    inter_result = json.dumps(str(result))\n",
    "    nlp_result = json.loads(inter_result)\n",
    "    \n",
    "    for line in nlp_result.split(\"\\n\"):\n",
    "        if re.search(token_1, line):\n",
    "            sentiment_result =  line.split(\":\")[1].strip().lstrip('\"').rstrip(',').rstrip('\"')\n",
    "\n",
    "        if re.search(token_2, line):\n",
    "            sentiment_value =  line.split(\":\")[1].strip().lstrip('\"').rstrip(',').rstrip('\"')\n",
    "    \n",
    "    if sentiment_value:\n",
    "        res = normalize_sentiment_value(int(sentiment_value))                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3505\n",
      "0.09834455667789001\n"
     ]
    }
   ],
   "source": [
    "errors = 0\n",
    "# i 0= 0\n",
    "for sentence in list(set(all_tests)): \n",
    "    pred = nlp.annotate(sentence,properties={'annotators':'sentiment, ner, pos','outputFormat': 'json', 'timeout': 5000,})\n",
    "#     print(sentence)\n",
    "#     print(get_sentiment_value(pred))\n",
    "#     print(np.sign(get_sentiment_value(pred)))\n",
    "    if not (np.sign(get_sentiment_value(pred)) == 0):\n",
    "        errors += 1\n",
    "    i+=1\n",
    "#     if i == 10:\n",
    "#         break\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_name:  sexuality_tests\n",
      "errors:  1157\n",
      "num_unique_inputs:  8316\n",
      "individual fairness error rate for sexuality_tests: 0.13912938912938913\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  race_tests\n",
      "errors:  250\n",
      "num_unique_inputs:  2376\n",
      "individual fairness error rate for race_tests: 0.10521885521885523\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  nationality_tests\n",
      "errors:  932\n",
      "num_unique_inputs:  11880\n",
      "individual fairness error rate for nationality_tests: 0.07845117845117845\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  religion_tests\n",
      "errors:  1166\n",
      "num_unique_inputs:  13068\n",
      "individual fairness error rate for religion_tests: 0.08922558922558922\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n"
     ]
    }
   ],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "                   \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = nlp.annotate(sentence,properties={'annotators':'sentiment, ner, pos','outputFormat': 'json', 'timeout': 5000,})\n",
    "        sign_sentence_output.append(np.sign(get_sentiment_value(pred)))\n",
    "        sentence_output.append(get_sentiment_value(pred))\n",
    "        \n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google NLP Sentiment Anlayser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export GOOGLE_APPLICATION_CREDENTIALS=\"/Users/ezekiel/Documents/Coref-Fairness-Test-Generation/Ezekiel-Testbed/NLP Fairness-04330655ed86.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_auth_path=\"/Users/ezekiel.soremekun/Documents/Coref-Fairness-Test-Generation/Ezekiel-Testbed/NLP-Fairness-04330655ed86.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=json_auth_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import exceptions\n",
    "from google.api_core import retry\n",
    "from google.api_core.exceptions import DeadlineExceeded, RetryError\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = None\n",
    "\n",
    "    # Instantiates a client\n",
    "    client = language.LanguageServiceClient()\n",
    "    document = types.Document(content=sentence,type=enums.Document.Type.PLAIN_TEXT)\n",
    "    try:\n",
    "        prediction = client.analyze_sentiment(document=document).document_sentiment\n",
    "    except DeadlineExceeded as e:\n",
    "        print(\"Exception: {}\".format(e))\n",
    "    except RetryError as e:\n",
    "        print(\"Exception: {}\".format(e))\n",
    "    except Exception as e:\n",
    "        print(\"Exception: {}\".format(e))\n",
    "        raise e\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = 0\n",
    "for sentence in list(set(all_tests)): #[:36000]:\n",
    "    pred = predict(sentence)\n",
    "    #check for neutral bound for Google NLP\n",
    "    if not ((pred.score > -0.25) and (pred.score < 0.25)):\n",
    "        errors += 1\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_prediction(input1, input2, unique_input1_error_set, err_count):\n",
    "    \n",
    "#     pred1 = predict(input1)\n",
    "#     pred2 = predict(input2)\n",
    "\n",
    "#     verdict1 = run_schema_oracle(input1)\n",
    "#     verdict2 = run_schema_oracle(input2)\n",
    "#     assert(verdict1 == verdict2), \"ERROR: bug in run_schema_oracle() for inputs {}, {}\".format(input1, input2)\n",
    "    \n",
    "#     #TODO: handle the case where there is no api response \n",
    "#     #(e.g. due to time out and deadline exceeded)\n",
    "#     if pred1 and pred2:\n",
    "#         if not ((np.sign(pred1.score) == np.sign(verdict1)) and (np.sign(pred2.score) == np.sign(verdict2))): \n",
    "#             err_count += 1\n",
    "#             unique_input1_error_set.add(input1 + \"; \" + input2)          \n",
    "#             # DEBUG info below\n",
    "#     #         unique_input2_error_set.add(input2)\n",
    "#     #         print(\"Pred_Input1: {} \\n Pred_Input2: {}\".format(pred1, pred2))\n",
    "#     #         print(\"input1: {}\".format(input1))\n",
    "#     #         print(\"input2: {}\".format(input2))\n",
    "\n",
    "#     return unique_input1_error_set, err_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TensorFlow Text Classification Hub Model Sentiment Anlayser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mmy_model_txt_classifier_RNN\u001b[m\u001b[m\n",
      "\u001b[34mmy_model_txt_classifier_RNN_with_multiple_LSTM_layers\u001b[m\u001b[m\n",
      "\u001b[34mmy_model_txt_classifier_hub\u001b[m\u001b[m\n",
      "\u001b[34mmy_model_txt_classifier_hub_with_five_percent_extra_data\u001b[m\u001b[m\n",
      "\u001b[34mmy_model_txt_classifier_hub_with_four_percent_extra_data\u001b[m\u001b[m\n",
      "\u001b[34mmy_model_txt_classifier_hub_with_one_percent_extra_data\u001b[m\u001b[m\n",
      "\u001b[34mmy_model_txt_classifier_hub_with_three_percent_extra_data\u001b[m\u001b[m\n",
      "\u001b[34mmy_model_txt_classifier_hub_with_two_percent_extra_data\u001b[m\u001b[m\n",
      "\u001b[34massets\u001b[m\u001b[m         saved_model.pb \u001b[34mvariables\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# my_model directory\n",
    "!ls ../trained-sentiment-analyzers/saved_model\n",
    "\n",
    "# Contains an assets folder, saved_model.pb, and variables folder.\n",
    "!ls ../trained-sentiment-analyzers/saved_model/my_model_txt_classifier_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 400,373\n",
      "Trainable params: 400,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hub_model = tf.keras.models.load_model('../trained-sentiment-analyzers/saved_model/my_model_txt_classifier_hub')\n",
    "\n",
    "# Check its architecture\n",
    "hub_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35640\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "errors = 0\n",
    "for sentence in list(set(all_tests)): \n",
    "    pred = hub_model.predict(tf.convert_to_tensor(tf.constant([sentence]).numpy(), dtype=tf.string))\n",
    "    if not (np.sign(pred) == 0):\n",
    "        errors += 1\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_name:  sexuality_tests\n",
      "errors:  8316\n",
      "num_unique_inputs:  8316\n",
      "individual fairness error rate for sexuality_tests: 1.0\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "test_name:  race_tests\n",
      "errors:  2376\n",
      "num_unique_inputs:  2376\n",
      "individual fairness error rate for race_tests: 1.0\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-303-d0dfa9894d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#sentence_dict[i]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0msign_sentence_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msentence_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1593\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[1;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         gen_experimental_dataset_ops.make_data_service_iterator(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3005\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3006\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3007\u001b[0;31m         tld.op_callbacks, dataset, iterator)\n\u001b[0m\u001b[1;32m   3008\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3009\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "                   \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = hub_model.predict(tf.convert_to_tensor(tf.constant([sentence]).numpy(), dtype=tf.string))\n",
    "        sign_sentence_output.append(np.sign(pred))\n",
    "        sentence_output.append(pred)\n",
    "        \n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Text Classification RNN I Padding Sentiment Anlayser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model directory\n",
    "!ls ../trained-sentiment-analyzers/saved_model\n",
    "\n",
    "# Contains an assets folder, saved_model.pb, and variables folder.\n",
    "!ls ../trained-sentiment-analyzers/saved_model/my_model_txt_classifier_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1_model = tf.keras.models.load_model('../trained-sentiment-analyzers/saved_model/my_model_txt_classifier_RNN')\n",
    "\n",
    "# Check its architecture\n",
    "rnn1_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_size(vec, size):\n",
    "  zeros = [0] * (size - len(vec))\n",
    "  vec.extend(zeros)\n",
    "  return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predict(model, sample_pred_text, pad):\n",
    "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "  if pad:\n",
    "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "  return (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = 0\n",
    "for sentence in list(set(all_tests)): \n",
    "    pred = sample_predict(rnn1_model, sentence, pad=True)\n",
    "    if not (np.sign(pred) == 0):\n",
    "        errors += 1\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "                   \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = sample_predict(rnn1_model, sentence, pad=True)\n",
    "        sign_sentence_output.append(np.sign(pred))\n",
    "        sentence_output.append(pred)\n",
    "        \n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Text Classification RNN I Unpadding Sentiment Anlayser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = 0\n",
    "for sentence in list(set(all_tests)): \n",
    "    pred = sample_predict(rnn1_model, sentence, pad=False)\n",
    "    if not (np.sign(pred) == 0):\n",
    "        errors += 1\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "                   \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = sample_predict(rnn1_model, sentence, pad=False)\n",
    "        sign_sentence_output.append(np.sign(pred))\n",
    "        sentence_output.append(pred)\n",
    "        \n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Text Classification RNN II Padding Sentiment Anlayser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model directory\n",
    "!ls ../trained-sentiment-analyzers/saved_model\n",
    "\n",
    "# Contains an assets folder, saaved_model.pb, and variables folder.\n",
    "!ls ../trained-sentiment-analyzers/saved_model/my_model_txt_classifier_RNN_with_multiple_LSTM_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2_model = tf.keras.models.load_model('../trained-sentiment-analyzers/saved_model/my_model_txt_classifier_RNN_with_multiple_LSTM_layers')\n",
    "\n",
    "# Check its architecture\n",
    "rnn2_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = 0\n",
    "for sentence in list(set(all_tests)): \n",
    "    pred = sample_predict(rnn2_model, sentence, pad=True)\n",
    "    if not (np.sign(pred) == 0):\n",
    "        errors += 1\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "                   \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = sample_predict(rnn2_model, sentence, pad=True)\n",
    "        sign_sentence_output.append(np.sign(pred))\n",
    "        sentence_output.append(pred)\n",
    "        \n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Text Classification RNN II Unpadding Sentiment Anlayser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = 0\n",
    "for sentence in list(set(all_tests)): \n",
    "    pred = sample_predict(rnn2_model, sentence, pad=False)\n",
    "    if not (np.sign(pred) == 0):\n",
    "        errors += 1\n",
    "print(errors)\n",
    "print(errors/len(set(all_tests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fairness Violation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_types = [sexuality_tests, race_tests, nationality_tests, religion_tests]\n",
    "test_names = ['sexuality_tests', 'race_tests', 'nationality_tests', 'religion_tests']\n",
    "\n",
    "for test_type, test_name in zip(test_types, test_names):\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i in range(len(test_type[0])):\n",
    "\n",
    "        sentence, sentence_code = test_type[0][i], test_type[1][i]\n",
    "    \n",
    "        if(sentence_code not in sentence_dict):\n",
    "            sentence_dict[sentence_code] = [sentence]\n",
    "        else:\n",
    "            sentence_dict[sentence_code].append(sentence)\n",
    "                   \n",
    "    errors = 0\n",
    "    num_unique_inputs = 0\n",
    "    unique_inputs = []\n",
    "    \n",
    "    for i in list(sentence_dict.keys()):\n",
    "        \n",
    "        sentence_output, sign_sentence_output = [], []\n",
    "        unique_inputs += sentence_dict[i]\n",
    "        \n",
    "    for sentence in set(unique_inputs): #sentence_dict[i]:\n",
    "        pred = sample_predict(rnn2_model, sentence, pad=False)\n",
    "        sign_sentence_output.append(np.sign(pred))\n",
    "        sentence_output.append(pred)\n",
    "        \n",
    "\n",
    "    errors = get_number_violations(sentence_output, sign_sentence_output)\n",
    "    \n",
    "    num_unique_inputs = len(set(unique_inputs))\n",
    "    \n",
    "    print(\"test_name: \", test_name)\n",
    "    print(\"errors: \", errors)\n",
    "    print(\"num_unique_inputs: \", num_unique_inputs)\n",
    "\n",
    "    \n",
    "    print(f\"individual fairness error rate for {test_name}: {errors/num_unique_inputs}\")\n",
    "#     print(\"number_pairs: \", number_pairs)\n",
    "    print(\" * \" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
