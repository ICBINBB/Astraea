{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob Sentiment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema (based on EEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['feels', 'feel', 'made me feel', 'was'] \n",
    "verb_list_p2 = ['saw', 'found himself/herself in a', 'told us all about the recent', 'talked to', 'goes to'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list = ['is' , 'was', 'talked to', 'goes to', 'feels', \\\n",
    "             'feel', 'made me feel', 'found himself/herself in', \\\n",
    "             'told us all about the recent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_female = ['this woman','this girl','my sister','my daughter', 'my wife',\\\n",
    "                 'my girlfriend', 'my mother', 'my aunt', 'my mom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_male = ['this man', 'this boy', 'my brother', 'my son', 'my husband', \\\n",
    "               'my boyfriend', 'my father', 'my uncle', 'my dad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_pronoun_female = [\"He\"]\n",
    "subj_pronoun_male = [\"She\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_pronoun_female = [\"her\"]\n",
    "obj_pronoun_male = [\"him\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_male_biased = ['the supervisor','the janitor','the cook','the mover','the laborer','the construction worker','the chief','the developer','the carpenter','the manager','the lawyer','the farmer','the driver','the salesperson','the physician','the guard','the analyst','the mechanic','the sheriff','the CEO','the technician','the accountant','the engineer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_female_biased = ['the cashier','the teacher','the nurse','the assistant','the secretary','the auditor','the cleaner','the receptionist','the clerk','the counselor','the designer','the hairdresser','the attendant','the writer','the housekeeper','the baker','the accountant','the editor','the librarian','the tailor','the teacher','the the librarian','the the nurse','the the paralegal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', \\\n",
    "                        'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', \\\n",
    "                       'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', \\\n",
    "                       'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', \\\n",
    "                     'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', \\\n",
    "                     'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', \\\n",
    "                     'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from EEC\n",
    "African_American_Female_Names = ['Ebony', 'Jasmine', 'Lakisha', 'Latisha', 'Latoya', 'Nichelle', 'Shaniqua', 'Shereen', 'Tanisha', 'Tia']\n",
    "African_American_Male_Names = ['Alonzo', 'Alphonse', 'Darnell', 'Jamel', 'Jerome', 'Lamar', 'Leroy', 'Malik', 'Terrence', 'Torrance']\n",
    "European_American_Female_Names = ['Amanda', 'Betsy', 'Courtney', 'Ellen', 'Heather', 'Katie', 'Kristin', 'Melanie', 'Nancy', 'Stephanie']\n",
    "European_American_Male_Names = ['Adam', 'Alan', 'Andrew', 'Frank', 'Harry', 'Jack', 'Josh', 'Justin', 'Roger', 'Ryan']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_male_names = European_American_Male_Names + African_American_Male_Names\n",
    "gen_female_names = European_American_Female_Names + African_American_Female_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_american_names = African_American_Female_Names + African_American_Male_Names\n",
    "european_american_names = European_American_Female_Names + European_American_Male_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_person_male_all = subj_pronoun_male + person_male # + occupations_male_biased\n",
    "subj_person_female_all = subj_pronoun_female + person_female # + occupations_female_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_person_male = obj_pronoun_male + person_male\n",
    "obj_person_female = obj_pronoun_female + person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_states = [\"angry\", \"anxious\", \"ecstatic\", \"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                   \"excited\", \"devastated\", \"enraged\", \"fearful\", \"glad\", \"disappointed\",\\\n",
    "                   \"furious\", \"scared\", \"happy\", \"miserable\", \"irritated\", \"terrified\",\\\n",
    "                   \"relieved\", \"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emotional_states = [\"ecstatic\", \"excited\", \"glad\", \"happy\", \"relieved\"]\n",
    "\n",
    "negative_emotional_states = [\"angry\", \"anxious\",\"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                             \"devastated\", \"enraged\", \"fearful\", \"disappointed\",\\\n",
    "                             \"furious\", \"scared\", \"miserable\", \"irritated\", \"terrified\", \"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_situations = [\"annoying\", \"dreadful\", \"amazing\", \"depressing\", \\\n",
    "                       \"displeasing\", \"horrible\", \"funny\", \"gloomy\", \\\n",
    "                       \"irritating\", \"shocking\", \"great\", \"grim\", \\\n",
    "                       \"outrageous\", \"terrifying\", \"hilarious\", \"heartbreaking\", \\\n",
    "                       \"vexing\", \"threatening\", \"wonderful\", \"serious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emotional_situations = [\"amazing\", \"funny\", \"great\", \"hilarious\",\"wonderful\"]\n",
    "\n",
    "negative_emotional_situations = [\"annoying\", \"dreadful\", \"depressing\", \"displeasing\", \"horrible\",\\\n",
    "                                \"gloomy\", \"irritating\", \"shocking\", \"grim\", \"outrageous\", \"terrifying\", \"heartbreaking\",\\\n",
    "                                \"vexing\",  \"threatening\", \"serious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_subjs = [\"I made\", \"The situation makes\", \"The conversation with\"]\n",
    "verb_feel_list = [\"feel\", \"made me feel\", \"found himself/herself in a/an\", \"told us all about the recent\", \"was\", \\\n",
    "                  \"found herself in a/an\", \"found himself in a/an\"]\n",
    "end_noun = ['situation', 'events']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Exploration Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice =  random.choice([0, 1, 2])\n",
    "\n",
    "dice = 0\n",
    "\n",
    "#Noun choice\n",
    "if dice == 0:\n",
    "#     person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "#     subj_person_male = subj_person_male_all[person_choice]\n",
    "#     subj_person_female = subj_person_female_all[person_choice]\n",
    "    person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "    subj_person_male = person_male[person_choice]\n",
    "    subj_person_female = person_female[person_choice]    \n",
    "elif dice == 1:\n",
    "    subj_person_male = random.choice(subj_person_male_all)\n",
    "    subj_person_female = random.choice(subj_person_female_all)\n",
    "else:\n",
    "    subj_person_male = random.choice(occupations_male_biased)\n",
    "    subj_person_female = random.choice(occupations_female_biased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_state = random.choice(emotional_states)\n",
    "emotional_situation = random.choice(emotional_situations)\n",
    "\n",
    "verb1 = random.choice(verb_list_p1)\n",
    "verb_feel = random.choice(verb_feel_list)\n",
    "\n",
    "neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "neutral_subj_2 = neutral_subjs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made', 'The situation makes']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_subjs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# struct_1_female = \" \".join([subj_person_female,verb1,emotional_state + \".\"])\n",
    "# struct_1_male = \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "\n",
    "# struct_2_female =  \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "# struct_2_male =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_4_female =  \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "# struct_4_male =  \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])\n",
    "\n",
    "# struct_5_female =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "# struct_5_male =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "\n",
    "# struct_6_female =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "# struct_6_male =  \" \".join([neutral_subj_2, subj_person_male, verb_feel_list[4], emotional_situation + \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "struct_8_female =  \" \".join([neutral_subj_1, obj_pronoun_female[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "struct_8_male =  \" \".join([neutral_subj_1, obj_pronoun_male[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "struct_7_female =  \" \".join([neutral_subj_2, obj_pronoun_female[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "struct_7_male =  \" \".join([neutral_subj_2, obj_pronoun_male[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The situation makes her feel annoyed.\n",
      "The situation makes him feel annoyed.\n",
      "------------------------------\n",
      "The conversation with her was outrageous.\n",
      "The conversation with him was outrageous.\n"
     ]
    }
   ],
   "source": [
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_8_female)\n",
    "print(struct_8_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_7_female)\n",
    "print(struct_7_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(struct_1_female)\n",
    "# print(struct_1_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_2_female)\n",
    "# print(struct_2_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_3_female)\n",
    "# print(struct_3_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_5_female)\n",
    "# print(struct_5_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_6_female)\n",
    "# print(struct_6_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_schema_oracle(inp):\n",
    "    res = 0\n",
    "    token_list = inp.rstrip(\".\").split()\n",
    "    for token in token_list:\n",
    "        if (token in positive_emotional_situations) or \\\n",
    "            (token in positive_emotional_states):\n",
    "            res = 1\n",
    "            break\n",
    "        elif (token in negative_emotional_situations) or \\\n",
    "            (token in negative_emotional_states):\n",
    "            res = -1\n",
    "            break           \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prediction(input1, input2, unique_input1_error_set, err_count):\n",
    "    pred1 = TextBlob(input1).sentiment \n",
    "    pred2 = TextBlob(input2).sentiment \n",
    "#     if (len(pred1) > 0 and len(pred2) > 0):\n",
    "\n",
    "    verdict1 = run_schema_oracle(input1)\n",
    "    verdict2 = run_schema_oracle(input2)\n",
    "    assert(verdict1 == verdict2), \"ERROR: bug in run_schema_oracle() for inputs {}, {}\".format(input1, input2)\n",
    "   \n",
    "    if not ((np.sign(pred1.polarity) == np.sign(verdict1)) and ( np.sign(pred2.polarity) == np.sign(verdict2))): \n",
    "        err_count += 1\n",
    "        unique_input1_error_set.add(input1 + \"; \" + input2)          \n",
    "#             unique_input2_error_set.add(input2)\n",
    "#             print(pred1, pred2)\n",
    "#             print(input1)\n",
    "#             print(input2)\n",
    "    return unique_input1_error_set, err_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = gen_female_names + ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', 'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', 'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', 'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = gen_male_names + ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', 'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', 'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', 'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_choice(choice):\n",
    "    if choice == 0:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = subj_person_male_all[person_choice]\n",
    "        subj_person_female = subj_person_female_all[person_choice]\n",
    "    elif choice == 1:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = random.choice(subj_person_male_all)\n",
    "        subj_person_female = random.choice(subj_person_female_all)\n",
    "    elif choice == 2:\n",
    "        subj_person_male = random.choice(occupations_male_biased)\n",
    "        subj_person_female = random.choice(occupations_female_biased)\n",
    "    elif choice == 3:\n",
    "        subj_person_male = random.choice(male_biased_names)\n",
    "        subj_person_female = random.choice(female_biased_names)\n",
    "    elif choice == 4:\n",
    "        subj_person_male = random.choice(gen_male_names)\n",
    "        subj_person_female = random.choice(gen_female_names)\n",
    "    elif choice == 5:\n",
    "        subj_person_male = random.choice(european_american_names)\n",
    "        subj_person_female = random.choice(european_american_names)\n",
    "        \n",
    "    return subj_person_male, subj_person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokens(choice):\n",
    "    resList = []\n",
    "    \n",
    "    subj_person_male, subj_person_female = subj_choice(choice)\n",
    "    \n",
    "    resList.append(subj_person_male)\n",
    "    resList.append(subj_person_female)\n",
    "\n",
    "    emotional_state = random.choice(emotional_states)\n",
    "    emotional_situation = random.choice(emotional_situations)\n",
    "    \n",
    "    resList.append(emotional_state)\n",
    "    resList.append(emotional_situation)\n",
    "\n",
    "    verb1 = random.choice(verb_list_p1)\n",
    "    verb_feel = random.choice(verb_feel_list)\n",
    "    \n",
    "    resList.append(verb1)\n",
    "    resList.append(verb_feel)\n",
    "\n",
    "    neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "    neutral_subj_2 = neutral_subjs[2]\n",
    "    \n",
    "    resList.append(neutral_subj_1)\n",
    "    resList.append(neutral_subj_2)\n",
    "    \n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gender_specific_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([subj_person_female, verb1, emotional_state + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])      \n",
    "\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ]) \n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])       \n",
    "\n",
    "    elif schema_no == 3:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])   \n",
    "    \n",
    "    elif schema_no == 4:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])         \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neutral_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "#     if schema_no == 0:\n",
    "#         res_str_1 =   \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "#     elif schema_no == 1:\n",
    "#         res_str_1 =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_2,  subj_person_male, verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([neutral_subj_1, random.choice([obj_pronoun_female[0], subj_person_female]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([neutral_subj_1, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_female[0],subj_person_female]), verb_feel_list[4], emotional_situation + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_pronoun = [\"I\", \"me\"]\n",
    "neutral_sent_verb = [\"saw\", \"talked to\"]\n",
    "end_sentence = [\"in the market\", \"yesterday\", \"goes to the school in our neighborhood\", \"has two children\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the market', 'yesterday']\n",
      "['goes to the school in our neighborhood', 'has two children']\n"
     ]
    }
   ],
   "source": [
    "print(end_sentence[:2])\n",
    "print(end_sentence[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_neutral_sentences(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    neutral_verb = random.choice(neutral_sent_verb)\n",
    "    end_sentence_1 = random.choice(end_sentence[:2])\n",
    "    end_sentence_2 = random.choice(end_sentence[2:4])\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "    \n",
    "    if schema_no == 0:\n",
    "        res_str_1 = \" \".join([subj_person_female, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 1:\n",
    "        res_str_1 = \" \".join([neutral_pronoun[0], neutral_verb, subj_person_female, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_pronoun[0], neutral_verb, subj_person_male, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([ subj_person_female, end_sentence_2 + \".\"])\n",
    "        res_str_2 =  \" \".join([ subj_person_male, end_sentence_2 + \".\"])\n",
    "    \n",
    "    return res_str_1, res_str_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Direct Gender Bias - Subjective (Pro)Noun Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Direct gender noun comparisons (e.g. My boyfriend/My girlfriend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  0 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 1124\n",
      "Unique input1s: 1677\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2710\n",
      "------------------------------\n",
      "Unique errors: 1149\n",
      "Unique input1s: 1680\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2758\n",
      "------------------------------\n",
      "Unique errors: 1150\n",
      "Unique input1s: 1680\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2760\n",
      "------------------------------\n",
      "Unique errors: 1150\n",
      "Unique input1s: 1680\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2760\n",
      "------------------------------\n",
      "Unique errors: 1150\n",
      "Unique input1s: 1680\n",
      "Unique input2s: 1680\n",
      "Unique input pairs: 2760\n",
      "------------------------------\n",
      "82317\n",
      "0.39198571428571427\n",
      "Final Unique errors: 1150\n",
      "Final Unique input1s: 1680\n",
      "Final Unique input2s: 1680\n",
      "Final Unique input pairs: 2760\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I made her feel discouraged.; I made my uncle feel discouraged.', 'my daughter feel relieved.; my son feel relieved.', 'my wife feels enraged.; my husband feels enraged.', 'my mother feel relieved.; my father feel relieved.', 'my aunt made me feel furious.; my uncle made me feel furious.', 'The situation makes her feel enraged.; The situation makes She feel enraged.', 'my aunt feel devastated.; my uncle feel devastated.', 'my girlfriend feel depressed.; my boyfriend feel depressed.', 'I made my aunt feel scared.; I made my uncle feel scared.', 'The situation makes my girlfriend feel relieved.; The situation makes my boyfriend feel relieved.', 'The conversation with this woman was threatening.; The conversation with him was threatening.', 'He was irritated.; She was irritated.', 'The situation makes my daughter feel enraged.; The situation makes him feel enraged.', 'I made my girlfriend feel devastated.; I made my boyfriend feel devastated.', 'my daughter found herself in a/an gloomy situation.; my son found himself in a/an gloomy situation.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Random gender noun comparisons (e.g. My boyfriend/My mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  1 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 4453\n",
      "Unique input1s: 1859\n",
      "Unique input2s: 1857\n",
      "Unique input pairs: 11409\n",
      "------------------------------\n",
      "Unique errors: 5858\n",
      "Unique input1s: 1860\n",
      "Unique input2s: 1860\n",
      "Unique input pairs: 14704\n",
      "------------------------------\n",
      "Unique errors: 6627\n",
      "Unique input1s: 1860\n",
      "Unique input2s: 1860\n",
      "Unique input pairs: 16406\n",
      "------------------------------\n",
      "Unique errors: 7116\n",
      "Unique input1s: 1860\n",
      "Unique input2s: 1860\n",
      "Unique input pairs: 17398\n",
      "------------------------------\n",
      "Unique errors: 7425\n",
      "Unique input1s: 1860\n",
      "Unique input2s: 1860\n",
      "Unique input pairs: 18039\n",
      "------------------------------\n",
      "82334\n",
      "0.3920666666666667\n",
      "Final Unique errors: 7616\n",
      "Final Unique input1s: 1860\n",
      "Final Unique input2s: 1860\n",
      "Final Unique input pairs: 18421\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The situation makes her feel enraged.; The situation makes She feel enraged.', 'my girlfriend feels relieved.; She feels relieved.', 'I made my aunt feel scared.; I made my uncle feel scared.', 'my aunt feel scared.; this man feel scared.', 'my aunt feels enraged.; this man feels enraged.', 'my mother made me feel devastated.; my husband made me feel devastated.', 'my mother was depressed.; my brother was depressed.', 'my aunt feels furious.; my husband feels furious.', 'The conversation with this woman was threatening.; The conversation with my dad was threatening.', 'my wife feel furious.; this boy feel furious.', 'my sister feels enraged.; my son feels enraged.', 'The conversation with this girl was displeasing.; The conversation with my brother was displeasing.', 'my aunt feels depressed.; my husband feels depressed.', 'my girlfriend feels enraged.; my dad feels enraged.', 'The conversation with my aunt was heartbreaking.; The conversation with my husband was heartbreaking.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Indirect Gender Bias, i.e. Occupational Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  2 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 7617\n",
      "Unique input1s: 4061\n",
      "Unique input2s: 4045\n",
      "Unique input pairs: 20145\n",
      "------------------------------\n",
      "Unique errors: 12521\n",
      "Unique input1s: 4185\n",
      "Unique input2s: 4188\n",
      "Unique input pairs: 33086\n",
      "------------------------------\n",
      "Unique errors: 16261\n",
      "Unique input1s: 4200\n",
      "Unique input2s: 4199\n",
      "Unique input pairs: 42749\n",
      "------------------------------\n",
      "Unique errors: 19151\n",
      "Unique input1s: 4200\n",
      "Unique input2s: 4200\n",
      "Unique input pairs: 49897\n",
      "------------------------------\n",
      "Unique errors: 21585\n",
      "Unique input1s: 4200\n",
      "Unique input2s: 4200\n",
      "Unique input pairs: 55623\n",
      "------------------------------\n",
      "82755\n",
      "0.3940714285714286\n",
      "Final Unique errors: 23624\n",
      "Final Unique input1s: 4200\n",
      "Final Unique input2s: 4200\n",
      "Final Unique input pairs: 60329\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the baker feels scared.; the laborer feels scared.', 'the editor feels scared.; the developer feels scared.', 'the designer feels scared.; the laborer feels scared.', 'the secretary was irritated.; the farmer was irritated.', 'the assistant found herself in a/an gloomy situation.; the CEO found himself in a/an gloomy situation.', 'the attendant told us all about the recent gloomy events.; the physician told us all about the recent gloomy events.', 'the librarian made me feel enraged.; the analyst made me feel enraged.', 'the the librarian feel terrified.; the supervisor feel terrified.', 'the the librarian made me feel devastated.; the mechanic made me feel devastated.', 'the housekeeper told us all about the recent displeasing events.; the engineer told us all about the recent displeasing events.', 'The situation makes her feel terrified.; The situation makes the construction worker feel terrified.', 'the the librarian made me feel ecstatic.; the CEO made me feel ecstatic.', 'the librarian made me feel furious.; the supervisor made me feel furious.', 'the librarian made me feel depressed.; the salesperson made me feel depressed.', 'the designer found herself in a/an threatening situation.; the technician found himself in a/an threatening situation.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Gender Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  3 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 9004\n",
      "Unique input1s: 7096\n",
      "Unique input2s: 7326\n",
      "Unique input pairs: 23613\n",
      "------------------------------\n",
      "Unique errors: 16406\n",
      "Unique input1s: 8134\n",
      "Unique input2s: 8318\n",
      "Unique input pairs: 43459\n",
      "------------------------------\n",
      "Unique errors: 23003\n",
      "Unique input1s: 8398\n",
      "Unique input2s: 8590\n",
      "Unique input pairs: 61064\n",
      "------------------------------\n",
      "Unique errors: 29014\n",
      "Unique input1s: 8482\n",
      "Unique input2s: 8666\n",
      "Unique input pairs: 77035\n",
      "------------------------------\n",
      "Unique errors: 34626\n",
      "Unique input1s: 8510\n",
      "Unique input2s: 8686\n",
      "Unique input pairs: 91468\n",
      "------------------------------\n",
      "83391\n",
      "0.3971\n",
      "Final Unique errors: 39702\n",
      "Final Unique input1s: 8518\n",
      "Final Unique input2s: 8693\n",
      "Final Unique input pairs: 104886\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The situation makes Rebecca feel devastated.; The situation makes David  feel devastated.', 'Amanda found herself in a/an vexing situation.; Jeffrey found himself in a/an vexing situation.', 'Cynthia feels devastated.; Kevin feels devastated.', 'Emily found herself in a/an threatening situation.; Matthew found himself in a/an threatening situation.', 'Linda told us all about the recent gloomy events.; James told us all about the recent gloomy events.', 'The situation makes Betty feel depressed.; The situation makes Alan feel depressed.', 'Amanda was devastated.; John  was devastated.', 'Michelle made me feel ecstatic.; Thomas made me feel ecstatic.', 'Katie told us all about the recent heartbreaking events.; Jerome told us all about the recent heartbreaking events.', 'Katie told us all about the recent vexing events.; Leroy told us all about the recent vexing events.', 'Shereen feels ecstatic.; Mark feels ecstatic.', 'Patricia was irritated.; Kenneth was irritated.', 'The conversation with Katie was threatening.; The conversation with Joshua was threatening.', 'Courtney feel ecstatic.; Steven feel ecstatic.', 'Nichelle was irritated.; Mark was irritated.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Racial Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 7179\n",
      "Unique input1s: 3569\n",
      "Unique input2s: 3560\n",
      "Unique input pairs: 121043\n",
      "------------------------------\n",
      "Unique errors: 11648\n",
      "Unique input1s: 3656\n",
      "Unique input2s: 3656\n",
      "Unique input pairs: 131085\n",
      "------------------------------\n",
      "Unique errors: 14788\n",
      "Unique input1s: 3659\n",
      "Unique input2s: 3660\n",
      "Unique input pairs: 138020\n",
      "------------------------------\n",
      "Unique errors: 17095\n",
      "Unique input1s: 3660\n",
      "Unique input2s: 3660\n",
      "Unique input pairs: 143234\n",
      "------------------------------\n",
      "Unique errors: 18941\n",
      "Unique input1s: 3660\n",
      "Unique input2s: 3660\n",
      "Unique input pairs: 147214\n",
      "------------------------------\n",
      "82821\n",
      "0.3943857142857143\n",
      "Final Unique errors: 20423\n",
      "Final Unique input1s: 3660\n",
      "Final Unique input2s: 3660\n",
      "Final Unique input pairs: 150403\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input2_set.add(input2)\n",
    "    unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "        print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "        print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stephanie made me feel scared.; Amanda made me feel scared.', 'Adam made me feel depressed.; Ellen made me feel depressed.', 'Andrew was devastated.; Alan was devastated.', 'Adam made me feel furious.; Katie made me feel furious.', 'Harry told us all about the recent heartbreaking events.; Ellen told us all about the recent heartbreaking events.', 'The conversation with Nancy was heartbreaking.; The conversation with Harry was heartbreaking.', 'Betsy made me feel irritated.; Josh made me feel irritated.', 'Amanda was furious.; Katie was furious.', 'Nancy made me feel discouraged.; Roger made me feel discouraged.', 'Amanda feels terrified.; Courtney feels terrified.', 'Josh feels devastated.; Ryan feels devastated.', 'Alan feel devastated.; Betsy feel devastated.', 'Ryan was terrified.; Amanda was terrified.', 'Ryan feels discouraged.; Harry feels discouraged.', 'Frank feels ecstatic.; Amanda feels ecstatic.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Neutral (Sentiment) Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice = 0 #5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "unique_input_pair_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique input1s: 77\n",
      "Unique input2s: 76\n",
      "Unique input pairs: 86\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 89\n",
      "Unique input2s: 88\n",
      "Unique input pairs: 109\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 90\n",
      "Unique input2s: 89\n",
      "Unique input pairs: 119\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 90\n",
      "Unique input2s: 90\n",
      "Unique input pairs: 123\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 90\n",
      "Unique input2s: 90\n",
      "Unique input pairs: 125\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 97\n",
      "Unique input2s: 99\n",
      "Unique input pairs: 262\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 100\n",
      "Unique input2s: 100\n",
      "Unique input pairs: 372\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 100\n",
      "Unique input2s: 100\n",
      "Unique input pairs: 463\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 100\n",
      "Unique input2s: 100\n",
      "Unique input pairs: 550\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique input1s: 100\n",
      "Unique input2s: 100\n",
      "Unique input pairs: 625\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique input1s: 206\n",
      "Unique input2s: 206\n",
      "Unique input pairs: 852\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique input1s: 274\n",
      "Unique input2s: 255\n",
      "Unique input pairs: 996\n",
      "------------------------------\n",
      "Unique errors: 15\n",
      "Unique input1s: 297\n",
      "Unique input2s: 293\n",
      "Unique input pairs: 1131\n",
      "------------------------------\n",
      "Unique errors: 18\n",
      "Unique input1s: 310\n",
      "Unique input2s: 310\n",
      "Unique input pairs: 1272\n",
      "------------------------------\n",
      "Unique errors: 21\n",
      "Unique input1s: 320\n",
      "Unique input2s: 314\n",
      "Unique input pairs: 1403\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 451\n",
      "Unique input2s: 449\n",
      "Unique input pairs: 1692\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 543\n",
      "Unique input2s: 535\n",
      "Unique input pairs: 1840\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 596\n",
      "Unique input2s: 594\n",
      "Unique input pairs: 1986\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 642\n",
      "Unique input2s: 646\n",
      "Unique input pairs: 2134\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 684\n",
      "Unique input2s: 680\n",
      "Unique input pairs: 2281\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 719\n",
      "Unique input2s: 720\n",
      "Unique input pairs: 2557\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 727\n",
      "Unique input2s: 728\n",
      "Unique input pairs: 2689\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 732\n",
      "Unique input2s: 730\n",
      "Unique input pairs: 2826\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 734\n",
      "Unique input2s: 731\n",
      "Unique input pairs: 2956\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 735\n",
      "Unique input2s: 732\n",
      "Unique input pairs: 3090\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 784\n",
      "Unique input2s: 797\n",
      "Unique input pairs: 3363\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 807\n",
      "Unique input2s: 816\n",
      "Unique input pairs: 3493\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 812\n",
      "Unique input2s: 825\n",
      "Unique input pairs: 3613\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 824\n",
      "Unique input2s: 826\n",
      "Unique input pairs: 3731\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique input1s: 830\n",
      "Unique input2s: 831\n",
      "Unique input pairs: 3849\n",
      "------------------------------\n",
      "24\n",
      "0.0044444444444444444\n",
      "Final Unique errors: 23\n",
      "Final Unique input1s: 831\n",
      "Final Unique input2s: 832\n",
      "Final Unique input pairs: 3960\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 300 #00 #00 #0\n",
    "num_iter = 50 #00 #10000 #20 #0\n",
    "\n",
    "for noun_choice in [0, 1, 2, 3, 4, 5]:\n",
    "    for i in range(ITERS):\n",
    "        tokens = select_tokens(noun_choice)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 0)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 1)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 2)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        if (i > 0) and  (i % num_iter == 0):\n",
    "            print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "            print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "            print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "            print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "            print(\"------------------------------\")\n",
    "        #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(6 * 3 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I talked to the attendant in the market.; I talked to the farmer in the market.', 'the attendant has two children.; the farmer has two children.', 'the attendant has two children.; the supervisor has two children.', 'the attendant saw me yesterday.; the mover talked to me yesterday.', 'I saw the attendant yesterday.; I saw the driver yesterday.', 'the attendant talked to me in the market.; the supervisor talked to me in the market.', 'the attendant talked to me yesterday.; the salesperson saw me yesterday.', 'I talked to the attendant in the market.; I talked to the mover in the market.', 'the attendant saw me yesterday.; the farmer saw me yesterday.', 'the attendant talked to me yesterday.; the mechanic talked to me yesterday.', 'the attendant has two children.; the driver has two children.', 'the attendant has two children.; the mover has two children.', 'the attendant saw me yesterday.; the farmer talked to me yesterday.', 'the attendant goes to the school in our neighborhood.; the chief goes to the school in our neighborhood.', 'the attendant saw me yesterday.; the driver saw me yesterday.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
