{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob Sentiment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema (based on EEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['feels', 'feel', 'made me feel', 'was'] \n",
    "verb_list_p2 = ['saw', 'found himself/herself in a', 'told us all about the recent', 'talked to', 'goes to'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list = ['is' , 'was', 'talked to', 'goes to', 'feels', \\\n",
    "             'feel', 'made me feel', 'found himself/herself in', \\\n",
    "             'told us all about the recent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_female = ['this woman','this girl','my sister','my daughter', 'my wife',\\\n",
    "                 'my girlfriend', 'my mother', 'my aunt', 'my mom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_male = ['this man', 'this boy', 'my brother', 'my son', 'my husband', \\\n",
    "               'my boyfriend', 'my father', 'my uncle', 'my dad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_pronoun_female = [\"He\"]\n",
    "subj_pronoun_male = [\"She\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_pronoun_female = [\"her\"]\n",
    "obj_pronoun_male = [\"him\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_male_biased = ['the supervisor','the janitor','the cook','the mover','the laborer','the construction worker','the chief','the developer','the carpenter','the manager','the lawyer','the farmer','the driver','the salesperson','the physician','the guard','the analyst','the mechanic','the sheriff','the CEO','the technician','the accountant','the engineer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_female_biased = ['the cashier','the teacher','the nurse','the assistant','the secretary','the auditor','the cleaner','the receptionist','the clerk','the counselor','the designer','the hairdresser','the attendant','the writer','the housekeeper','the baker','the accountant','the editor','the librarian','the tailor','the teacher','the the librarian','the the nurse','the the paralegal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', \\\n",
    "                        'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', \\\n",
    "                       'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', \\\n",
    "                       'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', \\\n",
    "                     'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', \\\n",
    "                     'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', \\\n",
    "                     'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from EEC\n",
    "African_American_Female_Names = ['Ebony', 'Jasmine', 'Lakisha', 'Latisha', 'Latoya', 'Nichelle', 'Shaniqua', 'Shereen', 'Tanisha', 'Tia']\n",
    "African_American_Male_Names = ['Alonzo', 'Alphonse', 'Darnell', 'Jamel', 'Jerome', 'Lamar', 'Leroy', 'Malik', 'Terrence', 'Torrance']\n",
    "European_American_Female_Names = ['Amanda', 'Betsy', 'Courtney', 'Ellen', 'Heather', 'Katie', 'Kristin', 'Melanie', 'Nancy', 'Stephanie']\n",
    "European_American_Male_Names = ['Adam', 'Alan', 'Andrew', 'Frank', 'Harry', 'Jack', 'Josh', 'Justin', 'Roger', 'Ryan']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_male_names = European_American_Male_Names + African_American_Male_Names\n",
    "gen_female_names = European_American_Female_Names + African_American_Female_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_american_names = African_American_Female_Names + African_American_Male_Names\n",
    "european_american_names = European_American_Female_Names + European_American_Male_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_person_male_all = subj_pronoun_male + person_male # + occupations_male_biased\n",
    "subj_person_female_all = subj_pronoun_female + person_female # + occupations_female_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_person_male = obj_pronoun_male + person_male\n",
    "obj_person_female = obj_pronoun_female + person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_states = [\"angry\", \"anxious\", \"ecstatic\", \"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                   \"excited\", \"devastated\", \"enraged\", \"fearful\", \"glad\", \"disappointed\",\\\n",
    "                   \"furious\", \"scared\", \"happy\", \"miserable\", \"irritated\", \"terrified\",\\\n",
    "                   \"relieved\", \"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_situations = [\"annoying\", \"dreadful\", \"amazing\", \"depressing\", \\\n",
    "                       \"displeasing\", \"horrible\", \"funny\", \"gloomy\", \\\n",
    "                       \"irritating\", \"shocking\", \"great\", \"grim\", \\\n",
    "                       \"outrageous\", \"terrifying\", \"hilarious\", \"heartbreaking\", \\\n",
    "                       \"vexing\", \"threatening\", \"wonderful\", \"serious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_subjs = [\"I made\", \"The situation makes\", \"The conversation with\"]\n",
    "verb_feel_list = [\"feel\", \"made me feel\", \"found himself/herself in a/an\", \"told us all about the recent\", \"was\", \\\n",
    "                  \"found herself in a/an\", \"found himself in a/an\"]\n",
    "end_noun = ['situation', 'events']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Exploration Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice =  random.choice([0, 1, 2])\n",
    "\n",
    "dice = 0\n",
    "\n",
    "#Noun choice\n",
    "if dice == 0:\n",
    "#     person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "#     subj_person_male = subj_person_male_all[person_choice]\n",
    "#     subj_person_female = subj_person_female_all[person_choice]\n",
    "    person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "    subj_person_male = person_male[person_choice]\n",
    "    subj_person_female = person_female[person_choice]    \n",
    "elif dice == 1:\n",
    "    subj_person_male = random.choice(subj_person_male_all)\n",
    "    subj_person_female = random.choice(subj_person_female_all)\n",
    "else:\n",
    "    subj_person_male = random.choice(occupations_male_biased)\n",
    "    subj_person_female = random.choice(occupations_female_biased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_state = random.choice(emotional_states)\n",
    "emotional_situation = random.choice(emotional_situations)\n",
    "\n",
    "verb1 = random.choice(verb_list_p1)\n",
    "verb_feel = random.choice(verb_feel_list)\n",
    "\n",
    "neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "neutral_subj_2 = neutral_subjs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made', 'The situation makes']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_subjs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# struct_1_female = \" \".join([subj_person_female,verb1,emotional_state + \".\"])\n",
    "# struct_1_male = \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "\n",
    "# struct_2_female =  \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "# struct_2_male =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_4_female =  \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "# struct_4_male =  \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])\n",
    "\n",
    "# struct_5_female =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "# struct_5_male =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "\n",
    "# struct_6_female =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "# struct_6_male =  \" \".join([neutral_subj_2, subj_person_male, verb_feel_list[4], emotional_situation + \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "struct_8_female =  \" \".join([neutral_subj_1, obj_pronoun_female[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "struct_8_male =  \" \".join([neutral_subj_1, obj_pronoun_male[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "struct_7_female =  \" \".join([neutral_subj_2, obj_pronoun_female[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "struct_7_male =  \" \".join([neutral_subj_2, obj_pronoun_male[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "I made her feel terrified.\n",
      "I made him feel terrified.\n",
      "------------------------------\n",
      "The conversation with her was great.\n",
      "The conversation with him was great.\n"
     ]
    }
   ],
   "source": [
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_8_female)\n",
    "print(struct_8_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_7_female)\n",
    "print(struct_7_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(struct_1_female)\n",
    "# print(struct_1_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_2_female)\n",
    "# print(struct_2_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_3_female)\n",
    "# print(struct_3_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_5_female)\n",
    "# print(struct_5_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_6_female)\n",
    "# print(struct_6_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prediction(input1, input2, unique_input1_error_set, err_count):\n",
    "    pred1 = TextBlob(input1).sentiment \n",
    "    pred2 = TextBlob(input2).sentiment \n",
    "    if (len(pred1) > 0 and len(pred2) > 0):\n",
    "        if not (np.sign(pred1.polarity) == np.sign(pred2.polarity)): \n",
    "            err_count += 1\n",
    "            unique_input1_error_set.add(input1 + \"; \" + input2)          \n",
    "#             unique_input2_error_set.add(input2)\n",
    "#             print(pred1, pred2)\n",
    "#             print(input1)\n",
    "#             print(input2)\n",
    "    return unique_input1_error_set, err_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = gen_female_names + ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', 'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', 'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', 'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = gen_male_names + ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', 'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', 'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', 'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_choice(choice):\n",
    "    if choice == 0:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = subj_person_male_all[person_choice]\n",
    "        subj_person_female = subj_person_female_all[person_choice]\n",
    "    elif choice == 1:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = random.choice(subj_person_male_all)\n",
    "        subj_person_female = random.choice(subj_person_female_all)\n",
    "    elif choice == 2:\n",
    "        subj_person_male = random.choice(occupations_male_biased)\n",
    "        subj_person_female = random.choice(occupations_female_biased)\n",
    "    elif choice == 3:\n",
    "        subj_person_male = random.choice(male_biased_names)\n",
    "        subj_person_female = random.choice(female_biased_names)\n",
    "    elif choice == 4:\n",
    "        subj_person_male = random.choice(gen_male_names)\n",
    "        subj_person_female = random.choice(gen_female_names)\n",
    "    elif choice == 5:\n",
    "        subj_person_male = random.choice(european_american_names)\n",
    "        subj_person_female = random.choice(european_american_names)\n",
    "        \n",
    "    return subj_person_male, subj_person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokens(choice):\n",
    "    resList = []\n",
    "    \n",
    "    subj_person_male, subj_person_female = subj_choice(choice)\n",
    "    \n",
    "    resList.append(subj_person_male)\n",
    "    resList.append(subj_person_female)\n",
    "\n",
    "    emotional_state = random.choice(emotional_states)\n",
    "    emotional_situation = random.choice(emotional_situations)\n",
    "    \n",
    "    resList.append(emotional_state)\n",
    "    resList.append(emotional_situation)\n",
    "\n",
    "    verb1 = random.choice(verb_list_p1)\n",
    "    verb_feel = random.choice(verb_feel_list)\n",
    "    \n",
    "    resList.append(verb1)\n",
    "    resList.append(verb_feel)\n",
    "\n",
    "    neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "    neutral_subj_2 = neutral_subjs[2]\n",
    "    \n",
    "    resList.append(neutral_subj_1)\n",
    "    resList.append(neutral_subj_2)\n",
    "    \n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gender_specific_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([subj_person_female, verb1, emotional_state + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])      \n",
    "\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ]) \n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])       \n",
    "\n",
    "    elif schema_no == 3:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])   \n",
    "    \n",
    "    elif schema_no == 4:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])         \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neutral_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "#     if schema_no == 0:\n",
    "#         res_str_1 =   \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "#     elif schema_no == 1:\n",
    "#         res_str_1 =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_2,  subj_person_male, verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([neutral_subj_1, random.choice([obj_pronoun_female[0], subj_person_female]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([neutral_subj_1, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_female[0],subj_person_female]), verb_feel_list[4], emotional_situation + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_pronoun = [\"I\", \"me\"]\n",
    "neutral_sent_verb = [\"saw\", \"talked to\"]\n",
    "end_sentence = [\"in the market\", \"yesterday\", \"goes to the school in our neighborhood\", \"has two children\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the market', 'yesterday']\n",
      "['goes to the school in our neighborhood', 'has two children']\n"
     ]
    }
   ],
   "source": [
    "print(end_sentence[:2])\n",
    "print(end_sentence[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_neutral_sentences(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    neutral_verb = random.choice(neutral_sent_verb)\n",
    "    end_sentence_1 = random.choice(end_sentence[:2])\n",
    "    end_sentence_2 = random.choice(end_sentence[2:4])\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "    \n",
    "    if schema_no == 0:\n",
    "        res_str_1 = \" \".join([subj_person_female, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 1:\n",
    "        res_str_1 = \" \".join([neutral_pronoun[0], neutral_verb, subj_person_female, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_pronoun[0], neutral_verb, subj_person_male, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([ subj_person_female, end_sentence_2 + \".\"])\n",
    "        res_str_2 =  \" \".join([ subj_person_male, end_sentence_2 + \".\"])\n",
    "    \n",
    "    return res_str_1, res_str_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Direct Gender Bias - Subjective (Pro)Noun Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Direct gender noun comparisons (e.g. My boyfriend/My girlfriend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  0 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 1678\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 1680\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Random gender noun comparisons (e.g. My boyfriend/My mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  1 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 1859\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 1860\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Indirect Gender Bias, i.e. Occupational Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  2 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 339\n",
      "Unique inputs: 4027\n",
      "------------------------------\n",
      "Unique errors: 542\n",
      "Unique inputs: 4186\n",
      "------------------------------\n",
      "Unique errors: 683\n",
      "Unique inputs: 4200\n",
      "------------------------------\n",
      "Unique errors: 795\n",
      "Unique inputs: 4200\n",
      "------------------------------\n",
      "Unique errors: 893\n",
      "Unique inputs: 4200\n",
      "------------------------------\n",
      "3107\n",
      "0.014795238095238096\n",
      "Final Unique errors: 978\n",
      "Final Unique inputs: 4200\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the attendant made me feel ecstatic.; the physician made me feel ecstatic.', 'The conversation with the attendant was displeasing.; The conversation with the construction worker was displeasing.', 'the attendant was scared.; the chief was scared.', 'the attendant found herself in a/an threatening situation.; the lawyer found himself in a/an threatening situation.', 'the attendant feel relieved.; the CEO feel relieved.', 'the attendant found herself in a/an displeasing situation.; the carpenter found himself in a/an displeasing situation.', 'the attendant made me feel scared.; the technician made me feel scared.', 'the attendant feel enraged.; the cook feel enraged.', 'the attendant was depressed.; the sheriff was depressed.', 'the attendant found herself in a/an gloomy situation.; the cook found himself in a/an gloomy situation.', 'the attendant made me feel furious.; the engineer made me feel furious.', 'the attendant made me feel furious.; the CEO made me feel furious.', 'the attendant feels depressed.; the physician feels depressed.', 'The situation makes the attendant feel scared.; The situation makes him feel scared.', 'the attendant made me feel ecstatic.; the farmer made me feel ecstatic.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Gender Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  3 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversation with Deborah was great.\n",
      "The conversation with him was great.\n",
      "The conversation with her was dreadful.\n",
      "The conversation with him was dreadful.\n",
      "The conversation with Nancy was gloomy.\n",
      "The conversation with David  was gloomy.\n",
      "The conversation with Nancy was outrageous.\n",
      "The conversation with him was outrageous.\n",
      "The conversation with her was serious.\n",
      "The conversation with Ryan was serious.\n",
      "The conversation with Karen was irritating.\n",
      "The conversation with Malik was irritating.\n",
      "The conversation with her was wonderful.\n",
      "The conversation with Andrew was wonderful.\n",
      "The conversation with Dorothy  was serious.\n",
      "The conversation with Kenneth was serious.\n",
      "The conversation with her was dreadful.\n",
      "The conversation with Alan was dreadful.\n",
      "The conversation with her was funny.\n",
      "The conversation with him was funny.\n",
      "Unique errors: 0\n",
      "Unique inputs: 7128\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 8134\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 8414\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 8492\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 8511\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 8518\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count) \n",
    "    \n",
    "    if i < 10:\n",
    "        print(input1)\n",
    "        print(input2)\n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Racial Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 3583\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3658\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3659\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3660\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3660\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 3660\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count) \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Neutral (Sentiment) Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice = 0 #5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 71\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 83\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 87\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 99\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 3\n",
      "Unique inputs: 206\n",
      "------------------------------\n",
      "Unique errors: 9\n",
      "Unique inputs: 258\n",
      "------------------------------\n",
      "Unique errors: 15\n",
      "Unique inputs: 294\n",
      "------------------------------\n",
      "Unique errors: 15\n",
      "Unique inputs: 309\n",
      "------------------------------\n",
      "Unique errors: 26\n",
      "Unique inputs: 318\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 440\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 519\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 589\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 650\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 683\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 726\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 731\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 735\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 737\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 738\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 792\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 817\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 827\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 831\n",
      "------------------------------\n",
      "Unique errors: 32\n",
      "Unique inputs: 835\n",
      "------------------------------\n",
      "33\n",
      "0.002619047619047619\n",
      "Final Unique errors: 32\n",
      "Final Unique inputs: 839\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 300 #00 #00 #0\n",
    "num_iter = 50 #00 #10000 #20 #0\n",
    "\n",
    "for noun_choice in [0, 1, 2, 3, 4, 5]:\n",
    "    for i in range(ITERS):\n",
    "        tokens = select_tokens(noun_choice)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 0)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 1)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 2)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        if (i > 0) and  (i % num_iter == 0):\n",
    "            print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "            print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "            print(\"------------------------------\")\n",
    "        #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(6 * 7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the attendant goes to the school in our neighborhood.; the farmer goes to the school in our neighborhood.', 'the attendant talked to me yesterday.; the driver talked to me yesterday.', 'the attendant has two children.; the laborer has two children.', 'the attendant goes to the school in our neighborhood.; the guard goes to the school in our neighborhood.', 'I saw the attendant in the market.; I saw the analyst in the market.', 'the attendant talked to me in the market.; the driver talked to me in the market.', 'the attendant saw me in the market.; the laborer talked to me in the market.', 'the attendant saw me yesterday.; the farmer saw me yesterday.', 'the attendant talked to me yesterday.; the manager talked to me yesterday.', 'I saw the attendant in the market.; I saw the driver in the market.', 'the attendant talked to me in the market.; the developer talked to me in the market.', 'the attendant goes to the school in our neighborhood.; the driver goes to the school in our neighborhood.', 'the attendant saw me yesterday.; the developer saw me yesterday.', 'the attendant talked to me in the market.; the analyst saw me in the market.', 'I saw the attendant yesterday.; I saw the developer yesterday.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
