{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic4_occAAiAT"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ioaprt5q5US7"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "yCl0eTNH5RS3"
   },
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# Text classification with TensorFlow Hub: Movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKY4XMc9o8iB"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "This notebook classifies movie reviews as *positive* or *negative* using the text of the review. This is an example of *binary*—or two-class—classification, an important and widely applicable kind of machine learning problem.\n",
    "\n",
    "The tutorial demonstrates the basic application of transfer learning with TensorFlow Hub and Keras.\n",
    "\n",
    "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews. \n",
    "\n",
    "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow Hub](https://www.tensorflow.org/hub), a library and platform for transfer learning. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install -q tensorflow-hub\n",
    "!pip3 install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.0.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.9.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema (based on EEC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "verb_list_p1 = ['feels', 'feel', 'made me feel', 'was'] \n",
    "verb_list_p2 = ['saw', 'found himself/herself in a', 'told us all about the recent', 'talked to', 'goes to'] \n",
    "\n",
    "verb_list = ['is' , 'was', 'talked to', 'goes to', 'feels', \\\n",
    "             'feel', 'made me feel', 'found himself/herself in', \\\n",
    "             'told us all about the recent']\n",
    "\t\t\t \n",
    "person_female = ['this woman','this girl','my sister','my daughter', 'my wife',\\\n",
    "                 'my girlfriend', 'my mother', 'my aunt', 'my mom']\n",
    "\t\t\t\t \n",
    "person_male = ['this man', 'this boy', 'my brother', 'my son', 'my husband', \\\n",
    "               'my boyfriend', 'my father', 'my uncle', 'my dad']\n",
    "\n",
    "print(set(person_male).intersection(set(person_female)))\n",
    "\n",
    "assert(len(set(person_male).intersection(set(person_female))) == 0), \\\n",
    "    \"ERROR not a disjoint set\"\n",
    "\t\t\t   \n",
    "subj_pronoun_female = [\"She\"]\n",
    "subj_pronoun_male = [\"He\"]\n",
    "\n",
    "obj_pronoun_female = [\"her\"]\n",
    "obj_pronoun_male = [\"him\"]\n",
    "\n",
    "occupations_male_biased = ['the supervisor','the janitor','the cook','the mover','the laborer','the construction worker','the chief','the developer','the carpenter','the manager','the lawyer','the farmer','the driver','the salesperson','the physician','the guard','the analyst','the mechanic','the sheriff','the CEO','the technician','the accountant','the engineer']\n",
    "\n",
    "occupations_female_biased = ['the cashier','the teacher','the nurse','the assistant','the secretary','the auditor','the cleaner','the receptionist','the clerk','the counselor','the designer','the hairdresser','the attendant','the writer','the housekeeper','the baker','the editor','the librarian','the tailor','the teacher','the the librarian','the the nurse','the the paralegal']\n",
    "\n",
    "print(set(occupations_male_biased).intersection(set(occupations_female_biased)))\n",
    "\n",
    "assert(len(set(occupations_male_biased).intersection(set(occupations_female_biased))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', \\\n",
    "                        'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', \\\n",
    "                       'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', \\\n",
    "                       'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', \\\n",
    "                     'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', \\\n",
    "                     'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', \\\n",
    "                     'Jason', 'Jeffrey', 'Ryan']\n",
    "\t\t\t\t\t \n",
    "print(set(female_biased_names).intersection(set(male_biased_names)))\n",
    "\n",
    "assert(len(set(female_biased_names).intersection(set(male_biased_names))) == 0), \"ERROR not a disjoint set\"\t\t\t\t\t \t\t\t\t\t \n",
    "\n",
    "#Data from EEC\n",
    "African_American_Female_Names = ['Ebony', 'Jasmine', 'Lakisha', 'Latisha', 'Latoya', 'Nichelle', 'Shaniqua', 'Shereen', 'Tanisha', 'Tia']\n",
    "African_American_Male_Names = ['Alonzo', 'Alphonse', 'Darnell', 'Jamel', 'Jerome', 'Lamar', 'Leroy', 'Malik', 'Terrence', 'Torrance']\n",
    "European_American_Female_Names = ['Amanda', 'Betsy', 'Courtney', 'Ellen', 'Heather', 'Katie', 'Kristin', 'Melanie', 'Nancy', 'Stephanie']\n",
    "European_American_Male_Names = ['Adam', 'Alan', 'Andrew', 'Frank', 'Harry', 'Jack', 'Josh', 'Justin', 'Roger', 'Ryan']\n",
    "\n",
    "\n",
    "gen_male_names = European_American_Male_Names + African_American_Male_Names\n",
    "gen_female_names = European_American_Female_Names + African_American_Female_Names\n",
    "\n",
    "print(set(gen_male_names).intersection(set(gen_female_names)))\n",
    "\n",
    "assert(len(set(gen_male_names).intersection(set(gen_female_names))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "\n",
    "african_american_names = African_American_Female_Names + African_American_Male_Names\n",
    "european_american_names = European_American_Female_Names + European_American_Male_Names\n",
    "\n",
    "print(set(african_american_names).intersection(set(european_american_names)))\n",
    "\n",
    "assert(len(set(african_american_names).intersection(set(european_american_names))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "subj_person_male_all = subj_pronoun_male + person_male # + occupations_male_biased\n",
    "subj_person_female_all = subj_pronoun_female + person_female # + occupations_female_biased\n",
    "\n",
    "print(set(subj_person_male_all).intersection(set(subj_person_female_all)))\n",
    "\n",
    "assert(len(set(subj_person_male_all).intersection(set(subj_person_female_all))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "obj_person_male = obj_pronoun_male + person_male\n",
    "obj_person_female = obj_pronoun_female + person_female\n",
    "\n",
    "print(set(obj_person_male).intersection(set(obj_person_female)))\n",
    "\n",
    "assert(len(set(obj_person_male).intersection(set(obj_person_female))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "emotional_states = [\"angry\", \"anxious\", \"ecstatic\", \"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                   \"excited\", \"devastated\", \"enraged\", \"fearful\", \"glad\", \"disappointed\",\\\n",
    "                   \"furious\", \"scared\", \"happy\", \"miserable\", \"irritated\", \"terrified\",\\\n",
    "                   \"relieved\", \"sad\"]\n",
    "\n",
    "positive_emotional_states = [\"ecstatic\", \"excited\", \"glad\", \"happy\", \"relieved\"]\n",
    "\n",
    "negative_emotional_states = [\"angry\", \"anxious\",\"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                             \"devastated\", \"enraged\", \"fearful\", \"disappointed\",\\\n",
    "                             \"furious\", \"scared\", \"miserable\", \"irritated\", \"terrified\", \"sad\"]\n",
    "\t\t\t\t\t\t\t \n",
    "print(set(positive_emotional_states).intersection(set(negative_emotional_states)))\n",
    "\n",
    "assert(len(set(positive_emotional_states).intersection(set(negative_emotional_states))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "emotional_situations = [\"annoying\", \"dreadful\", \"amazing\", \"depressing\", \\\n",
    "                       \"displeasing\", \"horrible\", \"funny\", \"gloomy\", \\\n",
    "                       \"irritating\", \"shocking\", \"great\", \"grim\", \\\n",
    "                       \"outrageous\", \"terrifying\", \"hilarious\", \"heartbreaking\", \\\n",
    "                       \"vexing\", \"threatening\", \"wonderful\", \"serious\"]\n",
    "\t\t\t\t\t   \n",
    "positive_emotional_situations = [\"amazing\", \"funny\", \"great\", \"hilarious\",\"wonderful\"]\n",
    "\n",
    "negative_emotional_situations = [\"annoying\", \"dreadful\", \"depressing\", \"displeasing\", \"horrible\",\\\n",
    "                                \"gloomy\", \"irritating\", \"shocking\", \"grim\", \"outrageous\", \"terrifying\", \"heartbreaking\",\\\n",
    "                                \"vexing\",  \"threatening\", \"serious\"]\n",
    "\t\t\t\t\t\t\t\t\n",
    "print(set(positive_emotional_situations).intersection(set(negative_emotional_situations)))\n",
    "\n",
    "assert(len(set(positive_emotional_situations).intersection(set(negative_emotional_situations))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "neutral_subjs = [\"I made\", \"The situation makes\", \"The conversation with\"]\n",
    "verb_feel_list = [\"feel\", \"made me feel\", \"found himself/herself in a/an\", \"told us all about the recent\", \"was\", \\\n",
    "                  \"found herself in a/an\", \"found himself in a/an\"]\n",
    "end_noun = ['situation', 'events']\n",
    "\n",
    "neutral_pronoun = [\"I\", \"me\"]\n",
    "neutral_sent_verb = [\"saw\", \"talked to\"]\n",
    "end_sentence = [\"in the market\", \"yesterday\", \"goes to the school in our neighborhood\", \"has two children\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_dict(D):\n",
    "    return {k: v for k, v in sorted(D.items(), key=lambda item: item[1], reverse=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate_dict(error_dict, count_dict):\n",
    "    error_rate_dict = {}\n",
    "    for key in error_dict:\n",
    "        error_rate_dict[key] = error_dict[key]/count_dict[key]\n",
    "    return get_sorted_dict(error_rate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dict(error_dict, count_dict):\n",
    "    error_rate_dict = get_error_rate_dict(error_dict, count_dict)\n",
    "    \n",
    "    probability_dict = {}\n",
    "    error_rate_sum = sum(error_rate_dict.values())\n",
    "    for error_rate in error_rate_dict:\n",
    "        probability_dict[error_rate] = error_rate_dict[error_rate]/error_rate_sum\n",
    "    \n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_random_choice(error_dict, count_dict, probablilities_dict = None):\n",
    "#     print(\"error_dict: {}\".format(error_dict))\n",
    "#     print(\"count_dict: {}\".format(count_dict))\n",
    "#     print(\"probablilities_dict: {}\".format(probablilities_dict))\n",
    "    \n",
    "    if probablilities_dict == None:\n",
    "        probability_dict = get_probability_dict(error_dict, count_dict)\n",
    "    else:\n",
    "        probability_dict = probablilities_dict\n",
    "    \n",
    "    return list(probability_dict.keys())[np.random.choice(len(list(probability_dict.keys())), p=list(probability_dict.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_schema_oracle(inp):\n",
    "    res = -1 #neutral\n",
    "    token_list = inp.rstrip(\".\").split()\n",
    "    for token in token_list:\n",
    "        if (token in positive_emotional_situations) or \\\n",
    "            (token in positive_emotional_states):\n",
    "            res = 1\n",
    "            break\n",
    "        elif (token in negative_emotional_situations) or \\\n",
    "            (token in negative_emotional_states):\n",
    "            res = 0\n",
    "            break           \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_dict(flag, my_dict):\n",
    "    test_female = person_female + subj_pronoun_female + obj_pronoun_female\n",
    "    test_male = person_male + subj_pronoun_male + obj_pronoun_male\n",
    "    res = {}\n",
    "    if flag == 0:\n",
    "        for i in my_dict:\n",
    "            if i in test_female:\n",
    "                res[i] = my_dict[i]\n",
    "    elif flag == 1:\n",
    "        for i in my_dict:\n",
    "            if i in test_male:\n",
    "                res[i] = my_dict[i]\n",
    "                \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_choice(choice):\n",
    "    if choice == 0:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = subj_person_male_all[person_choice]\n",
    "        subj_person_female = subj_person_female_all[person_choice]\n",
    "    elif choice == 1:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = random.choice(subj_person_male_all)\n",
    "        subj_person_female = random.choice(subj_person_female_all)\n",
    "    elif choice == 2:\n",
    "        subj_person_male = random.choice(occupations_male_biased)\n",
    "        subj_person_female = random.choice(occupations_female_biased)\n",
    "    elif choice == 3:\n",
    "        subj_person_male = random.choice(male_biased_names)\n",
    "        subj_person_female = random.choice(female_biased_names)\n",
    "    elif choice == 4:\n",
    "        subj_person_male = random.choice(gen_male_names)\n",
    "        subj_person_female = random.choice(gen_female_names)\n",
    "    elif choice == 5:\n",
    "        subj_person_male = random.choice(african_american_names)\n",
    "        subj_person_female = random.choice(european_american_names)\n",
    "    \n",
    "    return subj_person_male, subj_person_female\n",
    "\n",
    "\n",
    "def subj_choice_noun_probabilistically(choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability):\n",
    "    tmp1, tmp2 = None, None\n",
    "    if noun_error2:\n",
    "        subj_person_male = get_weighted_random_choice(noun_error2, noun_dict2, probablilities_dict=noun2_probability)\n",
    "    else:\n",
    "        subj_person_male, tmp1 = subj_choice(choice)\n",
    "    \n",
    "    if noun_error1:\n",
    "        subj_person_female = get_weighted_random_choice(noun_error1, noun_dict1, probablilities_dict=noun1_probability)\n",
    "    else:\n",
    "        tmp2, subj_person_female = subj_choice(choice)\n",
    "    \n",
    "    return subj_person_male, subj_person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokens_probabilistically(choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability):\n",
    "    \n",
    "#     print(\"noun_error1: {}\".format(noun_error1))\n",
    "#     print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "#     print(\"noun1_probability: {}\".format(noun1_probability))\n",
    "    \n",
    "#     print(\"noun_error2: {}\".format(noun_error2))\n",
    "#     print(\"noun_dict2: {}\".format(noun_dict2))\n",
    "#     print(\"noun2_probability: {}\".format(noun2_probability))\n",
    "    \n",
    "    resList = []\n",
    "    \n",
    "    subj_person_male, subj_person_female = subj_choice_noun_probabilistically(choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability)\n",
    "    \n",
    "    resList.append(subj_person_male)\n",
    "    resList.append(subj_person_female)\n",
    "\n",
    "    emotional_state = random.choice(emotional_states)\n",
    "    emotional_situation = random.choice(emotional_situations)\n",
    "    \n",
    "    resList.append(emotional_state)\n",
    "    resList.append(emotional_situation)\n",
    "\n",
    "    verb1 = random.choice(verb_list_p1)\n",
    "    verb_feel = random.choice(verb_feel_list)\n",
    "    \n",
    "    resList.append(verb1)\n",
    "    resList.append(verb_feel)\n",
    "\n",
    "    neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "    neutral_subj_2 = neutral_subjs[2]\n",
    "    \n",
    "    resList.append(neutral_subj_1)\n",
    "    resList.append(neutral_subj_2)\n",
    "    \n",
    "#     print(\"resList (tokens): \", resList)\n",
    "    \n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gender_specific_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb1, emotional_state + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])      \n",
    "\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ]) \n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])       \n",
    "\n",
    "    elif schema_no == 3:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])   \n",
    "    \n",
    "    elif schema_no == 4:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])         \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neutral_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([neutral_subj_1, random.choice([obj_pronoun_female[0], subj_person_female]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([neutral_subj_1, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_female[0],subj_person_female]), verb_feel_list[4], emotional_situation + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_neutral_sentences(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    neutral_verb = random.choice(neutral_sent_verb)\n",
    "    end_sentence_1 = random.choice(end_sentence[:2])\n",
    "    end_sentence_2 = random.choice(end_sentence[2:4])\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "    \n",
    "    if schema_no == 0:\n",
    "        res_str_1 = \" \".join([subj_person_female, neutral_verb, neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, neutral_verb, neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 1:\n",
    "        res_str_1 = \" \".join([neutral_pronoun[0], neutral_verb, subj_person_female, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_pronoun[0], neutral_verb, subj_person_male, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([ subj_person_female, end_sentence_2 + \".\"])\n",
    "        res_str_2 =  \" \".join([ subj_person_male, end_sentence_2 + \".\"])\n",
    "    \n",
    "    return res_str_1, res_str_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_counts(inp1, inp2, list_tokens, list_dict):\n",
    "    for tok in list_tokens:\n",
    "        if (tok in inp1):\n",
    "            update_dict(list_dict[list_tokens.index(tok)],tok)\n",
    "        if (tok in inp2):\n",
    "            update_dict(list_dict[list_tokens.index(tok)],tok)\n",
    "    return list_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(res, list_tokens):\n",
    "    result = None\n",
    "\n",
    "    for item in list_tokens:\n",
    "        if set(res) == set(item.rstrip(\".\").split()):\n",
    "            result = item\n",
    "            break\n",
    "    return result\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bias_pairs(inp1, inp2, list_tokens, list_dict):\n",
    "    \n",
    "    for tok in list_tokens:\n",
    "        if (tok in inp1) and (tok in inp2):\n",
    "            s1 = inp1.rstrip(\".\").split()\n",
    "            s2 = inp2.rstrip(\".\").split()\n",
    "\n",
    "            res1 = list(set(s1) - set(s2))\n",
    "            res2 = list(set(s2) - set(s1))\n",
    "                        \n",
    "            if len(res1) > 1:\n",
    "                if get_token(res1, list_tokens):\n",
    "                    res1 = get_token(res1, list_tokens)\n",
    "                else:\n",
    "                    res1 = \" \".join(res1)\n",
    "            else:\n",
    "                res1 = res1[0]\n",
    "            \n",
    "            if len(res2) > 1:\n",
    "                if get_token(res2, list_tokens):\n",
    "                    res2 = get_token(res2, list_tokens)\n",
    "                else:\n",
    "                    res2 = \" \".join(res2)\n",
    "            else:\n",
    "                res2 = res2[0]\n",
    "                \n",
    "            res = res1 + \", \" + res2\n",
    "            if tok in list_dict:\n",
    "                update_dict(list_dict[list_tokens[2:].index(tok)],res)    \n",
    "    return list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically create directories for saving pickles\n",
    "def create_dir(mode):\n",
    "    target_dir = 'saved_pickles/re-training/' + mode\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), target_dir)):\n",
    "        sub_dir = target_dir.split(\"/\")\n",
    "        k = os.getcwd()\n",
    "        for dir_loc in sub_dir:\n",
    "            k = os.path.join(k, dir_loc)\n",
    "            if not os.path.exists(str(k)):\n",
    "                os.mkdir(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_data(mode, noun_dict1, noun_dict2, noun_error1, noun_error2, unique_input1_set, unique_input2_set, unique_input_pair_set, unique_input1_error_set,\\\n",
    "            unique_input2_error_set, pred_err_count, fairness_err_count, unique_pred_input1_error_set, \\\n",
    "            unique_fairness_input1_error_set, retrain_dict, subj_person_male_count ,subj_person_female_count, emotional_state_count, \\\n",
    "                emotional_situation_count,verb_feel_count, verb1_count, neutral_subj_1_count, \\\n",
    "                neutral_subj_2_count, subj_person_male_pred_error ,subj_person_female_pred_error, emotional_state_pred_error, \\\n",
    "                emotional_situation_pred_error,verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, \\\n",
    "                neutral_subj_2_pred_error, subj_person_male_fairness_error ,subj_person_female_fairness_error, emotional_state_fairness_error, \\\n",
    "                emotional_situation_fairness_error,verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, \\\n",
    "                neutral_subj_2_fairness_error, bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error):\n",
    "\n",
    "    create_dir(mode)\n",
    "    \n",
    "    noun_data_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2]\n",
    "    noun_data_name = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\"] \n",
    "    \n",
    "    print(\"noun_dict1: \", noun_dict1)\n",
    "    print(\"noun_dict2: \", noun_dict2)\n",
    "    print(\"noun_error1: \", noun_error1)\n",
    "    print(\"noun_error2: \", noun_error2)\n",
    "    \n",
    "    assert len(noun_data_name) == len(noun_data_vals), \"ERROR: bug in variables names for stored inputs\"\n",
    "    for i in range(0, len(noun_data_vals)):\n",
    "        store = noun_data_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(noun_data_vals[i], handle)\n",
    "            \n",
    "            \n",
    "    data_vals_name = [\"unique_input1_set\", \"unique_input2_set\", \"unique_input_pair_set\", \"unique_input1_error_set\",\\\n",
    "            \"unique_input2_error_set\", \"pred_err_count\", \"fairness_err_count\", \"unique_pred_input1_error_set\", \\\n",
    "            \"unique_fairness_input1_error_set\", \"retrain_dict\"]\n",
    "\n",
    "    \n",
    "#     print(\"subj_person_male_fairness_error: \", list(subj_person_male_fairness_error)[0:4])\n",
    "    \n",
    "    data_vals = [unique_input1_set, unique_input2_set, unique_input_pair_set, unique_input1_error_set,\\\n",
    "            unique_input2_error_set, pred_err_count, fairness_err_count, unique_pred_input1_error_set, \\\n",
    "            unique_fairness_input1_error_set, retrain_dict]\n",
    "    \n",
    "    token_count_vals_name = [\"subj_person_male_count\" ,\"subj_person_female_count\", \"emotional_state_count\", \\\n",
    "                \"emotional_situation_count\",\"verb_feel_count\", \"verb1_count\", \"neutral_subj_1_count\", \\\n",
    "                \"neutral_subj_2_count\"] \n",
    "\n",
    "    token_count_vals = [subj_person_male_count ,subj_person_female_count, emotional_state_count, \\\n",
    "                emotional_situation_count,verb_feel_count, verb1_count, neutral_subj_1_count, \\\n",
    "                neutral_subj_2_count] \n",
    "    \n",
    "    pred_errors_count_vals_name = [\"subj_person_male_pred_error\" ,\"subj_person_female_pred_error\", \"emotional_state_pred_error\", \\\n",
    "                \"emotional_situation_pred_error\",\"verb_feel_pred_error\", \"verb1_pred_error\", \"neutral_subj_1_pred_error\", \\\n",
    "                \"neutral_subj_2_pred_error\"] \n",
    "\n",
    "    pred_errors_count_vals = [subj_person_male_pred_error ,subj_person_female_pred_error, emotional_state_pred_error, \\\n",
    "                emotional_situation_pred_error,verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, \\\n",
    "                neutral_subj_2_pred_error] \n",
    "    \n",
    "    fairness_error_count_vals_name = [\"subj_person_male_fairness_error\" ,\"subj_person_female_fairness_error\", \"emotional_state_fairness_error\", \\\n",
    "                \"emotional_situation_fairness_error\",\"verb_feel_fairness_error\", \"verb1_fairness_error\", \"neutral_subj_1_fairness_error\", \\\n",
    "                \"neutral_subj_2_fairness_error\"] \n",
    "\n",
    "    fairness_error_count_vals = [subj_person_male_fairness_error ,subj_person_female_fairness_error, emotional_state_fairness_error, \\\n",
    "                emotional_situation_fairness_error,verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, \\\n",
    "                neutral_subj_2_fairness_error] \n",
    "    \n",
    "    bias_count_vals_name = [\"bias_pair_count\" ,\"bias_pair_pred_error\", \"bias_pair_fairness_error\"] \n",
    "\n",
    "    bias_count_vals = [bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error] \n",
    "    \n",
    "    assert len(data_vals_name) == len(data_vals), \"ERROR: bug in variables names for stored inputs\"\n",
    "    for i in range(0, len(data_vals)):\n",
    "        store = data_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(data_vals[i], handle)\n",
    "    \n",
    "    \n",
    "    assert len(token_count_vals_name) == len(token_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs\".format(token_count_vals_name)\n",
    "    for i in range(0, len(token_count_vals)):\n",
    "        store = token_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(token_count_vals[i], handle)\n",
    "            \n",
    "    assert len(pred_errors_count_vals_name) == len(pred_errors_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs\".format(fairness_error_count_vals_name)\n",
    "    for i in range(0, len(pred_errors_count_vals)):\n",
    "        store = pred_errors_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(pred_errors_count_vals[i], handle)\n",
    "            \n",
    "            \n",
    "    assert len(fairness_error_count_vals_name) == len(fairness_error_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs {}, {}\".format(fairness_error_count_vals_name)\n",
    "    for i in range(0, len(fairness_error_count_vals)):\n",
    "        store = fairness_error_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(fairness_error_count_vals[i], handle)\n",
    "    \n",
    "    \n",
    "    assert len(bias_count_vals_name) == len(bias_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs {}, {}\".format(bias_count_vals_name)\n",
    "    for i in range(0, len(bias_count_vals)):\n",
    "        store = bias_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(bias_count_vals[i], handle)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new inputs for Dataset (train, val and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2):\n",
    "\n",
    "#     unique_input1_set = set()\n",
    "#     unique_input2_set = set()\n",
    "#     unique_input_pair_set = set()\n",
    "\n",
    "#     unique_input1_error_set = set()\n",
    "#     unique_input2_error_set = set()\n",
    "\n",
    "#     pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "#     unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "#     retrain_dict = dict()\n",
    "\n",
    "#     subj_person_male_count, subj_person_female_count, emotional_state_count, emotional_situation_count = {}, {}, {}, {}\n",
    "#     verb_feel_count, verb1_count, neutral_subj_1_count, neutral_subj_2_count= {}, {}, {}, {}\n",
    "\n",
    "#     subj_person_male_pred_error, subj_person_female_pred_error, emotional_state_pred_error, emotional_situation_pred_error = {}, {}, {}, {}\n",
    "#     verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, neutral_subj_2_pred_error= {}, {}, {}, {}\n",
    "\n",
    "#     subj_person_male_fairness_error, subj_person_female_fairness_error, emotional_state_fairness_error, emotional_situation_fairness_error = {}, {}, {}, {}\n",
    "#     verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, neutral_subj_2_fairness_error= {}, {}, {}, {}\n",
    "\n",
    "\n",
    "\n",
    "#     count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "#     pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "#     fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "#     #bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error = {}, {}, {}\n",
    "\n",
    "#     bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "#     bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "#     bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "    \n",
    "#     noun_dict1, noun_dict2, noun_error1, noun_error2, = {}, {}, {}, {}\n",
    "    \n",
    "    tokens = []\n",
    "\n",
    "    inputs = {}\n",
    "    label1, label2 =  None, None\n",
    "    tmp1, tmp2 = 0, 0\n",
    "    \n",
    "    noun1_probability = get_probability_dict(noun_error1, noun_dict1)\n",
    "    noun2_probability = get_probability_dict(noun_error2, noun_dict2)\n",
    "    \n",
    "    for i in range(ITERS): \n",
    "\n",
    "        tokens = select_tokens_probabilistically(noun_choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability)\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2        \n",
    "\n",
    "        input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "        \n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "        \n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "\n",
    "        input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "\n",
    "        if (len(inputs) == tmp1 == tmp2) or (len(inputs) >= max_input_gen_threshold):\n",
    "            print(\"Maximum input generation threshold reached, {} unique inputs generated\".format(len(inputs)))\n",
    "            break\n",
    "\n",
    "        if ITERS%2 == 0:\n",
    "            tmp1 = len(inputs)\n",
    "        else:\n",
    "            tmp2 = len(inputs)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Generate Data for Direct Gender Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  0 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/direct-gender-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'this woman': 2773, 'my girlfriend': 2805, 'this girl': 3118, 'my daughter': 3269, 'She': 2834, 'my mother': 2971, 'my sister': 3011, 'my aunt': 3943, 'my wife': 3162, 'her': 1117}\n",
      "noun_dict2 :  {'this man': 2812, 'my boyfriend': 2793, 'this boy': 3112, 'my son': 3270, 'He': 2846, 'my father': 2924, 'my brother': 3106, 'my uncle': 3871, 'my husband': 3142, 'him': 645}\n",
      "noun_error1 :  {'this girl': 297, 'my girlfriend': 212, 'my daughter': 887, 'my sister': 293, 'my aunt': 1052, 'this woman': 263, 'my mother': 433, 'She': 138, 'my wife': 405, 'her': 101}\n",
      "noun_error2 :  {'this boy': 294, 'my boyfriend': 192, 'my son': 789, 'my brother': 382, 'my uncle': 957, 'my father': 390, 'He': 155, 'my husband': 371, 'this man': 255, 'him': 69}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'this girl': 297, 'my girlfriend': 212, 'my daughter': 887, 'my sister': 293, 'my aunt': 1052, 'this woman': 263, 'my mother': 433, 'She': 138, 'my wife': 405}\n",
      "noun_dict1: {'this woman': 2773, 'my girlfriend': 2805, 'this girl': 3118, 'my daughter': 3269, 'She': 2834, 'my mother': 2971, 'my sister': 3011, 'my aunt': 3943, 'my wife': 3162}\n",
      "noun_error2: {'this boy': 294, 'my boyfriend': 192, 'my son': 789, 'my brother': 382, 'my uncle': 957, 'my father': 390, 'He': 155, 'my husband': 371, 'this man': 255}\n",
      "noun_dict2: {'this man': 2812, 'my boyfriend': 2793, 'this boy': 3112, 'my son': 3270, 'He': 2846, 'my father': 2924, 'my brother': 3106, 'my uncle': 3871, 'my husband': 3142}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate new inputs probabilistically from \n",
    "# new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3000 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(itertools.islice(new_train_data.items(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Generate Data for Random Gender Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  1 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/random-gender-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'my wife': 892, 'my girlfriend': 1010, 'this girl': 550, 'my mother': 985, 'my daughter': 924, 'this woman': 615, 'my mom': 507, 'my aunt': 1129, 'She': 695, 'my sister': 671, 'her': 1117}\n",
      "noun_dict2 :  {'this boy': 852, 'my father': 821, 'my uncle': 745, 'my husband': 733, 'He': 938, 'my boyfriend': 784, 'my brother': 925, 'my son': 605, 'my dad': 681, 'this man': 840, 'him': 677}\n",
      "noun_error1 :  {'my wife': 172, 'my girlfriend': 233, 'my mother': 143, 'this girl': 66, 'my daughter': 197, 'my aunt': 293, 'my sister': 53, 'She': 88, 'my mom': 24, 'this woman': 68, 'her': 99}\n",
      "noun_error2 :  {'this boy': 127, 'my father': 141, 'my uncle': 119, 'my husband': 119, 'my son': 77, 'my boyfriend': 123, 'He': 192, 'my brother': 148, 'my dad': 104, 'this man': 114, 'him': 82}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'my wife': 172, 'my girlfriend': 233, 'my mother': 143, 'this girl': 66, 'my daughter': 197, 'my aunt': 293, 'my sister': 53, 'She': 88, 'my mom': 24, 'this woman': 68}\n",
      "noun_dict1: {'my wife': 892, 'my girlfriend': 1010, 'this girl': 550, 'my mother': 985, 'my daughter': 924, 'this woman': 615, 'my mom': 507, 'my aunt': 1129, 'She': 695, 'my sister': 671}\n",
      "noun_error2: {'this boy': 127, 'my father': 141, 'my uncle': 119, 'my husband': 119, 'my son': 77, 'my boyfriend': 123, 'He': 192, 'my brother': 148, 'my dad': 104, 'this man': 114}\n",
      "noun_dict2: {'this boy': 852, 'my father': 821, 'my uncle': 745, 'my husband': 733, 'He': 938, 'my boyfriend': 784, 'my brother': 925, 'my son': 605, 'my dad': 681, 'this man': 840}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "\n",
    "\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3000 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3485"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(itertools.islice(new_train_data.items(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Generate Data for Random Gender Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  2 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/gender-occupation-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'the teacher': 517, 'the librarian': 235, 'the writer': 522, 'the attendant': 320, 'the receptionist': 195, 'the the nurse': 333, 'the the paralegal': 341, 'the clerk': 286, 'the designer': 298, 'the cashier': 303, 'the editor': 353, 'the the librarian': 229, 'the baker': 273, 'the auditor': 329, 'the tailor': 333, 'the counselor': 392, 'the cleaner': 424, 'the assistant': 269, 'the housekeeper': 279, 'the hairdresser': 279, 'the secretary': 183, 'the nurse': 377, 'her': 774}\n",
      "noun_dict2 :  {'the mover': 167, 'the engineer': 401, 'the carpenter': 357, 'the farmer': 289, 'the construction worker': 266, 'the salesperson': 462, 'the accountant': 330, 'the manager': 256, 'the lawyer': 325, 'the cook': 384, 'the sheriff': 188, 'the mechanic': 410, 'the janitor': 279, 'the analyst': 507, 'the chief': 304, 'the guard': 231, 'the physician': 258, 'the driver': 230, 'the CEO': 285, 'the technician': 351, 'the developer': 270, 'the supervisor': 274, 'the laborer': 233, 'him': 583}\n",
      "noun_error1 :  {'the attendant': 40, 'the clerk': 56, 'the writer': 136, 'the designer': 32, 'the the paralegal': 68, 'the teacher': 106, 'the editor': 63, 'the librarian': 39, 'the cleaner': 98, 'the the nurse': 56, 'the receptionist': 27, 'the baker': 35, 'the counselor': 74, 'the auditor': 80, 'the housekeeper': 46, 'the assistant': 27, 'the tailor': 63, 'the nurse': 73, 'the the librarian': 20, 'the hairdresser': 35, 'the cashier': 47, 'the secretary': 20, 'her': 92}\n",
      "noun_error2 :  {'the farmer': 58, 'the accountant': 49, 'the carpenter': 40, 'the cook': 127, 'the mechanic': 77, 'the construction worker': 42, 'the salesperson': 63, 'the engineer': 80, 'the chief': 57, 'the analyst': 126, 'the driver': 29, 'the CEO': 33, 'the janitor': 36, 'the technician': 70, 'the mover': 19, 'the lawyer': 54, 'the developer': 50, 'the physician': 34, 'the guard': 35, 'the sheriff': 31, 'the manager': 32, 'the supervisor': 67, 'the laborer': 23, 'him': 53}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'the attendant': 40, 'the clerk': 56, 'the writer': 136, 'the designer': 32, 'the the paralegal': 68, 'the teacher': 106, 'the editor': 63, 'the librarian': 39, 'the cleaner': 98, 'the the nurse': 56, 'the receptionist': 27, 'the baker': 35, 'the counselor': 74, 'the auditor': 80, 'the housekeeper': 46, 'the assistant': 27, 'the tailor': 63, 'the nurse': 73, 'the the librarian': 20, 'the hairdresser': 35, 'the cashier': 47, 'the secretary': 20}\n",
      "noun_dict1: {'the teacher': 517, 'the librarian': 235, 'the writer': 522, 'the attendant': 320, 'the receptionist': 195, 'the the nurse': 333, 'the the paralegal': 341, 'the clerk': 286, 'the designer': 298, 'the cashier': 303, 'the editor': 353, 'the the librarian': 229, 'the baker': 273, 'the auditor': 329, 'the tailor': 333, 'the counselor': 392, 'the cleaner': 424, 'the assistant': 269, 'the housekeeper': 279, 'the hairdresser': 279, 'the secretary': 183, 'the nurse': 377}\n",
      "noun_error2: {'the farmer': 58, 'the accountant': 49, 'the carpenter': 40, 'the cook': 127, 'the mechanic': 77, 'the construction worker': 42, 'the salesperson': 63, 'the engineer': 80, 'the chief': 57, 'the analyst': 126, 'the driver': 29, 'the CEO': 33, 'the janitor': 36, 'the technician': 70, 'the mover': 19, 'the lawyer': 54, 'the developer': 50, 'the physician': 34, 'the guard': 35, 'the sheriff': 31, 'the manager': 32, 'the supervisor': 67, 'the laborer': 23}\n",
      "noun_dict2: {'the mover': 167, 'the engineer': 401, 'the carpenter': 357, 'the farmer': 289, 'the construction worker': 266, 'the salesperson': 462, 'the accountant': 330, 'the manager': 256, 'the lawyer': 325, 'the cook': 384, 'the sheriff': 188, 'the mechanic': 410, 'the janitor': 279, 'the analyst': 507, 'the chief': 304, 'the guard': 231, 'the physician': 258, 'the driver': 230, 'the CEO': 285, 'the technician': 351, 'the developer': 270, 'the supervisor': 274, 'the laborer': 233}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "\n",
    "\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3000 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6367"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(itertools.islice(new_train_data.items(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Generate Data for Indirect Gender Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  3 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/gender-name-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'Donna': 183, 'Michelle': 237, 'Sarah': 242, 'Dorothy ': 257, 'Betty': 208, 'Deborah': 357, 'Emily': 189, 'Kimberly': 267, 'Patricia': 172, 'Carol': 361, 'Laura': 261, 'Margaret': 205, 'Jennifer': 219, 'Jessica': 190, 'Sandra': 279, 'Cynthia': 234, 'Sharon': 336, 'Nancy': 231, 'Susan': 202, 'Linda': 218, 'Karen': 320, 'Melissa': 106, 'Rebecca': 196, 'Amanda': 181, 'Barbara': 204, 'Elizabeth': 205, 'Ashley': 138, 'Stephanie': 199, 'Mary': 227, 'Lisa': 248, 'her': 573}\n",
      "noun_dict2 :  {'Paul': 220, 'Christopher': 302, 'Joseph': 216, 'Kenneth': 262, 'Michael ': 180, 'Charles': 168, 'Brian': 177, 'Mark': 303, 'Thomas': 320, 'Andrew': 176, 'Kevin': 250, 'David ': 140, 'John ': 253, 'Edward': 276, 'Jason': 191, 'James': 279, 'Robert ': 263, 'Daniel': 228, 'Ryan': 217, 'Joshua': 271, 'Richard': 222, 'Anthony': 256, 'Jeffrey': 210, 'William ': 282, 'Timothy': 232, 'Steven': 263, 'Donald': 147, 'George': 261, 'Ronald': 139, 'Matthew': 185, 'him': 566}\n",
      "noun_error1 :  {'Michelle': 76, 'Sarah': 40, 'Betty': 31, 'Deborah': 102, 'Kimberly': 90, 'Patricia': 27, 'Carol': 86, 'Donna': 30, 'Margaret': 52, 'Sandra': 67, 'Sharon': 93, 'Nancy': 55, 'Susan': 21, 'Karen': 75, 'Linda': 19, 'Melissa': 5, 'Rebecca': 18, 'Jennifer': 29, 'Dorothy ': 37, 'Cynthia': 26, 'Emily': 35, 'Amanda': 34, 'Barbara': 54, 'Elizabeth': 40, 'Laura': 59, 'Ashley': 9, 'Mary': 27, 'Lisa': 48, 'Stephanie': 39, 'Jessica': 23, 'her': 89}\n",
      "noun_error2 :  {'Christopher': 122, 'Joseph': 35, 'Kenneth': 52, 'Michael ': 22, 'Charles': 23, 'Thomas': 80, 'Andrew': 25, 'Kevin': 40, 'David ': 8, 'John ': 44, 'Edward': 63, 'Jason': 40, 'Robert ': 64, 'Daniel': 29, 'Ryan': 39, 'Joshua': 89, 'Richard': 37, 'William ': 62, 'Mark': 65, 'James': 68, 'Steven': 37, 'Timothy': 66, 'Donald': 15, 'George': 64, 'Jeffrey': 26, 'Paul': 31, 'Matthew': 40, 'Brian': 20, 'Ronald': 20, 'Anthony': 63, 'him': 54}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'Michelle': 76, 'Sarah': 40, 'Betty': 31, 'Deborah': 102, 'Kimberly': 90, 'Patricia': 27, 'Carol': 86, 'Donna': 30, 'Margaret': 52, 'Sandra': 67, 'Sharon': 93, 'Nancy': 55, 'Susan': 21, 'Karen': 75, 'Linda': 19, 'Melissa': 5, 'Rebecca': 18, 'Jennifer': 29, 'Dorothy ': 37, 'Cynthia': 26, 'Emily': 35, 'Amanda': 34, 'Barbara': 54, 'Elizabeth': 40, 'Laura': 59, 'Ashley': 9, 'Mary': 27, 'Lisa': 48, 'Stephanie': 39, 'Jessica': 23}\n",
      "noun_dict1: {'Donna': 183, 'Michelle': 237, 'Sarah': 242, 'Dorothy ': 257, 'Betty': 208, 'Deborah': 357, 'Emily': 189, 'Kimberly': 267, 'Patricia': 172, 'Carol': 361, 'Laura': 261, 'Margaret': 205, 'Jennifer': 219, 'Jessica': 190, 'Sandra': 279, 'Cynthia': 234, 'Sharon': 336, 'Nancy': 231, 'Susan': 202, 'Linda': 218, 'Karen': 320, 'Melissa': 106, 'Rebecca': 196, 'Amanda': 181, 'Barbara': 204, 'Elizabeth': 205, 'Ashley': 138, 'Stephanie': 199, 'Mary': 227, 'Lisa': 248}\n",
      "noun_error2: {'Christopher': 122, 'Joseph': 35, 'Kenneth': 52, 'Michael ': 22, 'Charles': 23, 'Thomas': 80, 'Andrew': 25, 'Kevin': 40, 'David ': 8, 'John ': 44, 'Edward': 63, 'Jason': 40, 'Robert ': 64, 'Daniel': 29, 'Ryan': 39, 'Joshua': 89, 'Richard': 37, 'William ': 62, 'Mark': 65, 'James': 68, 'Steven': 37, 'Timothy': 66, 'Donald': 15, 'George': 64, 'Jeffrey': 26, 'Paul': 31, 'Matthew': 40, 'Brian': 20, 'Ronald': 20, 'Anthony': 63}\n",
      "noun_dict2: {'Paul': 220, 'Christopher': 302, 'Joseph': 216, 'Kenneth': 262, 'Michael ': 180, 'Charles': 168, 'Brian': 177, 'Mark': 303, 'Thomas': 320, 'Andrew': 176, 'Kevin': 250, 'David ': 140, 'John ': 253, 'Edward': 276, 'Jason': 191, 'James': 279, 'Robert ': 263, 'Daniel': 228, 'Ryan': 217, 'Joshua': 271, 'Richard': 222, 'Anthony': 256, 'Jeffrey': 210, 'William ': 282, 'Timothy': 232, 'Steven': 263, 'Donald': 147, 'George': 261, 'Ronald': 139, 'Matthew': 185}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "\n",
    "\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3001 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9249"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(itertools.islice(new_train_data.items(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Generate Data for Indirect Racial Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  5 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/racial-name-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'Nancy': 407, 'Ryan': 305, 'Justin': 419, 'Adam': 395, 'Stephanie': 450, 'Andrew': 257, 'Kristin': 277, 'Katie': 396, 'Alan': 352, 'Melanie': 449, 'Amanda': 356, 'Betsy': 206, 'Jack': 364, 'Josh': 560, 'Roger': 276, 'Ellen': 370, 'Heather': 321, 'Courtney': 319, 'Harry': 363, 'Frank': 246, 'her': 742}\n",
      "noun_dict2 :  {'Alphonse': 346, 'Jerome': 397, 'Jamel': 412, 'Jasmine': 375, 'Lakisha': 358, 'Tia': 296, 'Shereen': 396, 'Nichelle': 381, 'Ebony': 212, 'Terrence': 364, 'Tanisha': 305, 'Shaniqua': 305, 'Malik': 484, 'Lamar': 438, 'Torrance': 225, 'Leroy': 396, 'Darnell': 357, 'Latisha': 396, 'Latoya': 258, 'Alonzo': 367, 'him': 582}\n",
      "noun_error1 :  {'Ryan': 36, 'Justin': 91, 'Adam': 42, 'Stephanie': 86, 'Alan': 50, 'Melanie': 93, 'Amanda': 56, 'Betsy': 8, 'Jack': 78, 'Andrew': 37, 'Josh': 203, 'Nancy': 65, 'Heather': 42, 'Roger': 47, 'Courtney': 47, 'Harry': 81, 'Ellen': 56, 'Kristin': 46, 'Katie': 71, 'Frank': 32, 'her': 70}\n",
      "noun_error2 :  {'Jerome': 62, 'Jamel': 74, 'Lakisha': 50, 'Nichelle': 71, 'Jasmine': 78, 'Ebony': 22, 'Shereen': 65, 'Terrence': 57, 'Tia': 46, 'Alphonse': 58, 'Shaniqua': 43, 'Malik': 129, 'Lamar': 98, 'Leroy': 68, 'Tanisha': 48, 'Latisha': 70, 'Torrance': 23, 'Darnell': 69, 'Alonzo': 66, 'Latoya': 34, 'him': 70}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'Ryan': 36, 'Justin': 91, 'Adam': 42, 'Stephanie': 86, 'Alan': 50, 'Melanie': 93, 'Amanda': 56, 'Betsy': 8, 'Jack': 78, 'Andrew': 37, 'Josh': 203, 'Nancy': 65, 'Heather': 42, 'Roger': 47, 'Courtney': 47, 'Harry': 81, 'Ellen': 56, 'Kristin': 46, 'Katie': 71, 'Frank': 32}\n",
      "noun_dict1: {'Nancy': 407, 'Ryan': 305, 'Justin': 419, 'Adam': 395, 'Stephanie': 450, 'Andrew': 257, 'Kristin': 277, 'Katie': 396, 'Alan': 352, 'Melanie': 449, 'Amanda': 356, 'Betsy': 206, 'Jack': 364, 'Josh': 560, 'Roger': 276, 'Ellen': 370, 'Heather': 321, 'Courtney': 319, 'Harry': 363, 'Frank': 246}\n",
      "noun_error2: {'Jerome': 62, 'Jamel': 74, 'Lakisha': 50, 'Nichelle': 71, 'Jasmine': 78, 'Ebony': 22, 'Shereen': 65, 'Terrence': 57, 'Tia': 46, 'Alphonse': 58, 'Shaniqua': 43, 'Malik': 129, 'Lamar': 98, 'Leroy': 68, 'Tanisha': 48, 'Latisha': 70, 'Torrance': 23, 'Darnell': 69, 'Alonzo': 66, 'Latoya': 34}\n",
      "noun_dict2: {'Alphonse': 346, 'Jerome': 397, 'Jamel': 412, 'Jasmine': 375, 'Lakisha': 358, 'Tia': 296, 'Shereen': 396, 'Nichelle': 381, 'Ebony': 212, 'Terrence': 364, 'Tanisha': 305, 'Shaniqua': 305, 'Malik': 484, 'Lamar': 438, 'Torrance': 225, 'Leroy': 396, 'Darnell': 357, 'Latisha': 396, 'Latoya': 258, 'Alonzo': 367}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "\n",
    "\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3005 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12060"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically create directories for saving pickles\n",
    "def create_dir(mode):\n",
    "    target_dir = 'saved_pickles/re-training/' + mode\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), target_dir)):\n",
    "        sub_dir = target_dir.split(\"/\")\n",
    "        k = os.getcwd()\n",
    "        for dir_loc in sub_dir:\n",
    "            k = os.path.join(k, dir_loc)\n",
    "            if not os.path.exists(str(k)):\n",
    "                os.mkdir(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"hub/new_training_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = \"new_train_data\" + \".pickle\"\n",
    "with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "    pickle.dump(new_train_data, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "del noun_error2\n",
    "del noun_dict2\n",
    "del noun_dict1\n",
    "del noun_error1\n",
    "del c_vals\n",
    "del pred_err_count\n",
    "del fairness_err_count\n",
    "del unique_pred_input1_error_set\n",
    "del unique_fairness_input1_error_set\n",
    "del retrain_dict\n",
    "del count_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pred_error_dict\n",
    "del fairness_error_dict\n",
    "del bias_pair_count\n",
    "del bias_pair_pred_error\n",
    "del bias_pair_fairness_error\n",
    "# del pred_err_count\n",
    "# del fairness_err_count\n",
    "# del unique_pred_input1_error_set\n",
    "# del unique_fairness_input1_error_set\n",
    "# del retrain_dict\n",
    "# del count_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Download the IMDB dataset\n",
    "\n",
    "The IMDB dataset is available on [imdb reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) or on [TensorFlow datasets](https://www.tensorflow.org/datasets). The following code downloads the IMDB dataset to your machine (or the colab runtime):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_data, validation_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment IMDB Training Data with new generated test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dict_randomly(new_train_data, size):\n",
    "    res_dict = {}\n",
    "    keys = random.sample(list(new_train_data), size)\n",
    "#     values = [new_train_data[k] for k in keys]\n",
    "    for i in keys:\n",
    "        res_dict[i] = new_train_data[i]\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Emily feel miserable.': 0, 'Terrence found himself in a/an heartbreaking situation.': 0, 'Christopher was happy.': 1, 'Deborah feels devastated.': 0, 'Donna was glad.': 1}\n"
     ]
    }
   ],
   "source": [
    "print(slice_dict_randomly(new_train_data, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent = [0.1 * len(train_data), 0.2 * len(train_data), 0.3 * len(train_data)]\n",
    "percent = [0.1 * length, 0.2 * length, 0.3 * length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1500.0, 3000.0, 4500.0]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataset for ten_percent_dataset, twenty_percent_dataset and thirty_percent_dataset\n",
    "#slice from new train data, with size 10, 20 and 30 % of original train data \n",
    "ten_percent_dataset = slice_dict_randomly(new_train_data, int(percent[0]))\n",
    "twenty_percent_dataset = slice_dict_randomly(new_train_data, int(percent[1]))\n",
    "thirty_percent_dataset = slice_dict_randomly(new_train_data, int(percent[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_data(percent_dataset, train_data):\n",
    "#     res_data = train_data.prefetch(1)\n",
    "#     print(\"Init res_data: \", len(res_data))\n",
    "#     print(\"Type res_data: \", type(res_data))\n",
    "#     res_data.clear()\n",
    "#     prefetched_tensor_tuple = tuple()\n",
    "    c = 0\n",
    "    for i in percent_dataset:\n",
    "        c+= 1\n",
    "        if (c % 2) == 0:\n",
    "            gc.collect()\n",
    "        label_tmp = tf.constant([percent_dataset[i]]).numpy()[0]\n",
    "        data_tmp = tf.constant([i]).numpy()[0]\n",
    "        label = tf.convert_to_tensor(label_tmp, np.int64)\n",
    "        data = tf.convert_to_tensor(data_tmp)\n",
    "        data_point = (data, label)\n",
    "        tensor_data_point = tf.data.Dataset.from_tensors(data_point)\n",
    "#         prefetched_tensor = tensor_data_point.prefetch(len(tensor_data_point))\n",
    "        prefetched_tensor = tensor_data_point.prefetch(1)\n",
    "#         prefetched_tensor_tuple\n",
    "        train_data = train_data.concatenate(prefetched_tensor) \n",
    "        ops.reset_default_graph()\n",
    "#     print(\"Final res_data: \", len(res_data))\n",
    "#     print(\"Type res_data: \", type(res_data))\n",
    "#         tf.reset_default_graph #()\n",
    "#     tf.reset_default_graph #()\n",
    "    ops.reset_default_graph()\n",
    "    return train_data #.concatenate(res_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_percent_additional_train_dataset = add_new_data(ten_percent_dataset, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twenty_percent_additional_train_dataset = add_new_data(twenty_percent_dataset, train_data)\n",
    "# thirty_percent_additional_train_dataset = add_new_data(thirty_percent_dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_data)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ten_percent_additional_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph #.reset_default_graph()\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(twenty_percent_additional_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(thirty_percent_additional_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_at_thresholds(dataset, length):\n",
    "    t1 = 15000 #int(percent[0])\n",
    "    t2 = length #len(dataset)\n",
    "    print(\"len: \", t2)\n",
    "    print(\" \" * 30)\n",
    "    c = 0\n",
    "    for i in dataset:\n",
    "        if c < 2:\n",
    "            print(\"data @ {} is {}\".format(c, i))\n",
    "            print(\" \" * 30)\n",
    "        if (c == t1) or (c == (t1-1)):\n",
    "            print(\"data @ {} is {}\".format(c, i))\n",
    "            print(\" \" * 30)\n",
    "        if (c == (t2-1)) or (c == (t2-2)):\n",
    "            print(\"data @ {} is {}\".format(c, i))\n",
    "            print(\" \" * 30)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# length = 16500\n",
    "# print_at_thresholds(ten_percent_additional_train_dataset, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = 16500\n",
    "# print_at_thresholds(ten_percent_additional_train_dataset, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = 18000\n",
    "# print_at_thresholds(ten_percent_additional_train_dataset, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = 19501\n",
    "# print_at_thresholds(ten_percent_additional_train_dataset, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_int_feature(value):\n",
    "#   return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "# with tf.io.TFRecordWriter(\"gs://my_bucket/data.tfrecord\") as writer:\n",
    "#     for sample in ten_percent_additional_train_dataset:\n",
    "#         feature = {\n",
    "#             'input_ids': create_int_feature(sample['input_ids']),\n",
    "#             'attention_mask': create_int_feature(sample['attention_mask']),\n",
    "#             'decoder_input_ids': create_int_feature(sample['decoder_input_ids']),\n",
    "#             'lm_labels': create_int_feature(sample['lm_labels'])\n",
    "#         }\n",
    "#         gc.collect()\n",
    "#         example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "#         writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.map(tf.io.serialize_tensor)\n",
    "writer = tf.data.experimental.TFRecordWriter(\"file.tfrecord\")\n",
    "writer.write(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in converted code:\n    relative to /opt/anaconda3/lib/python3.7:\n\n    site-packages/tensorflow_core/python/ops/gen_parsing_ops.py:1732 serialize_tensor\n        \"SerializeTensor\", tensor=tensor, name=name)\n    site-packages/tensorflow_core/python/framework/op_def_library.py:410 _apply_op_helper\n        with g.as_default(), ops.name_scope(name) as scope:\n    site-packages/tensorflow_core/python/framework/ops.py:6349 __enter__\n        return self._name_scope.__enter__()\n    contextlib.py:112 __enter__\n        return next(self.gen)\n    site-packages/tensorflow_core/python/framework/ops.py:4118 name_scope\n        if name:\n    site-packages/tensorflow_core/python/framework/ops.py:765 __bool__\n        self._disallow_bool_casting()\n    site-packages/tensorflow_core/python/framework/ops.py:528 _disallow_bool_casting\n        \"using a `tf.Tensor` as a Python `bool`\")\n    site-packages/tensorflow_core/python/framework/ops.py:513 _disallow_when_autograph_disabled\n        \" Try decorating it directly with @tf.function.\".format(task))\n\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-05486c645bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mten_percent_additional_train_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file.tfrecord\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \"\"\"\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3414\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3415\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3416\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3417\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   3418\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m       \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 1854\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   1855\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2687\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   2688\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2689\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2632\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in converted code:\n    relative to /opt/anaconda3/lib/python3.7:\n\n    site-packages/tensorflow_core/python/ops/gen_parsing_ops.py:1732 serialize_tensor\n        \"SerializeTensor\", tensor=tensor, name=name)\n    site-packages/tensorflow_core/python/framework/op_def_library.py:410 _apply_op_helper\n        with g.as_default(), ops.name_scope(name) as scope:\n    site-packages/tensorflow_core/python/framework/ops.py:6349 __enter__\n        return self._name_scope.__enter__()\n    contextlib.py:112 __enter__\n        return next(self.gen)\n    site-packages/tensorflow_core/python/framework/ops.py:4118 name_scope\n        if name:\n    site-packages/tensorflow_core/python/framework/ops.py:765 __bool__\n        self._disallow_bool_casting()\n    site-packages/tensorflow_core/python/framework/ops.py:528 _disallow_bool_casting\n        \"using a `tf.Tensor` as a Python `bool`\")\n    site-packages/tensorflow_core/python/framework/ops.py:513 _disallow_when_autograph_disabled\n        \" Try decorating it directly with @tf.function.\".format(task))\n\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.\n"
     ]
    }
   ],
   "source": [
    "dataset = ten_percent_additional_train_dataset\n",
    "dataset = dataset.map(tf.io.serialize_tensor)\n",
    "writer = tf.data.experimental.TFRecordWriter(\"file.tfrecord\")\n",
    "writer.write(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapped_fn(_):\n",
    "    X = tf.random_uniform([3,3])\n",
    "    y = tf.random_uniform([3,1])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,y))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n\n    TypeError: tf__mapped_fn() takes 1 positional argument but 2 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-825fb942cbd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dataset = tf.data.Dataset.range(3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mflattened_ten_percent_additional_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mten_percent_additional_train_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1238\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \"\"\"\n\u001b[0;32m-> 1240\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   3484\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3485\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 3486\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   3487\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3488\u001b[0m       raise TypeError(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m       \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 1854\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   1855\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2687\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   2688\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2689\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2632\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n\n    TypeError: tf__mapped_fn() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "# dataset = tf.data.Dataset.range(3)\n",
    "flattened_ten_percent_additional_train_dataset = ten_percent_additional_train_dataset.flat_map(mapped_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flat_map() missing 1 required positional argument: 'map_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-04d47adb1ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dataset = tf.data.Dataset.range(3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mflattened_ten_percent_additional_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mten_percent_additional_train_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflattened_ten_percent_additional_train_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# map_fn(tf.io.serialize_tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file.tfrecord\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: flat_map() missing 1 required positional argument: 'map_func'"
     ]
    }
   ],
   "source": [
    "dataset = flattened_ten_percent_additional_train_dataset.map_fn(tf.io.serialize_tensor)\n",
    "# map_fn(tf.io.serialize_tensor)\n",
    "writer = tf.data.experimental.TFRecordWriter(\"file.tfrecord\")\n",
    "writer.write(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resource\n",
    "# import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_writer = tf.data.experimental.TFRecordWriter(\"filtered/onlyGuitar.tfrecord\")\n",
    "# tf_writer.write(ten_percent_additional_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save(dataset):\n",
    "#     location= os.getcwd()\n",
    "#     dataset = dataset.map(tf.io.serialize_tensor)\n",
    "#     writer = tf.data.experimental.TFRecordWriter(location)\n",
    "#     writer.write(dataset)\n",
    "#     return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(ten_percent_additional_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import tempfile\n",
    "# path = os.getcwd()\n",
    "# #/Users/ezekiel.soremekun/Documents/Coref-Fairness-Test-Generation/Ezekiel-Testbed/trained-sentiment-analyzers\" #os.path.join(tempfile.gettempdir(), \"saved_data\")\n",
    "# # Save a dataset\n",
    "# dataset = tf.data.Dataset.range(2)\n",
    "# tf.data.experimental.save(dataset, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write\n",
    "# a = tf.zeros((100, 512), tf.int32)\n",
    "# ds = tf.data.Dataset.from_tensor_slices((a, a, a, a[:, 0]))\n",
    "# for i, _ in enumerate(ds.element_spec):\n",
    "#     ds_i = ds.map(lambda *args: args[i]).map(tf.io.serialize_tensor)\n",
    "#     writer = tf.data.experimental.TFRecordWriter(f'mydata.{i}.tfrecord')\n",
    "#     writer.write(ds_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = tf.data.Dataset.from_tensor_slices((ten_percent_additional_train_dataset, ten_percent_additional_train_dataset, ten_percent_additional_train_dataset, ten_percent_additional_train_dataset[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i, _ in enumerate(ten_percent_additional_train_dataset.element_spec):\n",
    "#     ds_i = ten_percent_additional_train_dataset.map(lambda *args: args[i]).map(tf.io.serialize_tensor)\n",
    "#     writer = tf.data.experimental.TFRecordWriter(f'mydata.{i}.tfrecord')\n",
    "#     writer.write(ds_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(resource.getrlimit(resource.RLIMIT_STACK))\n",
    "# print(sys.getrecursionlimit())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_rec = 0x100000\n",
    "\n",
    "# # May segfault without this line. 0x100 is a guess at the size of each stack frame.\n",
    "# # resource.setrlimit(resource.RLIMIT_STACK, [0x100 * max_rec, resource.RLIM_INFINITY])\n",
    "# sys.setrecursionlimit(max_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ten_percent_additional_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf = h5py.File('train_dataset.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_numpy = tfds.as_numpy(ten_percent_additional_train_dataset)\n",
    "# next(ds_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf.create_dataset('ten_percent_data', data=ds_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_numpy = tfds.as_numpy(ten_percent_additional_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>> gen = (x for x in range(100))\n",
    "# >>>> next(gen)\n",
    "# 0\n",
    "# >>>> pickled = pickle.dumps(gen)\n",
    "# >>>> next(pickle.loads(pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store = \"ten_percent_additional_train_dataset\" + \".pickle\"\n",
    "# with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "#     next(ds_numpy)\n",
    "#     pickle.dump(ds_numpy, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store = \"twenty_percent_additional_train_dataset\" + \".pickle\"\n",
    "# with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "#     pickle.dump(twenty_percent_additional_train_dataset, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store = \"thirty_percent_additional_train_dataset\" + \".pickle\"\n",
    "# with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "#     pickle.dump(thirty_percent_additional_train_dataset, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_with_hub.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
