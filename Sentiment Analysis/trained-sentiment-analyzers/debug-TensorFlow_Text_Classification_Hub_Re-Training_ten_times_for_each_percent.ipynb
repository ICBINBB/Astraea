{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic4_occAAiAT"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ioaprt5q5US7"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "yCl0eTNH5RS3"
   },
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# Text classification with TensorFlow Hub: Movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKY4XMc9o8iB"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/text_classification_with_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "This notebook classifies movie reviews as *positive* or *negative* using the text of the review. This is an example of *binary*—or two-class—classification, an important and widely applicable kind of machine learning problem.\n",
    "\n",
    "The tutorial demonstrates the basic application of transfer learning with TensorFlow Hub and Keras.\n",
    "\n",
    "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews. \n",
    "\n",
    "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow Hub](https://www.tensorflow.org/hub), a library and platform for transfer learning. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in /opt/anaconda3/lib/python3.7/site-packages (1.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q tensorflow-hub\n",
    "!pip3 install -q tensorflow-datasets\n",
    "# !pip3 install -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.3.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.9.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema (based on EEC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "verb_list_p1 = ['feels', 'feel', 'made me feel', 'was'] \n",
    "verb_list_p2 = ['saw', 'found himself/herself in a', 'told us all about the recent', 'talked to', 'goes to'] \n",
    "\n",
    "verb_list = ['is' , 'was', 'talked to', 'goes to', 'feels', \\\n",
    "             'feel', 'made me feel', 'found himself/herself in', \\\n",
    "             'told us all about the recent']\n",
    "\t\t\t \n",
    "person_female = ['this woman','this girl','my sister','my daughter', 'my wife',\\\n",
    "                 'my girlfriend', 'my mother', 'my aunt', 'my mom']\n",
    "\t\t\t\t \n",
    "person_male = ['this man', 'this boy', 'my brother', 'my son', 'my husband', \\\n",
    "               'my boyfriend', 'my father', 'my uncle', 'my dad']\n",
    "\n",
    "print(set(person_male).intersection(set(person_female)))\n",
    "\n",
    "assert(len(set(person_male).intersection(set(person_female))) == 0), \\\n",
    "    \"ERROR not a disjoint set\"\n",
    "\t\t\t   \n",
    "subj_pronoun_female = [\"She\"]\n",
    "subj_pronoun_male = [\"He\"]\n",
    "\n",
    "obj_pronoun_female = [\"her\"]\n",
    "obj_pronoun_male = [\"him\"]\n",
    "\n",
    "occupations_male_biased = ['the supervisor','the janitor','the cook','the mover','the laborer','the construction worker','the chief','the developer','the carpenter','the manager','the lawyer','the farmer','the driver','the salesperson','the physician','the guard','the analyst','the mechanic','the sheriff','the CEO','the technician','the accountant','the engineer']\n",
    "\n",
    "occupations_female_biased = ['the cashier','the teacher','the nurse','the assistant','the secretary','the auditor','the cleaner','the receptionist','the clerk','the counselor','the designer','the hairdresser','the attendant','the writer','the housekeeper','the baker','the editor','the librarian','the tailor','the teacher','the the librarian','the the nurse','the the paralegal']\n",
    "\n",
    "print(set(occupations_male_biased).intersection(set(occupations_female_biased)))\n",
    "\n",
    "assert(len(set(occupations_male_biased).intersection(set(occupations_female_biased))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', \\\n",
    "                        'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', \\\n",
    "                       'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', \\\n",
    "                       'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', \\\n",
    "                     'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', \\\n",
    "                     'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', \\\n",
    "                     'Jason', 'Jeffrey', 'Ryan']\n",
    "\t\t\t\t\t \n",
    "print(set(female_biased_names).intersection(set(male_biased_names)))\n",
    "\n",
    "assert(len(set(female_biased_names).intersection(set(male_biased_names))) == 0), \"ERROR not a disjoint set\"\t\t\t\t\t \t\t\t\t\t \n",
    "\n",
    "#Data from EEC\n",
    "African_American_Female_Names = ['Ebony', 'Jasmine', 'Lakisha', 'Latisha', 'Latoya', 'Nichelle', 'Shaniqua', 'Shereen', 'Tanisha', 'Tia']\n",
    "African_American_Male_Names = ['Alonzo', 'Alphonse', 'Darnell', 'Jamel', 'Jerome', 'Lamar', 'Leroy', 'Malik', 'Terrence', 'Torrance']\n",
    "European_American_Female_Names = ['Amanda', 'Betsy', 'Courtney', 'Ellen', 'Heather', 'Katie', 'Kristin', 'Melanie', 'Nancy', 'Stephanie']\n",
    "European_American_Male_Names = ['Adam', 'Alan', 'Andrew', 'Frank', 'Harry', 'Jack', 'Josh', 'Justin', 'Roger', 'Ryan']\n",
    "\n",
    "\n",
    "gen_male_names = European_American_Male_Names + African_American_Male_Names\n",
    "gen_female_names = European_American_Female_Names + African_American_Female_Names\n",
    "\n",
    "print(set(gen_male_names).intersection(set(gen_female_names)))\n",
    "\n",
    "assert(len(set(gen_male_names).intersection(set(gen_female_names))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "\n",
    "african_american_names = African_American_Female_Names + African_American_Male_Names\n",
    "european_american_names = European_American_Female_Names + European_American_Male_Names\n",
    "\n",
    "print(set(african_american_names).intersection(set(european_american_names)))\n",
    "\n",
    "assert(len(set(african_american_names).intersection(set(european_american_names))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "subj_person_male_all = subj_pronoun_male + person_male # + occupations_male_biased\n",
    "subj_person_female_all = subj_pronoun_female + person_female # + occupations_female_biased\n",
    "\n",
    "print(set(subj_person_male_all).intersection(set(subj_person_female_all)))\n",
    "\n",
    "assert(len(set(subj_person_male_all).intersection(set(subj_person_female_all))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "obj_person_male = obj_pronoun_male + person_male\n",
    "obj_person_female = obj_pronoun_female + person_female\n",
    "\n",
    "print(set(obj_person_male).intersection(set(obj_person_female)))\n",
    "\n",
    "assert(len(set(obj_person_male).intersection(set(obj_person_female))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "emotional_states = [\"angry\", \"anxious\", \"ecstatic\", \"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                   \"excited\", \"devastated\", \"enraged\", \"fearful\", \"glad\", \"disappointed\",\\\n",
    "                   \"furious\", \"scared\", \"happy\", \"miserable\", \"irritated\", \"terrified\",\\\n",
    "                   \"relieved\", \"sad\"]\n",
    "\n",
    "positive_emotional_states = [\"ecstatic\", \"excited\", \"glad\", \"happy\", \"relieved\"]\n",
    "\n",
    "negative_emotional_states = [\"angry\", \"anxious\",\"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                             \"devastated\", \"enraged\", \"fearful\", \"disappointed\",\\\n",
    "                             \"furious\", \"scared\", \"miserable\", \"irritated\", \"terrified\", \"sad\"]\n",
    "\t\t\t\t\t\t\t \n",
    "print(set(positive_emotional_states).intersection(set(negative_emotional_states)))\n",
    "\n",
    "assert(len(set(positive_emotional_states).intersection(set(negative_emotional_states))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "emotional_situations = [\"annoying\", \"dreadful\", \"amazing\", \"depressing\", \\\n",
    "                       \"displeasing\", \"horrible\", \"funny\", \"gloomy\", \\\n",
    "                       \"irritating\", \"shocking\", \"great\", \"grim\", \\\n",
    "                       \"outrageous\", \"terrifying\", \"hilarious\", \"heartbreaking\", \\\n",
    "                       \"vexing\", \"threatening\", \"wonderful\", \"serious\"]\n",
    "\t\t\t\t\t   \n",
    "positive_emotional_situations = [\"amazing\", \"funny\", \"great\", \"hilarious\",\"wonderful\"]\n",
    "\n",
    "negative_emotional_situations = [\"annoying\", \"dreadful\", \"depressing\", \"displeasing\", \"horrible\",\\\n",
    "                                \"gloomy\", \"irritating\", \"shocking\", \"grim\", \"outrageous\", \"terrifying\", \"heartbreaking\",\\\n",
    "                                \"vexing\",  \"threatening\", \"serious\"]\n",
    "\t\t\t\t\t\t\t\t\n",
    "print(set(positive_emotional_situations).intersection(set(negative_emotional_situations)))\n",
    "\n",
    "assert(len(set(positive_emotional_situations).intersection(set(negative_emotional_situations))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "neutral_subjs = [\"I made\", \"The situation makes\", \"The conversation with\"]\n",
    "verb_feel_list = [\"feel\", \"made me feel\", \"found himself/herself in a/an\", \"told us all about the recent\", \"was\", \\\n",
    "                  \"found herself in a/an\", \"found himself in a/an\"]\n",
    "end_noun = ['situation', 'events']\n",
    "\n",
    "neutral_pronoun = [\"I\", \"me\"]\n",
    "neutral_sent_verb = [\"saw\", \"talked to\"]\n",
    "end_sentence = [\"in the market\", \"yesterday\", \"goes to the school in our neighborhood\", \"has two children\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_dict(D):\n",
    "    return {k: v for k, v in sorted(D.items(), key=lambda item: item[1], reverse=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate_dict(error_dict, count_dict):\n",
    "    error_rate_dict = {}\n",
    "    for key in error_dict:\n",
    "        error_rate_dict[key] = error_dict[key]/count_dict[key]\n",
    "    return get_sorted_dict(error_rate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dict(error_dict, count_dict):\n",
    "    error_rate_dict = get_error_rate_dict(error_dict, count_dict)\n",
    "    \n",
    "    probability_dict = {}\n",
    "    error_rate_sum = sum(error_rate_dict.values())\n",
    "    for error_rate in error_rate_dict:\n",
    "        probability_dict[error_rate] = error_rate_dict[error_rate]/error_rate_sum\n",
    "    \n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_random_choice(error_dict, count_dict, probablilities_dict = None):\n",
    "#     print(\"error_dict: {}\".format(error_dict))\n",
    "#     print(\"count_dict: {}\".format(count_dict))\n",
    "#     print(\"probablilities_dict: {}\".format(probablilities_dict))\n",
    "    \n",
    "    if probablilities_dict == None:\n",
    "        probability_dict = get_probability_dict(error_dict, count_dict)\n",
    "    else:\n",
    "        probability_dict = probablilities_dict\n",
    "    \n",
    "    return list(probability_dict.keys())[np.random.choice(len(list(probability_dict.keys())), p=list(probability_dict.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_schema_oracle(inp):\n",
    "    res = -1 #neutral\n",
    "    token_list = inp.rstrip(\".\").split()\n",
    "    for token in token_list:\n",
    "        if (token in positive_emotional_situations) or \\\n",
    "            (token in positive_emotional_states):\n",
    "            res = 1\n",
    "            break\n",
    "        elif (token in negative_emotional_situations) or \\\n",
    "            (token in negative_emotional_states):\n",
    "            res = 0\n",
    "            break           \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_dict(flag, my_dict):\n",
    "    test_female = person_female + subj_pronoun_female + obj_pronoun_female\n",
    "    test_male = person_male + subj_pronoun_male + obj_pronoun_male\n",
    "    res = {}\n",
    "    if flag == 0:\n",
    "        for i in my_dict:\n",
    "            if i in test_female:\n",
    "                res[i] = my_dict[i]\n",
    "    elif flag == 1:\n",
    "        for i in my_dict:\n",
    "            if i in test_male:\n",
    "                res[i] = my_dict[i]\n",
    "                \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_choice(choice):\n",
    "    if choice == 0:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = subj_person_male_all[person_choice]\n",
    "        subj_person_female = subj_person_female_all[person_choice]\n",
    "    elif choice == 1:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = random.choice(subj_person_male_all)\n",
    "        subj_person_female = random.choice(subj_person_female_all)\n",
    "    elif choice == 2:\n",
    "        subj_person_male = random.choice(occupations_male_biased)\n",
    "        subj_person_female = random.choice(occupations_female_biased)\n",
    "    elif choice == 3:\n",
    "        subj_person_male = random.choice(male_biased_names)\n",
    "        subj_person_female = random.choice(female_biased_names)\n",
    "    elif choice == 4:\n",
    "        subj_person_male = random.choice(gen_male_names)\n",
    "        subj_person_female = random.choice(gen_female_names)\n",
    "    elif choice == 5:\n",
    "        subj_person_male = random.choice(african_american_names)\n",
    "        subj_person_female = random.choice(european_american_names)\n",
    "    \n",
    "    return subj_person_male, subj_person_female\n",
    "\n",
    "\n",
    "def subj_choice_noun_probabilistically(choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability):\n",
    "    tmp1, tmp2 = None, None\n",
    "    if noun_error2:\n",
    "        subj_person_male = get_weighted_random_choice(noun_error2, noun_dict2, probablilities_dict=noun2_probability)\n",
    "    else:\n",
    "        subj_person_male, tmp1 = subj_choice(choice)\n",
    "    \n",
    "    if noun_error1:\n",
    "        subj_person_female = get_weighted_random_choice(noun_error1, noun_dict1, probablilities_dict=noun1_probability)\n",
    "    else:\n",
    "        tmp2, subj_person_female = subj_choice(choice)\n",
    "    \n",
    "    return subj_person_male, subj_person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokens_probabilistically(choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability):\n",
    "    \n",
    "#     print(\"noun_error1: {}\".format(noun_error1))\n",
    "#     print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "#     print(\"noun1_probability: {}\".format(noun1_probability))\n",
    "    \n",
    "#     print(\"noun_error2: {}\".format(noun_error2))\n",
    "#     print(\"noun_dict2: {}\".format(noun_dict2))\n",
    "#     print(\"noun2_probability: {}\".format(noun2_probability))\n",
    "    \n",
    "    resList = []\n",
    "    \n",
    "    subj_person_male, subj_person_female = subj_choice_noun_probabilistically(choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability)\n",
    "    \n",
    "    resList.append(subj_person_male)\n",
    "    resList.append(subj_person_female)\n",
    "\n",
    "    emotional_state = random.choice(emotional_states)\n",
    "    emotional_situation = random.choice(emotional_situations)\n",
    "    \n",
    "    resList.append(emotional_state)\n",
    "    resList.append(emotional_situation)\n",
    "\n",
    "    verb1 = random.choice(verb_list_p1)\n",
    "    verb_feel = random.choice(verb_feel_list)\n",
    "    \n",
    "    resList.append(verb1)\n",
    "    resList.append(verb_feel)\n",
    "\n",
    "    neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "    neutral_subj_2 = neutral_subjs[2]\n",
    "    \n",
    "    resList.append(neutral_subj_1)\n",
    "    resList.append(neutral_subj_2)\n",
    "    \n",
    "#     print(\"resList (tokens): \", resList)\n",
    "    \n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gender_specific_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb1, emotional_state + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])      \n",
    "\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ]) \n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])       \n",
    "\n",
    "    elif schema_no == 3:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])   \n",
    "    \n",
    "    elif schema_no == 4:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])         \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neutral_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([neutral_subj_1, random.choice([obj_pronoun_female[0], subj_person_female]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([neutral_subj_1, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_female[0],subj_person_female]), verb_feel_list[4], emotional_situation + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_neutral_sentences(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    neutral_verb = random.choice(neutral_sent_verb)\n",
    "    end_sentence_1 = random.choice(end_sentence[:2])\n",
    "    end_sentence_2 = random.choice(end_sentence[2:4])\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "    \n",
    "    if schema_no == 0:\n",
    "        res_str_1 = \" \".join([subj_person_female, neutral_verb, neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, neutral_verb, neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 1:\n",
    "        res_str_1 = \" \".join([neutral_pronoun[0], neutral_verb, subj_person_female, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_pronoun[0], neutral_verb, subj_person_male, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([ subj_person_female, end_sentence_2 + \".\"])\n",
    "        res_str_2 =  \" \".join([ subj_person_male, end_sentence_2 + \".\"])\n",
    "    \n",
    "    return res_str_1, res_str_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_counts(inp1, inp2, list_tokens, list_dict):\n",
    "    for tok in list_tokens:\n",
    "        if (tok in inp1):\n",
    "            update_dict(list_dict[list_tokens.index(tok)],tok)\n",
    "        if (tok in inp2):\n",
    "            update_dict(list_dict[list_tokens.index(tok)],tok)\n",
    "    return list_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(res, list_tokens):\n",
    "    result = None\n",
    "\n",
    "    for item in list_tokens:\n",
    "        if set(res) == set(item.rstrip(\".\").split()):\n",
    "            result = item\n",
    "            break\n",
    "    return result\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bias_pairs(inp1, inp2, list_tokens, list_dict):\n",
    "    \n",
    "    for tok in list_tokens:\n",
    "        if (tok in inp1) and (tok in inp2):\n",
    "            s1 = inp1.rstrip(\".\").split()\n",
    "            s2 = inp2.rstrip(\".\").split()\n",
    "\n",
    "            res1 = list(set(s1) - set(s2))\n",
    "            res2 = list(set(s2) - set(s1))\n",
    "                        \n",
    "            if len(res1) > 1:\n",
    "                if get_token(res1, list_tokens):\n",
    "                    res1 = get_token(res1, list_tokens)\n",
    "                else:\n",
    "                    res1 = \" \".join(res1)\n",
    "            else:\n",
    "                res1 = res1[0]\n",
    "            \n",
    "            if len(res2) > 1:\n",
    "                if get_token(res2, list_tokens):\n",
    "                    res2 = get_token(res2, list_tokens)\n",
    "                else:\n",
    "                    res2 = \" \".join(res2)\n",
    "            else:\n",
    "                res2 = res2[0]\n",
    "                \n",
    "            res = res1 + \", \" + res2\n",
    "            if tok in list_dict:\n",
    "                update_dict(list_dict[list_tokens[2:].index(tok)],res)    \n",
    "    return list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically create directories for saving pickles\n",
    "def create_dir(mode):\n",
    "    target_dir = 'saved_pickles/iniining/' + mode\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), target_dir)):\n",
    "        sub_dir = target_dir.split(\"/\")\n",
    "        k = os.getcwd()\n",
    "        for dir_loc in sub_dir:\n",
    "            k = os.path.join(k, dir_loc)\n",
    "            if not os.path.exists(str(k)):\n",
    "                os.mkdir(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_data(mode, noun_dict1, noun_dict2, noun_error1, noun_error2, unique_input1_set, unique_input2_set, unique_input_pair_set, unique_input1_error_set,\\\n",
    "            unique_input2_error_set, pred_err_count, fairness_err_count, unique_pred_input1_error_set, \\\n",
    "            unique_fairness_input1_error_set, retrain_dict, subj_person_male_count ,subj_person_female_count, emotional_state_count, \\\n",
    "                emotional_situation_count,verb_feel_count, verb1_count, neutral_subj_1_count, \\\n",
    "                neutral_subj_2_count, subj_person_male_pred_error ,subj_person_female_pred_error, emotional_state_pred_error, \\\n",
    "                emotional_situation_pred_error,verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, \\\n",
    "                neutral_subj_2_pred_error, subj_person_male_fairness_error ,subj_person_female_fairness_error, emotional_state_fairness_error, \\\n",
    "                emotional_situation_fairness_error,verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, \\\n",
    "                neutral_subj_2_fairness_error, bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error):\n",
    "\n",
    "    create_dir(mode)\n",
    "    \n",
    "    noun_data_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2]\n",
    "    noun_data_name = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\"] \n",
    "    \n",
    "    print(\"noun_dict1: \", noun_dict1)\n",
    "    print(\"noun_dict2: \", noun_dict2)\n",
    "    print(\"noun_error1: \", noun_error1)\n",
    "    print(\"noun_error2: \", noun_error2)\n",
    "    \n",
    "    assert len(noun_data_name) == len(noun_data_vals), \"ERROR: bug in variables names for stored inputs\"\n",
    "    for i in range(0, len(noun_data_vals)):\n",
    "        store = noun_data_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(noun_data_vals[i], handle)\n",
    "            \n",
    "            \n",
    "    data_vals_name = [\"unique_input1_set\", \"unique_input2_set\", \"unique_input_pair_set\", \"unique_input1_error_set\",\\\n",
    "            \"unique_input2_error_set\", \"pred_err_count\", \"fairness_err_count\", \"unique_pred_input1_error_set\", \\\n",
    "            \"unique_fairness_input1_error_set\", \"retrain_dict\"]\n",
    "\n",
    "    \n",
    "#     print(\"subj_person_male_fairness_error: \", list(subj_person_male_fairness_error)[0:4])\n",
    "    \n",
    "    data_vals = [unique_input1_set, unique_input2_set, unique_input_pair_set, unique_input1_error_set,\\\n",
    "            unique_input2_error_set, pred_err_count, fairness_err_count, unique_pred_input1_error_set, \\\n",
    "            unique_fairness_input1_error_set, retrain_dict]\n",
    "    \n",
    "    token_count_vals_name = [\"subj_person_male_count\" ,\"subj_person_female_count\", \"emotional_state_count\", \\\n",
    "                \"emotional_situation_count\",\"verb_feel_count\", \"verb1_count\", \"neutral_subj_1_count\", \\\n",
    "                \"neutral_subj_2_count\"] \n",
    "\n",
    "    token_count_vals = [subj_person_male_count ,subj_person_female_count, emotional_state_count, \\\n",
    "                emotional_situation_count,verb_feel_count, verb1_count, neutral_subj_1_count, \\\n",
    "                neutral_subj_2_count] \n",
    "    \n",
    "    pred_errors_count_vals_name = [\"subj_person_male_pred_error\" ,\"subj_person_female_pred_error\", \"emotional_state_pred_error\", \\\n",
    "                \"emotional_situation_pred_error\",\"verb_feel_pred_error\", \"verb1_pred_error\", \"neutral_subj_1_pred_error\", \\\n",
    "                \"neutral_subj_2_pred_error\"] \n",
    "\n",
    "    pred_errors_count_vals = [subj_person_male_pred_error ,subj_person_female_pred_error, emotional_state_pred_error, \\\n",
    "                emotional_situation_pred_error,verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, \\\n",
    "                neutral_subj_2_pred_error] \n",
    "    \n",
    "    fairness_error_count_vals_name = [\"subj_person_male_fairness_error\" ,\"subj_person_female_fairness_error\", \"emotional_state_fairness_error\", \\\n",
    "                \"emotional_situation_fairness_error\",\"verb_feel_fairness_error\", \"verb1_fairness_error\", \"neutral_subj_1_fairness_error\", \\\n",
    "                \"neutral_subj_2_fairness_error\"] \n",
    "\n",
    "    fairness_error_count_vals = [subj_person_male_fairness_error ,subj_person_female_fairness_error, emotional_state_fairness_error, \\\n",
    "                emotional_situation_fairness_error,verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, \\\n",
    "                neutral_subj_2_fairness_error] \n",
    "    \n",
    "    bias_count_vals_name = [\"bias_pair_count\" ,\"bias_pair_pred_error\", \"bias_pair_fairness_error\"] \n",
    "\n",
    "    bias_count_vals = [bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error] \n",
    "    \n",
    "    assert len(data_vals_name) == len(data_vals), \"ERROR: bug in variables names for stored inputs\"\n",
    "    for i in range(0, len(data_vals)):\n",
    "        store = data_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(data_vals[i], handle)\n",
    "    \n",
    "    \n",
    "    assert len(token_count_vals_name) == len(token_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs\".format(token_count_vals_name)\n",
    "    for i in range(0, len(token_count_vals)):\n",
    "        store = token_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(token_count_vals[i], handle)\n",
    "            \n",
    "    assert len(pred_errors_count_vals_name) == len(pred_errors_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs\".format(fairness_error_count_vals_name)\n",
    "    for i in range(0, len(pred_errors_count_vals)):\n",
    "        store = pred_errors_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(pred_errors_count_vals[i], handle)\n",
    "            \n",
    "            \n",
    "    assert len(fairness_error_count_vals_name) == len(fairness_error_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs {}, {}\".format(fairness_error_count_vals_name)\n",
    "    for i in range(0, len(fairness_error_count_vals)):\n",
    "        store = fairness_error_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(fairness_error_count_vals[i], handle)\n",
    "    \n",
    "    \n",
    "    assert len(bias_count_vals_name) == len(bias_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs {}, {}\".format(bias_count_vals_name)\n",
    "    for i in range(0, len(bias_count_vals)):\n",
    "        store = bias_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/re-training/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(bias_count_vals[i], handle)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new inputs for Dataset (train, val and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2):\n",
    "\n",
    "#     unique_input1_set = set()\n",
    "#     unique_input2_set = set()\n",
    "#     unique_input_pair_set = set()\n",
    "\n",
    "#     unique_input1_error_set = set()\n",
    "#     unique_input2_error_set = set()\n",
    "\n",
    "#     pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "#     unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "#     retrain_dict = dict()\n",
    "\n",
    "#     subj_person_male_count, subj_person_female_count, emotional_state_count, emotional_situation_count = {}, {}, {}, {}\n",
    "#     verb_feel_count, verb1_count, neutral_subj_1_count, neutral_subj_2_count= {}, {}, {}, {}\n",
    "\n",
    "#     subj_person_male_pred_error, subj_person_female_pred_error, emotional_state_pred_error, emotional_situation_pred_error = {}, {}, {}, {}\n",
    "#     verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, neutral_subj_2_pred_error= {}, {}, {}, {}\n",
    "\n",
    "#     subj_person_male_fairness_error, subj_person_female_fairness_error, emotional_state_fairness_error, emotional_situation_fairness_error = {}, {}, {}, {}\n",
    "#     verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, neutral_subj_2_fairness_error= {}, {}, {}, {}\n",
    "\n",
    "\n",
    "\n",
    "#     count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "#     pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "#     fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "#     #bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error = {}, {}, {}\n",
    "\n",
    "#     bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "#     bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "#     bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "    \n",
    "#     noun_dict1, noun_dict2, noun_error1, noun_error2, = {}, {}, {}, {}\n",
    "    \n",
    "    tokens = []\n",
    "\n",
    "    inputs = {}\n",
    "    label1, label2 =  None, None\n",
    "    tmp1, tmp2 = 0, 0\n",
    "    \n",
    "    noun1_probability = get_probability_dict(noun_error1, noun_dict1)\n",
    "    noun2_probability = get_probability_dict(noun_error2, noun_dict2)\n",
    "    \n",
    "    for i in range(ITERS): \n",
    "\n",
    "        tokens = select_tokens_probabilistically(noun_choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability)\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2        \n",
    "\n",
    "        input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "        \n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "        \n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "\n",
    "        input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "        label1 = run_schema_oracle(input1)\n",
    "        if not (input1 in inputs) and ((label1 == 1) or (label1 == 0)):\n",
    "            inputs[input1] = label1\n",
    "            \n",
    "        label2 = run_schema_oracle(input2)\n",
    "        if not (input2 in inputs) and ((label2 == 1) or (label2 == 0)):\n",
    "            inputs[input2] = label2\n",
    "\n",
    "        if (len(inputs) == tmp1 == tmp2) or (len(inputs) >= max_input_gen_threshold):\n",
    "            print(\"Maximum input generation threshold reached, {} unique inputs generated\".format(len(inputs)))\n",
    "            break\n",
    "\n",
    "        if ITERS%2 == 0:\n",
    "            tmp1 = len(inputs)\n",
    "        else:\n",
    "            tmp2 = len(inputs)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Generate Data for Direct Gender Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  0 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/direct-gender-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'this woman': 2773, 'my girlfriend': 2805, 'this girl': 3118, 'my daughter': 3269, 'She': 2834, 'my mother': 2971, 'my sister': 3011, 'my aunt': 3943, 'my wife': 3162, 'her': 1117}\n",
      "noun_dict2 :  {'this man': 2812, 'my boyfriend': 2793, 'this boy': 3112, 'my son': 3270, 'He': 2846, 'my father': 2924, 'my brother': 3106, 'my uncle': 3871, 'my husband': 3142, 'him': 645}\n",
      "noun_error1 :  {'this girl': 297, 'my girlfriend': 212, 'my daughter': 887, 'my sister': 293, 'my aunt': 1052, 'this woman': 263, 'my mother': 433, 'She': 138, 'my wife': 405, 'her': 101}\n",
      "noun_error2 :  {'this boy': 294, 'my boyfriend': 192, 'my son': 789, 'my brother': 382, 'my uncle': 957, 'my father': 390, 'He': 155, 'my husband': 371, 'this man': 255, 'him': 69}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'this girl': 297, 'my girlfriend': 212, 'my daughter': 887, 'my sister': 293, 'my aunt': 1052, 'this woman': 263, 'my mother': 433, 'She': 138, 'my wife': 405}\n",
      "noun_dict1: {'this woman': 2773, 'my girlfriend': 2805, 'this girl': 3118, 'my daughter': 3269, 'She': 2834, 'my mother': 2971, 'my sister': 3011, 'my aunt': 3943, 'my wife': 3162}\n",
      "noun_error2: {'this boy': 294, 'my boyfriend': 192, 'my son': 789, 'my brother': 382, 'my uncle': 957, 'my father': 390, 'He': 155, 'my husband': 371, 'this man': 255}\n",
      "noun_dict2: {'this man': 2812, 'my boyfriend': 2793, 'this boy': 3112, 'my son': 3270, 'He': 2846, 'my father': 2924, 'my brother': 3106, 'my uncle': 3871, 'my husband': 3142}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate new inputs probabilistically from \n",
    "# new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3001 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(itertools.islice(new_train_data.items(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Generate Data for Random Gender Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  1 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/random-gender-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'my wife': 892, 'my girlfriend': 1010, 'this girl': 550, 'my mother': 985, 'my daughter': 924, 'this woman': 615, 'my mom': 507, 'my aunt': 1129, 'She': 695, 'my sister': 671, 'her': 1117}\n",
      "noun_dict2 :  {'this boy': 852, 'my father': 821, 'my uncle': 745, 'my husband': 733, 'He': 938, 'my boyfriend': 784, 'my brother': 925, 'my son': 605, 'my dad': 681, 'this man': 840, 'him': 677}\n",
      "noun_error1 :  {'my wife': 172, 'my girlfriend': 233, 'my mother': 143, 'this girl': 66, 'my daughter': 197, 'my aunt': 293, 'my sister': 53, 'She': 88, 'my mom': 24, 'this woman': 68, 'her': 99}\n",
      "noun_error2 :  {'this boy': 127, 'my father': 141, 'my uncle': 119, 'my husband': 119, 'my son': 77, 'my boyfriend': 123, 'He': 192, 'my brother': 148, 'my dad': 104, 'this man': 114, 'him': 82}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' +  mode  + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'my wife': 172, 'my girlfriend': 233, 'my mother': 143, 'this girl': 66, 'my daughter': 197, 'my aunt': 293, 'my sister': 53, 'She': 88, 'my mom': 24, 'this woman': 68}\n",
      "noun_dict1: {'my wife': 892, 'my girlfriend': 1010, 'this girl': 550, 'my mother': 985, 'my daughter': 924, 'this woman': 615, 'my mom': 507, 'my aunt': 1129, 'She': 695, 'my sister': 671}\n",
      "noun_error2: {'this boy': 127, 'my father': 141, 'my uncle': 119, 'my husband': 119, 'my son': 77, 'my boyfriend': 123, 'He': 192, 'my brother': 148, 'my dad': 104, 'this man': 114}\n",
      "noun_dict2: {'this boy': 852, 'my father': 821, 'my uncle': 745, 'my husband': 733, 'He': 938, 'my boyfriend': 784, 'my brother': 925, 'my son': 605, 'my dad': 681, 'this man': 840}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "\n",
    "\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3000 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3493"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(itertools.islice(new_train_data.items(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Generate Data for Random Gender Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  2 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/gender-occupation-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'the teacher': 517, 'the librarian': 235, 'the writer': 522, 'the attendant': 320, 'the receptionist': 195, 'the the nurse': 333, 'the the paralegal': 341, 'the clerk': 286, 'the designer': 298, 'the cashier': 303, 'the editor': 353, 'the the librarian': 229, 'the baker': 273, 'the auditor': 329, 'the tailor': 333, 'the counselor': 392, 'the cleaner': 424, 'the assistant': 269, 'the housekeeper': 279, 'the hairdresser': 279, 'the secretary': 183, 'the nurse': 377, 'her': 774}\n",
      "noun_dict2 :  {'the mover': 167, 'the engineer': 401, 'the carpenter': 357, 'the farmer': 289, 'the construction worker': 266, 'the salesperson': 462, 'the accountant': 330, 'the manager': 256, 'the lawyer': 325, 'the cook': 384, 'the sheriff': 188, 'the mechanic': 410, 'the janitor': 279, 'the analyst': 507, 'the chief': 304, 'the guard': 231, 'the physician': 258, 'the driver': 230, 'the CEO': 285, 'the technician': 351, 'the developer': 270, 'the supervisor': 274, 'the laborer': 233, 'him': 583}\n",
      "noun_error1 :  {'the attendant': 40, 'the clerk': 56, 'the writer': 136, 'the designer': 32, 'the the paralegal': 68, 'the teacher': 106, 'the editor': 63, 'the librarian': 39, 'the cleaner': 98, 'the the nurse': 56, 'the receptionist': 27, 'the baker': 35, 'the counselor': 74, 'the auditor': 80, 'the housekeeper': 46, 'the assistant': 27, 'the tailor': 63, 'the nurse': 73, 'the the librarian': 20, 'the hairdresser': 35, 'the cashier': 47, 'the secretary': 20, 'her': 92}\n",
      "noun_error2 :  {'the farmer': 58, 'the accountant': 49, 'the carpenter': 40, 'the cook': 127, 'the mechanic': 77, 'the construction worker': 42, 'the salesperson': 63, 'the engineer': 80, 'the chief': 57, 'the analyst': 126, 'the driver': 29, 'the CEO': 33, 'the janitor': 36, 'the technician': 70, 'the mover': 19, 'the lawyer': 54, 'the developer': 50, 'the physician': 34, 'the guard': 35, 'the sheriff': 31, 'the manager': 32, 'the supervisor': 67, 'the laborer': 23, 'him': 53}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'the attendant': 40, 'the clerk': 56, 'the writer': 136, 'the designer': 32, 'the the paralegal': 68, 'the teacher': 106, 'the editor': 63, 'the librarian': 39, 'the cleaner': 98, 'the the nurse': 56, 'the receptionist': 27, 'the baker': 35, 'the counselor': 74, 'the auditor': 80, 'the housekeeper': 46, 'the assistant': 27, 'the tailor': 63, 'the nurse': 73, 'the the librarian': 20, 'the hairdresser': 35, 'the cashier': 47, 'the secretary': 20}\n",
      "noun_dict1: {'the teacher': 517, 'the librarian': 235, 'the writer': 522, 'the attendant': 320, 'the receptionist': 195, 'the the nurse': 333, 'the the paralegal': 341, 'the clerk': 286, 'the designer': 298, 'the cashier': 303, 'the editor': 353, 'the the librarian': 229, 'the baker': 273, 'the auditor': 329, 'the tailor': 333, 'the counselor': 392, 'the cleaner': 424, 'the assistant': 269, 'the housekeeper': 279, 'the hairdresser': 279, 'the secretary': 183, 'the nurse': 377}\n",
      "noun_error2: {'the farmer': 58, 'the accountant': 49, 'the carpenter': 40, 'the cook': 127, 'the mechanic': 77, 'the construction worker': 42, 'the salesperson': 63, 'the engineer': 80, 'the chief': 57, 'the analyst': 126, 'the driver': 29, 'the CEO': 33, 'the janitor': 36, 'the technician': 70, 'the mover': 19, 'the lawyer': 54, 'the developer': 50, 'the physician': 34, 'the guard': 35, 'the sheriff': 31, 'the manager': 32, 'the supervisor': 67, 'the laborer': 23}\n",
      "noun_dict2: {'the mover': 167, 'the engineer': 401, 'the carpenter': 357, 'the farmer': 289, 'the construction worker': 266, 'the salesperson': 462, 'the accountant': 330, 'the manager': 256, 'the lawyer': 325, 'the cook': 384, 'the sheriff': 188, 'the mechanic': 410, 'the janitor': 279, 'the analyst': 507, 'the chief': 304, 'the guard': 231, 'the physician': 258, 'the driver': 230, 'the CEO': 285, 'the technician': 351, 'the developer': 270, 'the supervisor': 274, 'the laborer': 233}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "\n",
    "\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3004 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6377"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(itertools.islice(new_train_data.items(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Generate Data for Indirect Gender Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  3 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/gender-name-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'Donna': 183, 'Michelle': 237, 'Sarah': 242, 'Dorothy ': 257, 'Betty': 208, 'Deborah': 357, 'Emily': 189, 'Kimberly': 267, 'Patricia': 172, 'Carol': 361, 'Laura': 261, 'Margaret': 205, 'Jennifer': 219, 'Jessica': 190, 'Sandra': 279, 'Cynthia': 234, 'Sharon': 336, 'Nancy': 231, 'Susan': 202, 'Linda': 218, 'Karen': 320, 'Melissa': 106, 'Rebecca': 196, 'Amanda': 181, 'Barbara': 204, 'Elizabeth': 205, 'Ashley': 138, 'Stephanie': 199, 'Mary': 227, 'Lisa': 248, 'her': 573}\n",
      "noun_dict2 :  {'Paul': 220, 'Christopher': 302, 'Joseph': 216, 'Kenneth': 262, 'Michael ': 180, 'Charles': 168, 'Brian': 177, 'Mark': 303, 'Thomas': 320, 'Andrew': 176, 'Kevin': 250, 'David ': 140, 'John ': 253, 'Edward': 276, 'Jason': 191, 'James': 279, 'Robert ': 263, 'Daniel': 228, 'Ryan': 217, 'Joshua': 271, 'Richard': 222, 'Anthony': 256, 'Jeffrey': 210, 'William ': 282, 'Timothy': 232, 'Steven': 263, 'Donald': 147, 'George': 261, 'Ronald': 139, 'Matthew': 185, 'him': 566}\n",
      "noun_error1 :  {'Michelle': 76, 'Sarah': 40, 'Betty': 31, 'Deborah': 102, 'Kimberly': 90, 'Patricia': 27, 'Carol': 86, 'Donna': 30, 'Margaret': 52, 'Sandra': 67, 'Sharon': 93, 'Nancy': 55, 'Susan': 21, 'Karen': 75, 'Linda': 19, 'Melissa': 5, 'Rebecca': 18, 'Jennifer': 29, 'Dorothy ': 37, 'Cynthia': 26, 'Emily': 35, 'Amanda': 34, 'Barbara': 54, 'Elizabeth': 40, 'Laura': 59, 'Ashley': 9, 'Mary': 27, 'Lisa': 48, 'Stephanie': 39, 'Jessica': 23, 'her': 89}\n",
      "noun_error2 :  {'Christopher': 122, 'Joseph': 35, 'Kenneth': 52, 'Michael ': 22, 'Charles': 23, 'Thomas': 80, 'Andrew': 25, 'Kevin': 40, 'David ': 8, 'John ': 44, 'Edward': 63, 'Jason': 40, 'Robert ': 64, 'Daniel': 29, 'Ryan': 39, 'Joshua': 89, 'Richard': 37, 'William ': 62, 'Mark': 65, 'James': 68, 'Steven': 37, 'Timothy': 66, 'Donald': 15, 'George': 64, 'Jeffrey': 26, 'Paul': 31, 'Matthew': 40, 'Brian': 20, 'Ronald': 20, 'Anthony': 63, 'him': 54}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'Michelle': 76, 'Sarah': 40, 'Betty': 31, 'Deborah': 102, 'Kimberly': 90, 'Patricia': 27, 'Carol': 86, 'Donna': 30, 'Margaret': 52, 'Sandra': 67, 'Sharon': 93, 'Nancy': 55, 'Susan': 21, 'Karen': 75, 'Linda': 19, 'Melissa': 5, 'Rebecca': 18, 'Jennifer': 29, 'Dorothy ': 37, 'Cynthia': 26, 'Emily': 35, 'Amanda': 34, 'Barbara': 54, 'Elizabeth': 40, 'Laura': 59, 'Ashley': 9, 'Mary': 27, 'Lisa': 48, 'Stephanie': 39, 'Jessica': 23}\n",
      "noun_dict1: {'Donna': 183, 'Michelle': 237, 'Sarah': 242, 'Dorothy ': 257, 'Betty': 208, 'Deborah': 357, 'Emily': 189, 'Kimberly': 267, 'Patricia': 172, 'Carol': 361, 'Laura': 261, 'Margaret': 205, 'Jennifer': 219, 'Jessica': 190, 'Sandra': 279, 'Cynthia': 234, 'Sharon': 336, 'Nancy': 231, 'Susan': 202, 'Linda': 218, 'Karen': 320, 'Melissa': 106, 'Rebecca': 196, 'Amanda': 181, 'Barbara': 204, 'Elizabeth': 205, 'Ashley': 138, 'Stephanie': 199, 'Mary': 227, 'Lisa': 248}\n",
      "noun_error2: {'Christopher': 122, 'Joseph': 35, 'Kenneth': 52, 'Michael ': 22, 'Charles': 23, 'Thomas': 80, 'Andrew': 25, 'Kevin': 40, 'David ': 8, 'John ': 44, 'Edward': 63, 'Jason': 40, 'Robert ': 64, 'Daniel': 29, 'Ryan': 39, 'Joshua': 89, 'Richard': 37, 'William ': 62, 'Mark': 65, 'James': 68, 'Steven': 37, 'Timothy': 66, 'Donald': 15, 'George': 64, 'Jeffrey': 26, 'Paul': 31, 'Matthew': 40, 'Brian': 20, 'Ronald': 20, 'Anthony': 63}\n",
      "noun_dict2: {'Paul': 220, 'Christopher': 302, 'Joseph': 216, 'Kenneth': 262, 'Michael ': 180, 'Charles': 168, 'Brian': 177, 'Mark': 303, 'Thomas': 320, 'Andrew': 176, 'Kevin': 250, 'David ': 140, 'John ': 253, 'Edward': 276, 'Jason': 191, 'James': 279, 'Robert ': 263, 'Daniel': 228, 'Ryan': 217, 'Joshua': 271, 'Richard': 222, 'Anthony': 256, 'Jeffrey': 210, 'William ': 282, 'Timothy': 232, 'Steven': 263, 'Donald': 147, 'George': 261, 'Ronald': 139, 'Matthew': 185}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "\n",
    "\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3001 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9259"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(itertools.islice(new_train_data.items(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Generate Data for Indirect Racial Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  5 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"hub/racial-name-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'Nancy': 407, 'Ryan': 305, 'Justin': 419, 'Adam': 395, 'Stephanie': 450, 'Andrew': 257, 'Kristin': 277, 'Katie': 396, 'Alan': 352, 'Melanie': 449, 'Amanda': 356, 'Betsy': 206, 'Jack': 364, 'Josh': 560, 'Roger': 276, 'Ellen': 370, 'Heather': 321, 'Courtney': 319, 'Harry': 363, 'Frank': 246, 'her': 742}\n",
      "noun_dict2 :  {'Alphonse': 346, 'Jerome': 397, 'Jamel': 412, 'Jasmine': 375, 'Lakisha': 358, 'Tia': 296, 'Shereen': 396, 'Nichelle': 381, 'Ebony': 212, 'Terrence': 364, 'Tanisha': 305, 'Shaniqua': 305, 'Malik': 484, 'Lamar': 438, 'Torrance': 225, 'Leroy': 396, 'Darnell': 357, 'Latisha': 396, 'Latoya': 258, 'Alonzo': 367, 'him': 582}\n",
      "noun_error1 :  {'Ryan': 36, 'Justin': 91, 'Adam': 42, 'Stephanie': 86, 'Alan': 50, 'Melanie': 93, 'Amanda': 56, 'Betsy': 8, 'Jack': 78, 'Andrew': 37, 'Josh': 203, 'Nancy': 65, 'Heather': 42, 'Roger': 47, 'Courtney': 47, 'Harry': 81, 'Ellen': 56, 'Kristin': 46, 'Katie': 71, 'Frank': 32, 'her': 70}\n",
      "noun_error2 :  {'Jerome': 62, 'Jamel': 74, 'Lakisha': 50, 'Nichelle': 71, 'Jasmine': 78, 'Ebony': 22, 'Shereen': 65, 'Terrence': 57, 'Tia': 46, 'Alphonse': 58, 'Shaniqua': 43, 'Malik': 129, 'Lamar': 98, 'Leroy': 68, 'Tanisha': 48, 'Latisha': 70, 'Torrance': 23, 'Darnell': 69, 'Alonzo': 66, 'Latoya': 34, 'him': 70}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = 'Exploitation/saved_pickles/exploitation/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('Exploitation/saved_pickles/exploitation/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]\n",
    "\n",
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'Ryan': 36, 'Justin': 91, 'Adam': 42, 'Stephanie': 86, 'Alan': 50, 'Melanie': 93, 'Amanda': 56, 'Betsy': 8, 'Jack': 78, 'Andrew': 37, 'Josh': 203, 'Nancy': 65, 'Heather': 42, 'Roger': 47, 'Courtney': 47, 'Harry': 81, 'Ellen': 56, 'Kristin': 46, 'Katie': 71, 'Frank': 32}\n",
      "noun_dict1: {'Nancy': 407, 'Ryan': 305, 'Justin': 419, 'Adam': 395, 'Stephanie': 450, 'Andrew': 257, 'Kristin': 277, 'Katie': 396, 'Alan': 352, 'Melanie': 449, 'Amanda': 356, 'Betsy': 206, 'Jack': 364, 'Josh': 560, 'Roger': 276, 'Ellen': 370, 'Heather': 321, 'Courtney': 319, 'Harry': 363, 'Frank': 246}\n",
      "noun_error2: {'Jerome': 62, 'Jamel': 74, 'Lakisha': 50, 'Nichelle': 71, 'Jasmine': 78, 'Ebony': 22, 'Shereen': 65, 'Terrence': 57, 'Tia': 46, 'Alphonse': 58, 'Shaniqua': 43, 'Malik': 129, 'Lamar': 98, 'Leroy': 68, 'Tanisha': 48, 'Latisha': 70, 'Torrance': 23, 'Darnell': 69, 'Alonzo': 66, 'Latoya': 34}\n",
      "noun_dict2: {'Alphonse': 346, 'Jerome': 397, 'Jamel': 412, 'Jasmine': 375, 'Lakisha': 358, 'Tia': 296, 'Shereen': 396, 'Nichelle': 381, 'Ebony': 212, 'Terrence': 364, 'Tanisha': 305, 'Shaniqua': 305, 'Malik': 484, 'Lamar': 438, 'Torrance': 225, 'Leroy': 396, 'Darnell': 357, 'Latisha': 396, 'Latoya': 258, 'Alonzo': 367}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "\n",
    "\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3005 unique inputs generated\n"
     ]
    }
   ],
   "source": [
    "# generate new inputs probabilistically from \n",
    "new_train_data.update(generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12022"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "del noun_error2\n",
    "del noun_dict2\n",
    "del noun_dict1\n",
    "del noun_error1\n",
    "del c_vals\n",
    "del pred_err_count\n",
    "del fairness_err_count\n",
    "del unique_pred_input1_error_set\n",
    "del unique_fairness_input1_error_set\n",
    "del retrain_dict\n",
    "del count_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pred_error_dict\n",
    "del fairness_error_dict\n",
    "del bias_pair_count\n",
    "del bias_pair_pred_error\n",
    "del bias_pair_fairness_error\n",
    "# del pred_err_count\n",
    "# del fairness_err_count\n",
    "# del unique_pred_input1_error_set\n",
    "# del unique_fairness_input1_error_set\n",
    "# del retrain_dict\n",
    "# del count_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Download the IMDB dataset\n",
    "\n",
    "The IMDB dataset is available on [imdb reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) or on [TensorFlow datasets](https://www.tensorflow.org/datasets). The following code downloads the IMDB dataset to your machine (or the colab runtime):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_data, validation_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment IMDB Training Data with new generated test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dict_randomly(new_train_data, size):\n",
    "    res_dict = {}\n",
    "    keys = random.sample(list(new_train_data), size)\n",
    "#     values = [new_train_data[k] for k in keys]\n",
    "    for i in keys:\n",
    "        res_dict[i] = new_train_data[i]\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The conversation with the auditor was threatening.': 0, 'my girlfriend feel miserable.': 0, 'Roger made me feel anxious.': 0, 'I made the CEO feel irritated.': 0, 'the baker told us all about the recent horrible events.': 0}\n"
     ]
    }
   ],
   "source": [
    "print(slice_dict_randomly(new_train_data, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Edward found himself in a/an outrageous situation.': 0, 'Ellen told us all about the recent heartbreaking events.': 0, 'my aunt told us all about the recent irritating events.': 0, 'Jessica was discouraged.': 0, 'Jack made me feel excited.': 1}\n"
     ]
    }
   ],
   "source": [
    "print(slice_dict_randomly(new_train_data, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the baker feels terrified.': 0, 'Elizabeth found herself in a/an dreadful situation.': 0, 'Amanda told us all about the recent serious events.': 0, 'my boyfriend found himself in a/an grim situation.': 0, 'William  told us all about the recent shocking events.': 0}\n"
     ]
    }
   ],
   "source": [
    "print(slice_dict_randomly(new_train_data, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent = [0.1 * len(train_data), 0.2 * len(train_data), 0.3 * len(train_data)]\n",
    "percent = [0.01 * length, 0.02 * length, 0.03 * length, 0.04 * length, 0.05 * length, 0.06 * length, 0.07 * length, 0.08 * length, 0.09 * length, 0.1 * length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[150.0, 300.0, 450.0, 600.0, 750.0, 900.0, 1050.0, 1200.0, 1350.0, 1500.0]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_one_percent = [round(0.0025 * length, 0), round(0.005 * length, 0), round(0.0075 * length, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38.0, 75.0, 112.0]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "below_one_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"one_quarter_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(below_one_percent[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "print(len(one_quarter_percent_dataset_rand_1))\n",
    "print(len(one_quarter_percent_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_quarter_percent_datasets = [one_quarter_percent_dataset_rand_1, one_quarter_percent_dataset_rand_2, one_quarter_percent_dataset_rand_3, one_quarter_percent_dataset_rand_4, \\\n",
    "                            one_quarter_percent_dataset_rand_5, one_quarter_percent_dataset_rand_6, one_quarter_percent_dataset_rand_7, one_quarter_percent_dataset_rand_8, \\\n",
    "                            one_quarter_percent_dataset_rand_9, one_quarter_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"half_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(below_one_percent[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_percent_datasets = [half_percent_dataset_rand_1, half_percent_dataset_rand_2, half_percent_dataset_rand_3, half_percent_dataset_rand_4, \\\n",
    "                            half_percent_dataset_rand_5, half_percent_dataset_rand_6, half_percent_dataset_rand_7, half_percent_dataset_rand_8, \\\n",
    "                            half_percent_dataset_rand_9, half_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"three_quarter_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(below_one_percent[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_quarter_percent_datasets = [three_quarter_percent_dataset_rand_1, three_quarter_percent_dataset_rand_2, three_quarter_percent_dataset_rand_3, three_quarter_percent_dataset_rand_4, \\\n",
    "                            three_quarter_percent_dataset_rand_5, three_quarter_percent_dataset_rand_6, three_quarter_percent_dataset_rand_7, three_quarter_percent_dataset_rand_8, \\\n",
    "                            three_quarter_percent_dataset_rand_9, three_quarter_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_quarter_percent_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(half_percent_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(three_quarter_percent_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"one_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_percent_datasets = [one_percent_dataset_rand_1, one_percent_dataset_rand_2, one_percent_dataset_rand_3, one_percent_dataset_rand_4, \\\n",
    "                            one_percent_dataset_rand_5, one_percent_dataset_rand_6, one_percent_dataset_rand_7, one_percent_dataset_rand_8, \\\n",
    "                            one_percent_dataset_rand_9, one_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"two_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_percent_datasets = [two_percent_dataset_rand_1, two_percent_dataset_rand_2, two_percent_dataset_rand_3, two_percent_dataset_rand_4, \\\n",
    "                            two_percent_dataset_rand_5, two_percent_dataset_rand_6, two_percent_dataset_rand_7, two_percent_dataset_rand_8, \\\n",
    "                            two_percent_dataset_rand_9, two_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"three_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_percent_datasets = [three_percent_dataset_rand_1, three_percent_dataset_rand_2, three_percent_dataset_rand_3, three_percent_dataset_rand_4, \\\n",
    "                            three_percent_dataset_rand_5, three_percent_dataset_rand_6, three_percent_dataset_rand_7, three_percent_dataset_rand_8, \\\n",
    "                            three_percent_dataset_rand_9, three_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"four_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_percent_datasets = [four_percent_dataset_rand_1, four_percent_dataset_rand_2, four_percent_dataset_rand_3, four_percent_dataset_rand_4, \\\n",
    "                            four_percent_dataset_rand_5, four_percent_dataset_rand_6, four_percent_dataset_rand_7, four_percent_dataset_rand_8, \\\n",
    "                            four_percent_dataset_rand_9, four_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"five_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_percent_datasets = [five_percent_dataset_rand_1, five_percent_dataset_rand_2, five_percent_dataset_rand_3, five_percent_dataset_rand_4, \\\n",
    "                            five_percent_dataset_rand_5, five_percent_dataset_rand_6, five_percent_dataset_rand_7, five_percent_dataset_rand_8, \\\n",
    "                            five_percent_dataset_rand_9, five_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"six_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_percent_datasets = [six_percent_dataset_rand_1, six_percent_dataset_rand_2, six_percent_dataset_rand_3, six_percent_dataset_rand_4, \\\n",
    "                            six_percent_dataset_rand_5, six_percent_dataset_rand_6, six_percent_dataset_rand_7, six_percent_dataset_rand_8, \\\n",
    "                            six_percent_dataset_rand_9, six_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"seven_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_percent_datasets = [seven_percent_dataset_rand_1, seven_percent_dataset_rand_2, seven_percent_dataset_rand_3, seven_percent_dataset_rand_4, \\\n",
    "                            seven_percent_dataset_rand_5, seven_percent_dataset_rand_6, seven_percent_dataset_rand_7, seven_percent_dataset_rand_8, \\\n",
    "                            seven_percent_dataset_rand_9, seven_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"eight_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_percent_datasets = [eight_percent_dataset_rand_1, eight_percent_dataset_rand_2, eight_percent_dataset_rand_3, eight_percent_dataset_rand_4, \\\n",
    "                            eight_percent_dataset_rand_5, eight_percent_dataset_rand_6, eight_percent_dataset_rand_7, eight_percent_dataset_rand_8, \\\n",
    "                            eight_percent_dataset_rand_9, eight_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"nine_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_percent_datasets = [nine_percent_dataset_rand_1, nine_percent_dataset_rand_2, nine_percent_dataset_rand_3, nine_percent_dataset_rand_4, \\\n",
    "                            nine_percent_dataset_rand_5, nine_percent_dataset_rand_6, nine_percent_dataset_rand_7, nine_percent_dataset_rand_8, \\\n",
    "                            nine_percent_dataset_rand_9, nine_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    str_var = \"ten_percent_dataset_rand_\" +  str(i)\n",
    "    locals()[str_var] = slice_dict_randomly(new_train_data, int(percent[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_percent_datasets = [ten_percent_dataset_rand_1, ten_percent_dataset_rand_2, ten_percent_dataset_rand_3, ten_percent_dataset_rand_4, \\\n",
    "                            ten_percent_dataset_rand_5, ten_percent_dataset_rand_6, ten_percent_dataset_rand_7, ten_percent_dataset_rand_8, \\\n",
    "                            ten_percent_dataset_rand_9, ten_percent_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ten_percent_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ten_percent_dataset_rand_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ten_percent_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ten_percent_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.framework import ops\n",
    "# ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_data(percent_dataset, train_data, threshold = None):\n",
    "#     res_data = train_data.prefetch(1)\n",
    "#     print(\"Init res_data: \", len(res_data))\n",
    "#     print(\"Type res_data: \", type(res_data))\n",
    "#     res_data.clear()\n",
    "#     prefetched_tensor_tuple = tuple()\n",
    "    c = 0\n",
    "    for i in percent_dataset:\n",
    "        c+= 1\n",
    "#         if (c % 2) == 0:\n",
    "#             gc.collect()\n",
    "        label_tmp = tf.constant([percent_dataset[i]]).numpy()[0]\n",
    "        data_tmp = tf.constant([i]).numpy()[0]\n",
    "        label = tf.convert_to_tensor(label_tmp, np.int64)\n",
    "        data = tf.convert_to_tensor(data_tmp)\n",
    "        data_point = (data, label)\n",
    "        tensor_data_point = tf.data.Dataset.from_tensors(data_point)\n",
    "        prefetched_tensor = tensor_data_point.prefetch(len(tensor_data_point))\n",
    "#         prefetched_tensor = tensor_data_point.prefetch(1)\n",
    "#         prefetched_tensor_tuple\n",
    "        if threshold:\n",
    "            train_data, _ = train_test_split(train_data, test_size=0.2,random_state=42,shuffle=True)\n",
    "#             train,test= train_test_split(all_images,test_size=0.2,random_state=42,shuffle=True)\n",
    "            #concatenate(prefetched_tensor)\n",
    "        else:\n",
    "            train_data = train_data.concatenate(prefetched_tensor) \n",
    "#         ops.reset_default_graph()\n",
    "#     print(\"Final res_data: \", len(res_data))\n",
    "#     print(\"Type res_data: \", type(res_data))\n",
    "#         tf.reset_default_graph #()\n",
    "#     tf.reset_default_graph #()\n",
    "#     ops.reset_default_graph()\n",
    "    return train_data #.concatenate(res_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in one_quarter_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"one_quarter_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15038\n",
      "15038\n"
     ]
    }
   ],
   "source": [
    "print(len(one_quarter_percent_additional_train_dataset_rand_1))\n",
    "print(len(one_quarter_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in half_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"half_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15075\n",
      "15075\n"
     ]
    }
   ],
   "source": [
    "print(len(half_percent_additional_train_dataset_rand_1))\n",
    "print(len(half_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in three_quarter_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"three_quarter_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15112\n",
      "15112\n"
     ]
    }
   ],
   "source": [
    "print(len(three_quarter_percent_additional_train_dataset_rand_1))\n",
    "print(len(three_quarter_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in one_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"one_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15150\n",
      "15150\n"
     ]
    }
   ],
   "source": [
    "print(len(one_percent_additional_train_dataset_rand_1))\n",
    "print(len(one_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in two_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"two_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15300\n",
      "15300\n"
     ]
    }
   ],
   "source": [
    "print(len(two_percent_additional_train_dataset_rand_1))\n",
    "print(len(two_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in three_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"three_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15450\n",
      "15450\n"
     ]
    }
   ],
   "source": [
    "print(len(three_percent_additional_train_dataset_rand_1))\n",
    "print(len(three_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in four_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"four_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15600\n",
      "15600\n"
     ]
    }
   ],
   "source": [
    "print(len(four_percent_additional_train_dataset_rand_1))\n",
    "print(len(four_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in five_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"five_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15750\n",
      "15750\n"
     ]
    }
   ],
   "source": [
    "print(len(five_percent_additional_train_dataset_rand_1))\n",
    "print(len(five_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in six_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"six_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n      dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-37e637c80e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstr_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"six_percent_additional_train_dataset_rand_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0madd_new_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-203-f173cc91f6f8>\u001b[0m in \u001b[0;36madd_new_data\u001b[0;34m(percent_dataset, train_data, threshold)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#         prefetched_tensor_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#             train,test= train_test_split(all_images,test_size=0.2,random_state=42,shuffle=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#concatenate(prefetched_tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[1;32m    292\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 197\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n      dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for dataset in six_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"six_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(six_percent_additional_train_dataset_rand_1))\n",
    "print(len(six_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in seven_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"seven_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16050\n",
      "16050\n"
     ]
    }
   ],
   "source": [
    "print(len(seven_percent_additional_train_dataset_rand_1))\n",
    "print(len(seven_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in eight_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"eight_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200\n",
      "16200\n"
     ]
    }
   ],
   "source": [
    "print(len(eight_percent_additional_train_dataset_rand_1))\n",
    "print(len(eight_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in nine_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"nine_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16350\n",
      "16350\n"
     ]
    }
   ],
   "source": [
    "print(len(nine_percent_additional_train_dataset_rand_1))\n",
    "print(len(nine_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for dataset in ten_percent_datasets:\n",
    "    i += 1 \n",
    "    str_var = \"ten_percent_additional_train_dataset_rand_\" + str(i) \n",
    "    locals()[str_var]= add_new_data(dataset, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n",
      "16500\n"
     ]
    }
   ],
   "source": [
    "print(len(ten_percent_additional_train_dataset_rand_1))\n",
    "print(len(ten_percent_additional_train_dataset_rand_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_quarter_percent_additional_train_datasets = [one_quarter_percent_additional_train_dataset_rand_1, one_quarter_percent_additional_train_dataset_rand_2, \\\n",
    "                                        one_quarter_percent_additional_train_dataset_rand_3, one_quarter_percent_additional_train_dataset_rand_4, \\\n",
    "                                        one_quarter_percent_additional_train_dataset_rand_5, one_quarter_percent_additional_train_dataset_rand_6, \\\n",
    "                                        one_quarter_percent_additional_train_dataset_rand_7, one_quarter_percent_additional_train_dataset_rand_8, \\\n",
    "                                        one_quarter_percent_additional_train_dataset_rand_9, one_quarter_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_percent_additional_train_datasets = [half_percent_additional_train_dataset_rand_1, half_percent_additional_train_dataset_rand_2, \\\n",
    "                                        half_percent_additional_train_dataset_rand_3, half_percent_additional_train_dataset_rand_4, \\\n",
    "                                        half_percent_additional_train_dataset_rand_5, half_percent_additional_train_dataset_rand_6, \\\n",
    "                                        half_percent_additional_train_dataset_rand_7, half_percent_additional_train_dataset_rand_8, \\\n",
    "                                        half_percent_additional_train_dataset_rand_9, half_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_quarter_percent_additional_train_datasets = [three_quarter_percent_additional_train_dataset_rand_1, three_quarter_percent_additional_train_dataset_rand_2, \\\n",
    "                                        three_quarter_percent_additional_train_dataset_rand_3, three_quarter_percent_additional_train_dataset_rand_4, \\\n",
    "                                        three_quarter_percent_additional_train_dataset_rand_5, three_quarter_percent_additional_train_dataset_rand_6, \\\n",
    "                                        three_quarter_percent_additional_train_dataset_rand_7, three_quarter_percent_additional_train_dataset_rand_8, \\\n",
    "                                        three_quarter_percent_additional_train_dataset_rand_9, three_quarter_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_percent_additional_train_datasets = [one_percent_additional_train_dataset_rand_1, one_percent_additional_train_dataset_rand_2, \\\n",
    "                                        one_percent_additional_train_dataset_rand_3, one_percent_additional_train_dataset_rand_4, \\\n",
    "                                        one_percent_additional_train_dataset_rand_5, one_percent_additional_train_dataset_rand_6, \\\n",
    "                                        one_percent_additional_train_dataset_rand_7, one_percent_additional_train_dataset_rand_8, \\\n",
    "                                        one_percent_additional_train_dataset_rand_9, one_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_percent_additional_train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_percent_additional_train_datasets = [two_percent_additional_train_dataset_rand_1, two_percent_additional_train_dataset_rand_2, \\\n",
    "                                        two_percent_additional_train_dataset_rand_3, two_percent_additional_train_dataset_rand_4, \\\n",
    "                                        two_percent_additional_train_dataset_rand_5, two_percent_additional_train_dataset_rand_6, \\\n",
    "                                        two_percent_additional_train_dataset_rand_7, two_percent_additional_train_dataset_rand_8, \\\n",
    "                                        two_percent_additional_train_dataset_rand_9, two_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_percent_additional_train_datasets = [three_percent_additional_train_dataset_rand_1, three_percent_additional_train_dataset_rand_2, \\\n",
    "                                        three_percent_additional_train_dataset_rand_3, three_percent_additional_train_dataset_rand_4, \\\n",
    "                                        three_percent_additional_train_dataset_rand_5, three_percent_additional_train_dataset_rand_6, \\\n",
    "                                        three_percent_additional_train_dataset_rand_7, three_percent_additional_train_dataset_rand_8, \\\n",
    "                                        three_percent_additional_train_dataset_rand_9, three_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_percent_additional_train_datasets = [four_percent_additional_train_dataset_rand_1, four_percent_additional_train_dataset_rand_2, \\\n",
    "                                        four_percent_additional_train_dataset_rand_3, four_percent_additional_train_dataset_rand_4, \\\n",
    "                                        four_percent_additional_train_dataset_rand_5, four_percent_additional_train_dataset_rand_6, \\\n",
    "                                        four_percent_additional_train_dataset_rand_7, four_percent_additional_train_dataset_rand_8, \\\n",
    "                                        four_percent_additional_train_dataset_rand_9, four_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_percent_additional_train_datasets = [five_percent_additional_train_dataset_rand_1, five_percent_additional_train_dataset_rand_2, \\\n",
    "                                        five_percent_additional_train_dataset_rand_3, five_percent_additional_train_dataset_rand_4, \\\n",
    "                                        five_percent_additional_train_dataset_rand_5, five_percent_additional_train_dataset_rand_6, \\\n",
    "                                        five_percent_additional_train_dataset_rand_7, five_percent_additional_train_dataset_rand_8, \\\n",
    "                                        five_percent_additional_train_dataset_rand_9, five_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_percent_additional_train_datasets = [six_percent_additional_train_dataset_rand_1, six_percent_additional_train_dataset_rand_2, \\\n",
    "                                        six_percent_additional_train_dataset_rand_3, six_percent_additional_train_dataset_rand_4, \\\n",
    "                                        six_percent_additional_train_dataset_rand_5, six_percent_additional_train_dataset_rand_6, \\\n",
    "                                        six_percent_additional_train_dataset_rand_7, six_percent_additional_train_dataset_rand_8, \\\n",
    "                                        six_percent_additional_train_dataset_rand_9, six_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_percent_additional_train_datasets = [seven_percent_additional_train_dataset_rand_1, seven_percent_additional_train_dataset_rand_2, \\\n",
    "                                        seven_percent_additional_train_dataset_rand_3, seven_percent_additional_train_dataset_rand_4, \\\n",
    "                                        seven_percent_additional_train_dataset_rand_5, seven_percent_additional_train_dataset_rand_6, \\\n",
    "                                        seven_percent_additional_train_dataset_rand_7, seven_percent_additional_train_dataset_rand_8, \\\n",
    "                                        seven_percent_additional_train_dataset_rand_9, seven_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_percent_additional_train_datasets = [eight_percent_additional_train_dataset_rand_1, eight_percent_additional_train_dataset_rand_2, \\\n",
    "                                        eight_percent_additional_train_dataset_rand_3, eight_percent_additional_train_dataset_rand_4, \\\n",
    "                                        eight_percent_additional_train_dataset_rand_5, eight_percent_additional_train_dataset_rand_6, \\\n",
    "                                        eight_percent_additional_train_dataset_rand_7, eight_percent_additional_train_dataset_rand_8, \\\n",
    "                                        eight_percent_additional_train_dataset_rand_9, eight_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_percent_additional_train_datasets = [nine_percent_additional_train_dataset_rand_1, nine_percent_additional_train_dataset_rand_2, \\\n",
    "                                        nine_percent_additional_train_dataset_rand_3, nine_percent_additional_train_dataset_rand_4, \\\n",
    "                                        nine_percent_additional_train_dataset_rand_5, nine_percent_additional_train_dataset_rand_6, \\\n",
    "                                        nine_percent_additional_train_dataset_rand_7, nine_percent_additional_train_dataset_rand_8, \\\n",
    "                                        nine_percent_additional_train_dataset_rand_9, nine_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_percent_additional_train_datasets = [ten_percent_additional_train_dataset_rand_1, ten_percent_additional_train_dataset_rand_2, \\\n",
    "                                        ten_percent_additional_train_dataset_rand_3, ten_percent_additional_train_dataset_rand_4, \\\n",
    "                                        ten_percent_additional_train_dataset_rand_5, ten_percent_additional_train_dataset_rand_6, \\\n",
    "                                        ten_percent_additional_train_dataset_rand_7, ten_percent_additional_train_dataset_rand_8, \\\n",
    "                                        ten_percent_additional_train_dataset_rand_9, ten_percent_additional_train_dataset_rand_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_at_thresholds(dataset):\n",
    "    t1 = 15000 #int(percent[0])\n",
    "    t2 = len(dataset)\n",
    "    print(\"len: \", t2)\n",
    "    print(\" \" * 30)\n",
    "    c = 0\n",
    "    for i in dataset:\n",
    "        if c < 2:\n",
    "            print(\"data @ {} is {}\".format(c, i))\n",
    "            print(\" \" * 30)\n",
    "        if (c == t1) or (c == (t1-1)):\n",
    "            print(\"data @ {} is {}\".format(c, i))\n",
    "            print(\" \" * 30)\n",
    "        if (c == (t2-1)) or (c == (t2-2)):\n",
    "            print(\"*\" * 25, \"NEW\")\n",
    "            print(\"data @ {} is {}\".format(c, i))\n",
    "            print(\" \" * 30)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  15150\n",
      "                              \n",
      "data @ 0 is (<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 1 is (<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 14999 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Long, boring, blasphemous. Never have I been so glad to see ending credits roll.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 15000 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Dorothy  told us all about the recent great events.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15148 is (<tf.Tensor: shape=(), dtype=string, numpy=b'I made Anthony feel annoyed.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15149 is (<tf.Tensor: shape=(), dtype=string, numpy=b'the receptionist made me feel ecstatic.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "                              \n"
     ]
    }
   ],
   "source": [
    "print_at_thresholds(one_percent_additional_train_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  15300\n",
      "                              \n",
      "data @ 0 is (<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 1 is (<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 14999 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Long, boring, blasphemous. Never have I been so glad to see ending credits roll.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 15000 is (<tf.Tensor: shape=(), dtype=string, numpy=b'the tailor made me feel glad.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15298 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Barbara was angry.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15299 is (<tf.Tensor: shape=(), dtype=string, numpy=b'this woman was furious.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n"
     ]
    }
   ],
   "source": [
    "print_at_thresholds(two_percent_additional_train_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  15450\n",
      "                              \n",
      "data @ 0 is (<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 1 is (<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 14999 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Long, boring, blasphemous. Never have I been so glad to see ending credits roll.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 15000 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Steven made me feel anxious.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15448 is (<tf.Tensor: shape=(), dtype=string, numpy=b'my sister made me feel irritated.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15449 is (<tf.Tensor: shape=(), dtype=string, numpy=b'the writer feels excited.'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "                              \n"
     ]
    }
   ],
   "source": [
    "print_at_thresholds(three_percent_additional_train_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  15600\n",
      "                              \n",
      "data @ 0 is (<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 1 is (<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 14999 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Long, boring, blasphemous. Never have I been so glad to see ending credits roll.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 15000 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Emily was miserable.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15598 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Matthew made me feel furious.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15599 is (<tf.Tensor: shape=(), dtype=string, numpy=b'The conversation with my son was grim.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n"
     ]
    }
   ],
   "source": [
    "print_at_thresholds(four_percent_additional_train_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  15750\n",
      "                              \n",
      "data @ 0 is (<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 1 is (<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 14999 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Long, boring, blasphemous. Never have I been so glad to see ending credits roll.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 15000 is (<tf.Tensor: shape=(), dtype=string, numpy=b'this woman feel sad.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15748 is (<tf.Tensor: shape=(), dtype=string, numpy=b'my mother made me feel terrified.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15749 is (<tf.Tensor: shape=(), dtype=string, numpy=b'the librarian found herself in a/an terrifying situation.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n"
     ]
    }
   ],
   "source": [
    "print_at_thresholds(five_percent_additional_train_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  15900\n",
      "                              \n",
      "data @ 0 is (<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 1 is (<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 14999 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Long, boring, blasphemous. Never have I been so glad to see ending credits roll.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 15000 is (<tf.Tensor: shape=(), dtype=string, numpy=b'this woman told us all about the recent gloomy events.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15898 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Katie told us all about the recent serious events.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 15899 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Brian told us all about the recent horrible events.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n"
     ]
    }
   ],
   "source": [
    "print_at_thresholds(six_percent_additional_train_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  16200\n",
      "                              \n",
      "data @ 0 is (<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 1 is (<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 14999 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Long, boring, blasphemous. Never have I been so glad to see ending credits roll.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "data @ 15000 is (<tf.Tensor: shape=(), dtype=string, numpy=b'The situation makes the auditor feel enraged.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 16198 is (<tf.Tensor: shape=(), dtype=string, numpy=b'Nancy feel depressed.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n",
      "************************* NEW\n",
      "data @ 16199 is (<tf.Tensor: shape=(), dtype=string, numpy=b'this boy feel angry.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "                              \n"
     ]
    }
   ],
   "source": [
    "print_at_thresholds(eight_percent_additional_train_dataset_rand_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically create directories for saving models\n",
    "def create_model_subdir(dataset):\n",
    "    s = dataset.split(\"percent\")\n",
    "    target_dir = 'saved_model/' + s[0] + \"percent_models\"\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), target_dir)):\n",
    "        sub_dir = target_dir.split(\"/\")\n",
    "        k = os.getcwd()\n",
    "        for dir_loc in sub_dir:\n",
    "            k = os.path.join(k, dir_loc)\n",
    "            if not os.path.exists(str(k)):\n",
    "                os.mkdir(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Explore the data \n",
    "\n",
    "Let's take a moment to understand the format of the data. Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\n",
    "\n",
    "Let's print first 10 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [],
   "source": [
    "# train_examples_batch, train_labels_batch = next(iter(one_percent_additional_train_dataset_rand_1.batch(10)))\n",
    "# train_examples_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFtaCHTdc-GY"
   },
   "source": [
    "Let's also print the first 10 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvAjVXOWc6Mj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_labels_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "The neural network is created by stacking layers—this requires three main architectural decisions:\n",
    "\n",
    "* How to represent the text?\n",
    "* How many layers to use in the model?\n",
    "* How many *hidden units* to use for each layer?\n",
    "\n",
    "In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
    "\n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
    "\n",
    "*   we don't have to worry about text preprocessing,\n",
    "*   we can benefit from transfer learning,\n",
    "*   the embedding has a fixed size, so it's simpler to process.\n",
    "\n",
    "For this example we will use a **pre-trained text embedding model** from [TensorFlow Hub](https://www.tensorflow.org/hub) called [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1).\n",
    "\n",
    "There are three other pre-trained models to test for the sake of this tutorial:\n",
    "\n",
    "* [google/tf2-preview/gnews-swivel-20dim-with-oov/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1) - same as [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1), but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
    "* [google/tf2-preview/nnlm-en-dim50/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1) - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
    "* [google/tf2-preview/nnlm-en-dim128/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1) - Even larger model with ~1M vocabulary size and 128 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In2nDpTLkgKa"
   },
   "source": [
    "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NUbzVeYkgcO"
   },
   "outputs": [],
   "source": [
    "# embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "# hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                            dtype=tf.string, trainable=True)\n",
    "# hub_layer(train_examples_batch[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfSbV6igl1EH"
   },
   "source": [
    "Let's now build the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpKOoWgu-llD"
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential()\n",
    "# model.add(hub_layer)\n",
    "# model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PbKQ6mucuKL"
   },
   "source": [
    "The layers are stacked sequentially to build the classifier:\n",
    "\n",
    "1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that we are using ([google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n",
    "2. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
    "3. The last layer is densely connected with a single output node.\n",
    "\n",
    "Let's compile the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4EqVWg4-llM"
   },
   "source": [
    "### Loss function and optimizer\n",
    "\n",
    "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs logits (a single-unit layer with a linear activation), we'll use the `binary_crossentropy` loss function.\n",
    "\n",
    "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
    "\n",
    "Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n",
    "\n",
    "Now, configure the model to use an optimizer and a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = ten_percent_additional_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.shuffle(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 0.25% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import gc\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"one_quarter_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in one_quarter_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_one_quarter_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 0.5% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"half_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in half_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "       \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_half_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 0.75% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"three_quarter_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in three_quarter_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_three_quarter_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## Train 10 models with 1% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"one_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in one_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_one_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 2% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"two_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in two_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_two_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 3% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"three_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in three_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_three_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 4% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"four_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in four_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_four_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 5% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"five_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in five_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_five_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 6% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "\n",
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : \n",
    "# (1) DD this code to find the OOM instruction causing the kernel crash \n",
    "# (2) jupyter nbconvert to python and run for 6% to test. ... \n",
    "# (3) try running on GCP first - !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_text\n",
    "# https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\n",
    "# https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train,test= train_test_split(all_images,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.ConcatenateDataset'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConcatenateDataset' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-e498c21d072d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_10_20_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train[150:15900]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_examples_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConcatenateDataset' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "datasets =\"six_percent_additional_train_datasets\"\n",
    "create_model_subdir(datasets)\n",
    "s = datasets.split(\"percent\")\n",
    "pth = s[0] + \"percent_models\"\n",
    "\n",
    "for dataset in six_percent_additional_train_datasets:\n",
    "    i +=1 \n",
    "    print(type(dataset))\n",
    "#     dataset, _ = train_test_split(dataset,test_size=0.2,random_state=42,shuffle=True) #slice_dict_randomly(dataset, 15750)\n",
    "    \n",
    "    \n",
    "#     train_10_20_ds = dataset.load(_, split='train[150:15900]')\n",
    "    \n",
    "    train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "    embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "    hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                               dtype=tf.string, trainable=True)\n",
    "    hub_layer(train_examples_batch[:3])\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    \n",
    "    history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "                    epochs=20,\n",
    "                    validation_data=validation_data.batch(512),\n",
    "                    verbose=0) #, use_multiprocessing=False)\n",
    "\n",
    "    break\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(56), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_six_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 7% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"seven_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in seven_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_seven_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 8% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"eight_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in eight_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_eight_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 9% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"nine_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in nine_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_nine_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10 models with 10% of new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gc.collect()\n",
    "\n",
    "# class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs=None):\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# datasets =\"ten_percent_additional_train_datasets\"\n",
    "# create_model_subdir(datasets)\n",
    "# s = datasets.split(\"percent\")\n",
    "# pth = s[0] + \"percent_models\"\n",
    "\n",
    "# for dataset in ten_percent_additional_train_datasets:\n",
    "#     i +=1 \n",
    "    \n",
    "#     train_examples_batch, train_labels_batch = next(iter(dataset.batch(10)))\n",
    "    \n",
    "#     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#     hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                                dtype=tf.string, trainable=True)\n",
    "#     hub_layer(train_examples_batch[:3])\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(hub_layer)\n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     history = model.fit(dataset.shuffle(10000).batch(512),\n",
    "#                     epochs=20,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=0) #, use_multiprocessing=True)\n",
    "    \n",
    "#     results = model.evaluate(test_data.batch(512), verbose=0)\n",
    "\n",
    "#     print(\"RESULTS for dataset - {} with rand id #: {}\".format(datasets, str(i)))\n",
    "#     for name, value in zip(model.metrics_names, results):\n",
    "#       print(\"%s: %.3f\" % (name, value))\n",
    "    \n",
    "#     model_path = \"saved_model/\" + pth + \"/my_model_txt_classifier_hub_with_ten_percent_extra_data\" + \"_rand_\" + str(i)\n",
    "#     model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_with_hub.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
