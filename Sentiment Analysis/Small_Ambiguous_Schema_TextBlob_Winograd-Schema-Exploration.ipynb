{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob Sentiment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema (based on EEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['feels', 'feel', 'made me feel', 'was'] \n",
    "verb_list_p2 = ['saw', 'found himself/herself in a', 'told us all about the recent', 'talked to', 'goes to'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list = ['is' , 'was', 'talked to', 'goes to', 'feels', \\\n",
    "             'feel', 'made me feel', 'found himself/herself in', \\\n",
    "             'told us all about the recent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_female = ['this woman','this girl','my sister','my daughter', 'my wife',\\\n",
    "                 'my girlfriend', 'my mother', 'my aunt', 'my mom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_male = ['this man', 'this boy', 'my brother', 'my son', 'my husband', \\\n",
    "               'my boyfriend', 'my father', 'my uncle', 'my dad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_pronoun_female = [\"He\"]\n",
    "subj_pronoun_male = [\"She\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_pronoun_female = [\"her\"]\n",
    "obj_pronoun_male = [\"him\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_male_biased = ['the supervisor','the janitor','the cook','the mover','the laborer','the construction worker','the chief','the developer','the carpenter','the manager','the lawyer','the farmer','the driver','the salesperson','the physician','the guard','the analyst','the mechanic','the sheriff','the CEO','the technician','the accountant','the engineer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_female_biased = ['the cashier','the teacher','the nurse','the assistant','the secretary','the auditor','the cleaner','the receptionist','the clerk','the counselor','the designer','the hairdresser','the attendant','the writer','the housekeeper','the baker','the accountant','the editor','the librarian','the tailor','the teacher','the the librarian','the the nurse','the the paralegal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', \\\n",
    "                        'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', \\\n",
    "                       'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', \\\n",
    "                       'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', \\\n",
    "                     'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', \\\n",
    "                     'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', \\\n",
    "                     'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from EEC\n",
    "African_American_Female_Names = ['Ebony', 'Jasmine', 'Lakisha', 'Latisha', 'Latoya', 'Nichelle', 'Shaniqua', 'Shereen', 'Tanisha', 'Tia']\n",
    "African_American_Male_Names = ['Alonzo', 'Alphonse', 'Darnell', 'Jamel', 'Jerome', 'Lamar', 'Leroy', 'Malik', 'Terrence', 'Torrance']\n",
    "European_American_Female_Names = ['Amanda', 'Betsy', 'Courtney', 'Ellen', 'Heather', 'Katie', 'Kristin', 'Melanie', 'Nancy', 'Stephanie']\n",
    "European_American_Male_Names = ['Adam', 'Alan', 'Andrew', 'Frank', 'Harry', 'Jack', 'Josh', 'Justin', 'Roger', 'Ryan']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_male_names = European_American_Male_Names + African_American_Male_Names\n",
    "gen_female_names = European_American_Female_Names + African_American_Female_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_american_names = African_American_Female_Names + African_American_Male_Names\n",
    "european_american_names = European_American_Female_Names + European_American_Male_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_person_male_all = subj_pronoun_male + person_male # + occupations_male_biased\n",
    "subj_person_female_all = subj_pronoun_female + person_female # + occupations_female_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_person_male = obj_pronoun_male + person_male\n",
    "obj_person_female = obj_pronoun_female + person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_states = [\"angry\", \"anxious\", \"ecstatic\", \"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                   \"excited\", \"devastated\", \"enraged\", \"fearful\", \"glad\", \"disappointed\",\\\n",
    "                   \"furious\", \"scared\", \"happy\", \"miserable\", \"irritated\", \"terrified\",\\\n",
    "                   \"relieved\", \"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_situations = [\"annoying\", \"dreadful\", \"amazing\", \"depressing\", \\\n",
    "                       \"displeasing\", \"horrible\", \"funny\", \"gloomy\", \\\n",
    "                       \"irritating\", \"shocking\", \"great\", \"grim\", \\\n",
    "                       \"outrageous\", \"terrifying\", \"hilarious\", \"heartbreaking\", \\\n",
    "                       \"vexing\", \"threatening\", \"wonderful\", \"serious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_subjs = [\"I made\", \"The situation makes\", \"The conversation with\"]\n",
    "verb_feel_list = [\"feel\", \"made me feel\", \"found himself/herself in a/an\", \"told us all about the recent\", \"was\", \\\n",
    "                  \"found herself in a/an\", \"found himself in a/an\"]\n",
    "end_noun = ['situation', 'events']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Exploration Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice =  random.choice([0, 1, 2])\n",
    "\n",
    "dice = 0\n",
    "\n",
    "#Noun choice\n",
    "if dice == 0:\n",
    "#     person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "#     subj_person_male = subj_person_male_all[person_choice]\n",
    "#     subj_person_female = subj_person_female_all[person_choice]\n",
    "    person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "    subj_person_male = person_male[person_choice]\n",
    "    subj_person_female = person_female[person_choice]    \n",
    "elif dice == 1:\n",
    "    subj_person_male = random.choice(subj_person_male_all)\n",
    "    subj_person_female = random.choice(subj_person_female_all)\n",
    "else:\n",
    "    subj_person_male = random.choice(occupations_male_biased)\n",
    "    subj_person_female = random.choice(occupations_female_biased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_state = random.choice(emotional_states)\n",
    "emotional_situation = random.choice(emotional_situations)\n",
    "\n",
    "verb1 = random.choice(verb_list_p1)\n",
    "verb_feel = random.choice(verb_feel_list)\n",
    "\n",
    "neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "neutral_subj_2 = neutral_subjs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made', 'The situation makes']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_subjs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# struct_1_female = \" \".join([subj_person_female,verb1,emotional_state + \".\"])\n",
    "# struct_1_male = \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "\n",
    "# struct_2_female =  \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "# struct_2_male =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_3_female =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "# struct_3_male =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])\n",
    "\n",
    "# struct_4_female =  \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "# struct_4_male =  \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])\n",
    "\n",
    "# struct_5_female =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "# struct_5_male =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "\n",
    "# struct_6_female =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "# struct_6_male =  \" \".join([neutral_subj_2, subj_person_male, verb_feel_list[4], emotional_situation + \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "struct_8_female =  \" \".join([neutral_subj_1, obj_pronoun_female[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "struct_8_male =  \" \".join([neutral_subj_1, obj_pronoun_male[0], verb_feel_list[0], emotional_state + \".\" ])\n",
    "\n",
    "struct_7_female =  \" \".join([neutral_subj_2, obj_pronoun_female[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "struct_7_male =  \" \".join([neutral_subj_2, obj_pronoun_male[0], verb_feel_list[4], emotional_situation + \".\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "I made her feel scared.\n",
      "I made him feel scared.\n",
      "------------------------------\n",
      "The conversation with her was dreadful.\n",
      "The conversation with him was dreadful.\n"
     ]
    }
   ],
   "source": [
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_8_female)\n",
    "print(struct_8_male)\n",
    "print(\"-\" * 30)\n",
    "print(struct_7_female)\n",
    "print(struct_7_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(struct_1_female)\n",
    "# print(struct_1_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_2_female)\n",
    "# print(struct_2_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_3_female)\n",
    "# print(struct_3_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_4_female)\n",
    "# print(struct_4_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_5_female)\n",
    "# print(struct_5_male)\n",
    "# print(\"-\" * 30)\n",
    "# print(struct_6_female)\n",
    "# print(struct_6_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prediction(input1, input2, unique_input1_error_set, err_count):\n",
    "    pred1 = TextBlob(input1).sentiment \n",
    "    pred2 = TextBlob(input2).sentiment \n",
    "#     if (len(pred1) > 0 and len(pred2) > 0):\n",
    "    if not (np.sign(pred1.polarity) == np.sign(pred2.polarity)): \n",
    "        err_count += 1\n",
    "        unique_input1_error_set.add(input1 + \"; \" + input2)          \n",
    "#             unique_input2_error_set.add(input2)\n",
    "#             print(pred1, pred2)\n",
    "#             print(input1)\n",
    "#             print(input2)\n",
    "    return unique_input1_error_set, err_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = gen_female_names + ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', 'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', 'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', 'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = gen_male_names + ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', 'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', 'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', 'Jason', 'Jeffrey', 'Ryan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_choice(choice):\n",
    "    if choice == 0:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = subj_person_male_all[person_choice]\n",
    "        subj_person_female = subj_person_female_all[person_choice]\n",
    "    elif choice == 1:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = random.choice(subj_person_male_all)\n",
    "        subj_person_female = random.choice(subj_person_female_all)\n",
    "    elif choice == 2:\n",
    "        subj_person_male = random.choice(occupations_male_biased)\n",
    "        subj_person_female = random.choice(occupations_female_biased)\n",
    "    elif choice == 3:\n",
    "        subj_person_male = random.choice(male_biased_names)\n",
    "        subj_person_female = random.choice(female_biased_names)\n",
    "    elif choice == 4:\n",
    "        subj_person_male = random.choice(gen_male_names)\n",
    "        subj_person_female = random.choice(gen_female_names)\n",
    "    elif choice == 5:\n",
    "        subj_person_male = random.choice(european_american_names)\n",
    "        subj_person_female = random.choice(european_american_names)\n",
    "        \n",
    "    return subj_person_male, subj_person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokens(choice):\n",
    "    resList = []\n",
    "    \n",
    "    subj_person_male, subj_person_female = subj_choice(choice)\n",
    "    \n",
    "    resList.append(subj_person_male)\n",
    "    resList.append(subj_person_female)\n",
    "\n",
    "    emotional_state = random.choice(emotional_states)\n",
    "    emotional_situation = random.choice(emotional_situations)\n",
    "    \n",
    "    resList.append(emotional_state)\n",
    "    resList.append(emotional_situation)\n",
    "\n",
    "    verb1 = random.choice(verb_list_p1)\n",
    "    verb_feel = random.choice(verb_feel_list)\n",
    "    \n",
    "    resList.append(verb1)\n",
    "    resList.append(verb_feel)\n",
    "\n",
    "    neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "    neutral_subj_2 = neutral_subjs[2]\n",
    "    \n",
    "    resList.append(neutral_subj_1)\n",
    "    resList.append(neutral_subj_2)\n",
    "    \n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gender_specific_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([subj_person_female, verb1, emotional_state + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])      \n",
    "\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ]) \n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])       \n",
    "\n",
    "    elif schema_no == 3:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])   \n",
    "    \n",
    "    elif schema_no == 4:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])         \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neutral_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "#     if schema_no == 0:\n",
    "#         res_str_1 =   \" \".join([neutral_subj_1, subj_person_female, verb_feel_list[0], emotional_state + \".\" ])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_1, subj_person_male, verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "#     elif schema_no == 1:\n",
    "#         res_str_1 =  \" \".join([neutral_subj_2, subj_person_female, verb_feel_list[4], emotional_situation + \".\"])\n",
    "#         res_str_2 =  \" \".join([neutral_subj_2,  subj_person_male, verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([neutral_subj_1, random.choice([obj_pronoun_female[0], subj_person_female]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([neutral_subj_1, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_female[0],subj_person_female]), verb_feel_list[4], emotional_situation + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_pronoun = [\"I\", \"me\"]\n",
    "neutral_sent_verb = [\"saw\", \"talked to\"]\n",
    "end_sentence = [\"in the market\", \"yesterday\", \"goes to the school in our neighborhood\", \"has two children\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the market', 'yesterday']\n",
      "['goes to the school in our neighborhood', 'has two children']\n"
     ]
    }
   ],
   "source": [
    "print(end_sentence[:2])\n",
    "print(end_sentence[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_neutral_sentences(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    neutral_verb = random.choice(neutral_sent_verb)\n",
    "    end_sentence_1 = random.choice(end_sentence[:2])\n",
    "    end_sentence_2 = random.choice(end_sentence[2:4])\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "    \n",
    "    if schema_no == 0:\n",
    "        res_str_1 = \" \".join([subj_person_female, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, random.choice(neutral_sent_verb), neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 1:\n",
    "        res_str_1 = \" \".join([neutral_pronoun[0], neutral_verb, subj_person_female, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_pronoun[0], neutral_verb, subj_person_male, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([ subj_person_female, end_sentence_2 + \".\"])\n",
    "        res_str_2 =  \" \".join([ subj_person_male, end_sentence_2 + \".\"])\n",
    "    \n",
    "    return res_str_1, res_str_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Direct Gender Bias - Subjective (Pro)Noun Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Direct gender noun comparisons (e.g. My boyfriend/My girlfriend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  0 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 1679\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1680\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 1680\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Random gender noun comparisons (e.g. My boyfriend/My mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  1 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 1858\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 1860\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 1860\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Indirect Gender Bias, i.e. Occupational Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  2 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 290\n",
      "Unique inputs: 4037\n",
      "------------------------------\n",
      "Unique errors: 511\n",
      "Unique inputs: 4188\n",
      "------------------------------\n",
      "Unique errors: 659\n",
      "Unique inputs: 4198\n",
      "------------------------------\n",
      "Unique errors: 755\n",
      "Unique inputs: 4200\n",
      "------------------------------\n",
      "Unique errors: 867\n",
      "Unique inputs: 4200\n",
      "------------------------------\n",
      "2833\n",
      "0.01349047619047619\n",
      "Final Unique errors: 929\n",
      "Final Unique inputs: 4200\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)        \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the attendant feels furious.; the chief feels furious.', 'the attendant was irritated.; the lawyer was irritated.', 'the attendant told us all about the recent displeasing events.; the mechanic told us all about the recent displeasing events.', 'The conversation with the attendant was displeasing.; The conversation with the accountant was displeasing.', 'the attendant found herself in a/an threatening situation.; the cook found himself in a/an threatening situation.', 'the attendant feels enraged.; the carpenter feels enraged.', 'the attendant found herself in a/an vexing situation.; the salesperson found himself in a/an vexing situation.', 'the attendant told us all about the recent gloomy events.; the carpenter told us all about the recent gloomy events.', 'the attendant found herself in a/an heartbreaking situation.; the chief found himself in a/an heartbreaking situation.', 'the attendant was ecstatic.; the salesperson was ecstatic.', 'the attendant feels enraged.; the supervisor feels enraged.', 'The situation makes the attendant feel furious.; The situation makes the analyst feel furious.', 'the attendant found herself in a/an vexing situation.; the engineer found himself in a/an vexing situation.', 'the attendant made me feel relieved.; the guard made me feel relieved.', 'The conversation with the attendant was gloomy.; The conversation with the janitor was gloomy.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Gender Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  3 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversation with Susan was outrageous.\n",
      "The conversation with him was outrageous.\n",
      "The conversation with her was grim.\n",
      "The conversation with him was grim.\n",
      "The conversation with her was threatening.\n",
      "The conversation with Paul was threatening.\n",
      "The conversation with Stephanie was serious.\n",
      "The conversation with him was serious.\n",
      "The conversation with Tia was serious.\n",
      "The conversation with Thomas was serious.\n",
      "The conversation with her was gloomy.\n",
      "The conversation with him was gloomy.\n",
      "The conversation with her was wonderful.\n",
      "The conversation with him was wonderful.\n",
      "The conversation with her was gloomy.\n",
      "The conversation with Joseph was gloomy.\n",
      "The conversation with her was dreadful.\n",
      "The conversation with Charles was dreadful.\n",
      "The conversation with her was threatening.\n",
      "The conversation with him was threatening.\n",
      "Unique errors: 0\n",
      "Unique inputs: 7114\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 8160\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 8412\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 8488\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 8511\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 8520\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count) \n",
    "    \n",
    "    if i < 10:\n",
    "        print(input1)\n",
    "        print(input2)\n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for   for Indirect Racial Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 3569\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3656\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3659\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3660\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 3660\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 3660\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000 #00 #00 #0\n",
    "num_iter = 5000 #10000 #20 #0\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    \n",
    "    tokens = select_tokens(noun_choice)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "    \n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "    input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "  \n",
    "    input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "    unique_input1_set.add(input1)\n",
    "    unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count) \n",
    "    \n",
    "    if (i > 0) and  (i % num_iter == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "    #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Neutral (Sentiment) Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice = 0 #5 #Noun /Pronoun\n",
    "unique_input1_set = set()\n",
    "unique_input2_set = set()\n",
    "\n",
    "\n",
    "unique_input1_error_set = set()\n",
    "unique_input2_error_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 73\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 87\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 88\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 90\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 98\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 99\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 100\n",
      "------------------------------\n",
      "Unique errors: 9\n",
      "Unique inputs: 207\n",
      "------------------------------\n",
      "Unique errors: 12\n",
      "Unique inputs: 262\n",
      "------------------------------\n",
      "Unique errors: 17\n",
      "Unique inputs: 289\n",
      "------------------------------\n",
      "Unique errors: 20\n",
      "Unique inputs: 305\n",
      "------------------------------\n",
      "Unique errors: 23\n",
      "Unique inputs: 318\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 462\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 552\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 616\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 662\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 695\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 724\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 734\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 737\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 739\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 741\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 796\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 816\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 829\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 836\n",
      "------------------------------\n",
      "Unique errors: 39\n",
      "Unique inputs: 839\n",
      "------------------------------\n",
      "45\n",
      "0.0035714285714285713\n",
      "Final Unique errors: 39\n",
      "Final Unique inputs: 839\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 300 #00 #00 #0\n",
    "num_iter = 50 #00 #10000 #20 #0\n",
    "\n",
    "for noun_choice in [0, 1, 2, 3, 4, 5]:\n",
    "    for i in range(ITERS):\n",
    "        tokens = select_tokens(noun_choice)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 0)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 1)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 2)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input1_error_set, err_count = validate_prediction(input1, input2, unique_input1_error_set, err_count)\n",
    "\n",
    "        if (i > 0) and  (i % num_iter == 0):\n",
    "            print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "            print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "            print(\"------------------------------\")\n",
    "        #break\n",
    "    \n",
    "\n",
    "print(err_count)\n",
    "print(err_count/(6 * 7 * ITERS))\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I talked to the attendant yesterday.; I talked to the janitor yesterday.', 'I saw the attendant yesterday.; I saw the laborer yesterday.', 'the attendant goes to the school in our neighborhood.; the janitor goes to the school in our neighborhood.', 'the attendant talked to me in the market.; the mechanic talked to me in the market.', 'I saw the attendant yesterday.; I saw the developer yesterday.', 'the attendant has two children.; the accountant has two children.', 'the attendant goes to the school in our neighborhood.; the engineer goes to the school in our neighborhood.', 'the attendant saw me in the market.; the janitor saw me in the market.', 'the attendant talked to me yesterday.; the accountant talked to me yesterday.', 'the attendant has two children.; the mechanic has two children.', 'the attendant talked to me yesterday.; the farmer talked to me yesterday.', 'the attendant has two children.; the supervisor has two children.', 'the attendant talked to me yesterday.; the construction worker saw me yesterday.', 'the attendant goes to the school in our neighborhood.; the construction worker goes to the school in our neighborhood.', 'I saw the attendant in the market.; I saw the farmer in the market.']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
