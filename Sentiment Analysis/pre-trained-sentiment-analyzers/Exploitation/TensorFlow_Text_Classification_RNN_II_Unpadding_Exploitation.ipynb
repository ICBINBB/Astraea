{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Text Classification RNN II Unpadding Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema (based on EEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "verb_list_p1 = ['feels', 'feel', 'made me feel', 'was'] \n",
    "verb_list_p2 = ['saw', 'found himself/herself in a', 'told us all about the recent', 'talked to', 'goes to'] \n",
    "\n",
    "verb_list = ['is' , 'was', 'talked to', 'goes to', 'feels', \\\n",
    "             'feel', 'made me feel', 'found himself/herself in', \\\n",
    "             'told us all about the recent']\n",
    "\t\t\t \n",
    "person_female = ['this woman','this girl','my sister','my daughter', 'my wife',\\\n",
    "                 'my girlfriend', 'my mother', 'my aunt', 'my mom']\n",
    "\t\t\t\t \n",
    "person_male = ['this man', 'this boy', 'my brother', 'my son', 'my husband', \\\n",
    "               'my boyfriend', 'my father', 'my uncle', 'my dad']\n",
    "\n",
    "print(set(person_male).intersection(set(person_female)))\n",
    "\n",
    "assert(len(set(person_male).intersection(set(person_female))) == 0), \\\n",
    "    \"ERROR not a disjoint set\"\n",
    "\t\t\t   \n",
    "subj_pronoun_female = [\"She\"]\n",
    "subj_pronoun_male = [\"He\"]\n",
    "\n",
    "obj_pronoun_female = [\"her\"]\n",
    "obj_pronoun_male = [\"him\"]\n",
    "\n",
    "occupations_male_biased = ['the supervisor','the janitor','the cook','the mover','the laborer','the construction worker','the chief','the developer','the carpenter','the manager','the lawyer','the farmer','the driver','the salesperson','the physician','the guard','the analyst','the mechanic','the sheriff','the CEO','the technician','the accountant','the engineer']\n",
    "\n",
    "occupations_female_biased = ['the cashier','the teacher','the nurse','the assistant','the secretary','the auditor','the cleaner','the receptionist','the clerk','the counselor','the designer','the hairdresser','the attendant','the writer','the housekeeper','the baker','the editor','the librarian','the tailor','the teacher','the the librarian','the the nurse','the the paralegal']\n",
    "\n",
    "print(set(occupations_male_biased).intersection(set(occupations_female_biased)))\n",
    "\n",
    "assert(len(set(occupations_male_biased).intersection(set(occupations_female_biased))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "# Top 30 male and female names\n",
    "#Data from (13/07/2020) https://www.ssa.gov/OACT/babynames/decades/century.html\n",
    "female_biased_names = ['Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara', 'Susan', 'Jessica', \\\n",
    "                        'Sarah', 'Karen', 'Nancy', 'Margaret', 'Lisa', 'Betty', 'Dorothy ', 'Sandra', 'Ashley', \\\n",
    "                       'Kimberly', 'Donna', 'Emily', 'Michelle', 'Carol', 'Amanda', 'Melissa' , 'Deborah', \\\n",
    "                       'Stephanie', 'Rebecca', 'Laura', 'Sharon', 'Cynthia']\n",
    "male_biased_names = ['James', 'John ', 'Robert ', 'Michael ', 'William ', 'David ', 'Richard', 'Joseph', 'Thomas', \\\n",
    "                     'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Donald', 'Mark', 'Paul', 'Steven', \\\n",
    "                     'Andrew', 'Kenneth', 'Joshua', 'George', 'Kevin', 'Brian', 'Edward', 'Ronald', 'Timothy', \\\n",
    "                     'Jason', 'Jeffrey', 'Ryan']\n",
    "\t\t\t\t\t \n",
    "print(set(female_biased_names).intersection(set(male_biased_names)))\n",
    "\n",
    "assert(len(set(female_biased_names).intersection(set(male_biased_names))) == 0), \"ERROR not a disjoint set\"\t\t\t\t\t \t\t\t\t\t \n",
    "\n",
    "#Data from EEC\n",
    "African_American_Female_Names = ['Ebony', 'Jasmine', 'Lakisha', 'Latisha', 'Latoya', 'Nichelle', 'Shaniqua', 'Shereen', 'Tanisha', 'Tia']\n",
    "African_American_Male_Names = ['Alonzo', 'Alphonse', 'Darnell', 'Jamel', 'Jerome', 'Lamar', 'Leroy', 'Malik', 'Terrence', 'Torrance']\n",
    "European_American_Female_Names = ['Amanda', 'Betsy', 'Courtney', 'Ellen', 'Heather', 'Katie', 'Kristin', 'Melanie', 'Nancy', 'Stephanie']\n",
    "European_American_Male_Names = ['Adam', 'Alan', 'Andrew', 'Frank', 'Harry', 'Jack', 'Josh', 'Justin', 'Roger', 'Ryan']\n",
    "\n",
    "\n",
    "gen_male_names = European_American_Male_Names + African_American_Male_Names\n",
    "gen_female_names = European_American_Female_Names + African_American_Female_Names\n",
    "\n",
    "print(set(gen_male_names).intersection(set(gen_female_names)))\n",
    "\n",
    "assert(len(set(gen_male_names).intersection(set(gen_female_names))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "\n",
    "african_american_names = African_American_Female_Names + African_American_Male_Names\n",
    "european_american_names = European_American_Female_Names + European_American_Male_Names\n",
    "\n",
    "print(set(african_american_names).intersection(set(european_american_names)))\n",
    "\n",
    "assert(len(set(african_american_names).intersection(set(european_american_names))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "subj_person_male_all = subj_pronoun_male + person_male # + occupations_male_biased\n",
    "subj_person_female_all = subj_pronoun_female + person_female # + occupations_female_biased\n",
    "\n",
    "print(set(subj_person_male_all).intersection(set(subj_person_female_all)))\n",
    "\n",
    "assert(len(set(subj_person_male_all).intersection(set(subj_person_female_all))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "obj_person_male = obj_pronoun_male + person_male\n",
    "obj_person_female = obj_pronoun_female + person_female\n",
    "\n",
    "print(set(obj_person_male).intersection(set(obj_person_female)))\n",
    "\n",
    "assert(len(set(obj_person_male).intersection(set(obj_person_female))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "emotional_states = [\"angry\", \"anxious\", \"ecstatic\", \"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                   \"excited\", \"devastated\", \"enraged\", \"fearful\", \"glad\", \"disappointed\",\\\n",
    "                   \"furious\", \"scared\", \"happy\", \"miserable\", \"irritated\", \"terrified\",\\\n",
    "                   \"relieved\", \"sad\"]\n",
    "\n",
    "positive_emotional_states = [\"ecstatic\", \"excited\", \"glad\", \"happy\", \"relieved\"]\n",
    "\n",
    "negative_emotional_states = [\"angry\", \"anxious\",\"depressed\", \"annoyed\", \"discouraged\",\\\n",
    "                             \"devastated\", \"enraged\", \"fearful\", \"disappointed\",\\\n",
    "                             \"furious\", \"scared\", \"miserable\", \"irritated\", \"terrified\", \"sad\"]\n",
    "\t\t\t\t\t\t\t \n",
    "print(set(positive_emotional_states).intersection(set(negative_emotional_states)))\n",
    "\n",
    "assert(len(set(positive_emotional_states).intersection(set(negative_emotional_states))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "emotional_situations = [\"annoying\", \"dreadful\", \"amazing\", \"depressing\", \\\n",
    "                       \"displeasing\", \"horrible\", \"funny\", \"gloomy\", \\\n",
    "                       \"irritating\", \"shocking\", \"great\", \"grim\", \\\n",
    "                       \"outrageous\", \"terrifying\", \"hilarious\", \"heartbreaking\", \\\n",
    "                       \"vexing\", \"threatening\", \"wonderful\", \"serious\"]\n",
    "\t\t\t\t\t   \n",
    "positive_emotional_situations = [\"amazing\", \"funny\", \"great\", \"hilarious\",\"wonderful\"]\n",
    "\n",
    "negative_emotional_situations = [\"annoying\", \"dreadful\", \"depressing\", \"displeasing\", \"horrible\",\\\n",
    "                                \"gloomy\", \"irritating\", \"shocking\", \"grim\", \"outrageous\", \"terrifying\", \"heartbreaking\",\\\n",
    "                                \"vexing\",  \"threatening\", \"serious\"]\n",
    "\t\t\t\t\t\t\t\t\n",
    "print(set(positive_emotional_situations).intersection(set(negative_emotional_situations)))\n",
    "\n",
    "assert(len(set(positive_emotional_situations).intersection(set(negative_emotional_situations))) == 0), \"ERROR not a disjoint set\"\n",
    "\n",
    "neutral_subjs = [\"I made\", \"The situation makes\", \"The conversation with\"]\n",
    "verb_feel_list = [\"feel\", \"made me feel\", \"found himself/herself in a/an\", \"told us all about the recent\", \"was\", \\\n",
    "                  \"found herself in a/an\", \"found himself in a/an\"]\n",
    "end_noun = ['situation', 'events']\n",
    "\n",
    "neutral_pronoun = [\"I\", \"me\"]\n",
    "neutral_sent_verb = [\"saw\", \"talked to\"]\n",
    "end_sentence = [\"in the market\", \"yesterday\", \"goes to the school in our neighborhood\", \"has two children\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_dict(D):\n",
    "    return {k: v for k, v in sorted(D.items(), key=lambda item: item[1], reverse=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate_dict(error_dict, count_dict):\n",
    "    error_rate_dict = {}\n",
    "    for key in error_dict:\n",
    "        error_rate_dict[key] = error_dict[key]/count_dict[key]\n",
    "    return get_sorted_dict(error_rate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dict(error_dict, count_dict):\n",
    "    error_rate_dict = get_error_rate_dict(error_dict, count_dict)\n",
    "    \n",
    "    probability_dict = {}\n",
    "    error_rate_sum = sum(error_rate_dict.values())\n",
    "    for error_rate in error_rate_dict:\n",
    "        probability_dict[error_rate] = error_rate_dict[error_rate]/error_rate_sum\n",
    "    \n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_random_choice(error_dict, count_dict, probablilities_dict = None):\n",
    "#     print(\"error_dict: {}\".format(error_dict))\n",
    "#     print(\"count_dict: {}\".format(count_dict))\n",
    "#     print(\"probablilities_dict: {}\".format(probablilities_dict))\n",
    "    \n",
    "    if probablilities_dict == None:\n",
    "        probability_dict = get_probability_dict(error_dict, count_dict)\n",
    "    else:\n",
    "        probability_dict = probablilities_dict\n",
    "    \n",
    "    return list(probability_dict.keys())[np.random.choice(len(list(probability_dict.keys())), p=list(probability_dict.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_schema_oracle(inp):\n",
    "    res = 0\n",
    "    token_list = inp.rstrip(\".\").split()\n",
    "    for token in token_list:\n",
    "        if (token in positive_emotional_situations) or \\\n",
    "            (token in positive_emotional_states):\n",
    "            res = 1\n",
    "            break\n",
    "        elif (token in negative_emotional_situations) or \\\n",
    "            (token in negative_emotional_states):\n",
    "            res = -1\n",
    "            break           \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error):\n",
    "    pred1 = sample_predict(rnn2_model, input1, pad=False)\n",
    "    pred2 = sample_predict(rnn2_model, input2, pad=False)\n",
    "    \n",
    "    verdict1 = run_schema_oracle(input1)\n",
    "    verdict2 = run_schema_oracle(input2)\n",
    "          \n",
    "    if tokens[0] in input2:\n",
    "        update_dict(noun_dict2, tokens[0])\n",
    "        if tokens[0] in person_female:\n",
    "            print(\"issue with tokens[0] in tokens: \", tokens)\n",
    "            print(\"input2: \", input2)\n",
    "            print(\"person_female: \", person_female)\n",
    "\n",
    "    if tokens[1] in input1:\n",
    "        update_dict(noun_dict1, tokens[1])\n",
    "        if tokens[1] in person_male:\n",
    "            print(\"issue with tokens[1] in tokens: \", tokens)\n",
    "            print(\"input1: \", input1)\n",
    "            print(\"person_male: \", person_male)\n",
    "        \n",
    "        \n",
    "        \n",
    "    obj1 = obj_pronoun_female[0] + \" \"\n",
    "    obj2 = obj_pronoun_male[0] + \" \"\n",
    "        \n",
    "    if obj1 in input1:\n",
    "        update_dict(noun_dict1, obj_pronoun_female[0])\n",
    "\n",
    "    if obj2 in input2:\n",
    "        update_dict(noun_dict2, obj_pronoun_male[0])\n",
    "     \n",
    "    assert(verdict1 == verdict2), \"ERROR: bug in run_schema_oracle() for inputs {}, {}\".format(input1, input2)\n",
    "    \n",
    "    if not ((np.sign(pred1) == np.sign(verdict1)) and (np.sign(pred2)  == np.sign(verdict2))): \n",
    "        pred_err_count += 1\n",
    "        unique_pred_input1_error_set.add(input1 + \"; \" + input2)  \n",
    "        pred_error_dict = update_counts(input1, input2, tokens, pred_error_dict)\n",
    "        bias_pair_pred_error = update_bias_pairs(input1, input2, tokens, bias_pair_pred_error)\n",
    "\n",
    "    \n",
    "    if (np.sign(pred1) != np.sign(pred2)):\n",
    "        fairness_err_count += 1\n",
    "        unique_fairness_input1_error_set.add(input1 + \"; \" + input2)\n",
    "        fairness_error_dict = update_counts(input1, input2, tokens, fairness_error_dict)\n",
    "        bias_pair_fairness_error = update_bias_pairs(input1, input2, tokens, bias_pair_fairness_error)\n",
    "\n",
    "        if tokens[0] in input2:\n",
    "            update_dict(noun_error2, tokens[0])\n",
    "\n",
    "        if tokens[1] in input1:\n",
    "            update_dict(noun_error1, tokens[1])\n",
    "            \n",
    "        if obj1 in input1:\n",
    "            update_dict(noun_error1, obj_pronoun_female[0])\n",
    "\n",
    "        if obj2 in input2:\n",
    "            update_dict(noun_error2, obj_pronoun_male[0])\n",
    "        \n",
    "        if (np.sign(pred1) != np.sign(verdict1)):\n",
    "            #assert()\n",
    "            if not (input1 in retrain_dict):\n",
    "                retrain_dict[input1] = verdict1\n",
    "        else:\n",
    "            if not (input2 in retrain_dict):\n",
    "                retrain_dict[input2] = verdict2\n",
    "        \n",
    "    return noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_dict(flag, my_dict):\n",
    "    test_female = person_female + subj_pronoun_female + obj_pronoun_female\n",
    "    test_male = person_male + subj_pronoun_male + obj_pronoun_male\n",
    "    res = {}\n",
    "    if flag == 0:\n",
    "        for i in my_dict:\n",
    "            if i in test_female:\n",
    "                res[i] = my_dict[i]\n",
    "    elif flag == 1:\n",
    "        for i in my_dict:\n",
    "            if i in test_male:\n",
    "                res[i] = my_dict[i]\n",
    "                \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_choice(choice):\n",
    "    if choice == 0:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = subj_person_male_all[person_choice]\n",
    "        subj_person_female = subj_person_female_all[person_choice]\n",
    "    elif choice == 1:\n",
    "        person_choice = random.choice(range(0, len(subj_person_male_all) - 1))\n",
    "        subj_person_male = random.choice(subj_person_male_all)\n",
    "        subj_person_female = random.choice(subj_person_female_all)\n",
    "    elif choice == 2:\n",
    "        subj_person_male = random.choice(occupations_male_biased)\n",
    "        subj_person_female = random.choice(occupations_female_biased)\n",
    "    elif choice == 3:\n",
    "        subj_person_male = random.choice(male_biased_names)\n",
    "        subj_person_female = random.choice(female_biased_names)\n",
    "    elif choice == 4:\n",
    "        subj_person_male = random.choice(gen_male_names)\n",
    "        subj_person_female = random.choice(gen_female_names)\n",
    "    elif choice == 5:\n",
    "        subj_person_male = random.choice(african_american_names)\n",
    "        subj_person_female = random.choice(european_american_names)\n",
    "    \n",
    "    return subj_person_male, subj_person_female\n",
    "\n",
    "\n",
    "def subj_choice_noun_probabilistically(choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability):\n",
    "    tmp1, tmp2 = None, None\n",
    "    if noun_error2:\n",
    "        subj_person_male = get_weighted_random_choice(noun_error2, noun_dict2, probablilities_dict=noun2_probability)\n",
    "    else:\n",
    "        subj_person_male, tmp1 = subj_choice(choice)\n",
    "    \n",
    "    if noun_error1:\n",
    "        subj_person_female = get_weighted_random_choice(noun_error1, noun_dict1, probablilities_dict=noun1_probability)\n",
    "    else:\n",
    "        tmp2, subj_person_female = subj_choice(choice)\n",
    "    \n",
    "    return subj_person_male, subj_person_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokens_probabilistically(choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability):\n",
    "    \n",
    "    resList = []\n",
    "    \n",
    "    subj_person_male, subj_person_female = subj_choice_noun_probabilistically(choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability)\n",
    "    \n",
    "    resList.append(subj_person_male)\n",
    "    resList.append(subj_person_female)\n",
    "\n",
    "    emotional_state = random.choice(emotional_states)\n",
    "    emotional_situation = random.choice(emotional_situations)\n",
    "    \n",
    "    resList.append(emotional_state)\n",
    "    resList.append(emotional_situation)\n",
    "\n",
    "    verb1 = random.choice(verb_list_p1)\n",
    "    verb_feel = random.choice(verb_feel_list)\n",
    "    \n",
    "    resList.append(verb1)\n",
    "    resList.append(verb_feel)\n",
    "\n",
    "    neutral_subj_1 = random.choice(neutral_subjs[:2])\n",
    "    neutral_subj_2 = neutral_subjs[2]\n",
    "    \n",
    "    resList.append(neutral_subj_1)\n",
    "    resList.append(neutral_subj_2)\n",
    "    \n",
    "#     print(\"resList (tokens): \", resList)\n",
    "    \n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gender_specific_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb1, emotional_state + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb1, emotional_state + \".\"])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])      \n",
    "\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[1], emotional_state + \".\" ]) \n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[1], emotional_state + \".\" ])       \n",
    "\n",
    "    elif schema_no == 3:\n",
    "        res_str_1 = \" \".join([subj_person_female, verb_feel_list[5], emotional_situation, end_noun[0] + \".\"])\n",
    "        res_str_2 = \" \".join([subj_person_male, verb_feel_list[6], emotional_situation, end_noun[0] + \".\"])   \n",
    "    \n",
    "    elif schema_no == 4:\n",
    "        res_str_1 =  \" \".join([subj_person_female, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, verb_feel_list[3], emotional_situation, end_noun[1] + \".\"])         \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neutral_subject_sentence(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "\n",
    "    if schema_no == 0:\n",
    "        res_str_1 =   \" \".join([neutral_subj_1, random.choice([obj_pronoun_female[0], subj_person_female]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "        res_str_2 =  \" \".join([neutral_subj_1, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[0], emotional_state + \".\" ])\n",
    "    \n",
    "    elif schema_no == 1:\n",
    "        res_str_1 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_female[0],subj_person_female]), verb_feel_list[4], emotional_situation + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_subj_2, random.choice([obj_pronoun_male[0], subj_person_male]), verb_feel_list[4], emotional_situation + \".\"])      \n",
    "\n",
    "    return res_str_1, res_str_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_neutral_sentences(list_tokens, verb_feel_list, schema_no):\n",
    "    \n",
    "    subj_person_male, subj_person_female, emotional_state, emotional_situation, verb1, verb_feel, \\\n",
    "        neutral_subj_1, neutral_subj_2 = list_tokens\n",
    "    \n",
    "    neutral_verb = random.choice(neutral_sent_verb)\n",
    "    end_sentence_1 = random.choice(end_sentence[:2])\n",
    "    end_sentence_2 = random.choice(end_sentence[2:4])\n",
    "    \n",
    "    res_str_1, res_str_2 = \"\", \"\"\n",
    "    \n",
    "    if schema_no == 0:\n",
    "        res_str_1 = \" \".join([subj_person_female, neutral_verb, neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([subj_person_male, neutral_verb, neutral_pronoun[1], \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 1:\n",
    "        res_str_1 = \" \".join([neutral_pronoun[0], neutral_verb, subj_person_female, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "        res_str_2 =  \" \".join([neutral_pronoun[0], neutral_verb, subj_person_male, \\\n",
    "                              end_sentence_1 + \".\"])\n",
    "    elif schema_no == 2:\n",
    "        res_str_1 = \" \".join([ subj_person_female, end_sentence_2 + \".\"])\n",
    "        res_str_2 =  \" \".join([ subj_person_male, end_sentence_2 + \".\"])\n",
    "    \n",
    "    return res_str_1, res_str_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_counts(inp1, inp2, list_tokens, list_dict):\n",
    "    for tok in list_tokens:\n",
    "        if (tok in inp1):\n",
    "            update_dict(list_dict[list_tokens.index(tok)],tok)\n",
    "        if (tok in inp2):\n",
    "            update_dict(list_dict[list_tokens.index(tok)],tok)\n",
    "    return list_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(res, list_tokens):\n",
    "    result = None\n",
    "\n",
    "    for item in list_tokens:\n",
    "        if set(res) == set(item.rstrip(\".\").split()):\n",
    "            result = item\n",
    "            break\n",
    "    return result\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bias_pairs(inp1, inp2, list_tokens, list_dict):\n",
    "    \n",
    "    for tok in list_tokens:\n",
    "        if (tok in inp1) and (tok in inp2):\n",
    "            s1 = inp1.rstrip(\".\").split()\n",
    "            s2 = inp2.rstrip(\".\").split()\n",
    "\n",
    "            res1 = list(set(s1) - set(s2))\n",
    "            res2 = list(set(s2) - set(s1))\n",
    "                        \n",
    "            if len(res1) > 1:\n",
    "                if get_token(res1, list_tokens):\n",
    "                    res1 = get_token(res1, list_tokens)\n",
    "                else:\n",
    "                    res1 = \" \".join(res1)\n",
    "            else:\n",
    "                res1 = res1[0]\n",
    "            \n",
    "            if len(res2) > 1:\n",
    "                if get_token(res2, list_tokens):\n",
    "                    res2 = get_token(res2, list_tokens)\n",
    "                else:\n",
    "                    res2 = \" \".join(res2)\n",
    "            else:\n",
    "                res2 = res2[0]\n",
    "                \n",
    "            res = res1 + \", \" + res2\n",
    "            if tok in list_dict:\n",
    "                update_dict(list_dict[list_tokens[2:].index(tok)],res)    \n",
    "    return list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically create directories for saving pickles\n",
    "def create_dir(mode):\n",
    "    target_dir = 'saved_pickles/exploitation/' + mode\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), target_dir)):\n",
    "        sub_dir = target_dir.split(\"/\")\n",
    "        k = os.getcwd()\n",
    "        for dir_loc in sub_dir:\n",
    "            k = os.path.join(k, dir_loc)\n",
    "            if not os.path.exists(str(k)):\n",
    "                os.mkdir(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_data(mode, noun_dict1, noun_dict2, noun_error1, noun_error2, unique_input1_set, unique_input2_set, unique_input_pair_set, unique_input1_error_set,\\\n",
    "            unique_input2_error_set, pred_err_count, fairness_err_count, unique_pred_input1_error_set, \\\n",
    "            unique_fairness_input1_error_set, retrain_dict, subj_person_male_count ,subj_person_female_count, emotional_state_count, \\\n",
    "                emotional_situation_count,verb_feel_count, verb1_count, neutral_subj_1_count, \\\n",
    "                neutral_subj_2_count, subj_person_male_pred_error ,subj_person_female_pred_error, emotional_state_pred_error, \\\n",
    "                emotional_situation_pred_error,verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, \\\n",
    "                neutral_subj_2_pred_error, subj_person_male_fairness_error ,subj_person_female_fairness_error, emotional_state_fairness_error, \\\n",
    "                emotional_situation_fairness_error,verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, \\\n",
    "                neutral_subj_2_fairness_error, bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error):\n",
    "\n",
    "    create_dir(mode)\n",
    "    \n",
    "    noun_data_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2]\n",
    "    noun_data_name = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\"] \n",
    "    \n",
    "    print(\"noun_dict1: \", noun_dict1)\n",
    "    print(\"noun_dict2: \", noun_dict2)\n",
    "    print(\"noun_error1: \", noun_error1)\n",
    "    print(\"noun_error2: \", noun_error2)\n",
    "    \n",
    "    assert len(noun_data_name) == len(noun_data_vals), \"ERROR: bug in variables names for stored inputs\"\n",
    "    for i in range(0, len(noun_data_vals)):\n",
    "        store = noun_data_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/exploitation/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(noun_data_vals[i], handle)\n",
    "            \n",
    "            \n",
    "    data_vals_name = [\"unique_input1_set\", \"unique_input2_set\", \"unique_input_pair_set\", \"unique_input1_error_set\",\\\n",
    "            \"unique_input2_error_set\", \"pred_err_count\", \"fairness_err_count\", \"unique_pred_input1_error_set\", \\\n",
    "            \"unique_fairness_input1_error_set\", \"retrain_dict\"]\n",
    "\n",
    "    \n",
    "#     print(\"subj_person_male_fairness_error: \", list(subj_person_male_fairness_error)[0:4])\n",
    "    \n",
    "    data_vals = [unique_input1_set, unique_input2_set, unique_input_pair_set, unique_input1_error_set,\\\n",
    "            unique_input2_error_set, pred_err_count, fairness_err_count, unique_pred_input1_error_set, \\\n",
    "            unique_fairness_input1_error_set, retrain_dict]\n",
    "    \n",
    "    token_count_vals_name = [\"subj_person_male_count\" ,\"subj_person_female_count\", \"emotional_state_count\", \\\n",
    "                \"emotional_situation_count\",\"verb_feel_count\", \"verb1_count\", \"neutral_subj_1_count\", \\\n",
    "                \"neutral_subj_2_count\"] \n",
    "\n",
    "    token_count_vals = [subj_person_male_count ,subj_person_female_count, emotional_state_count, \\\n",
    "                emotional_situation_count,verb_feel_count, verb1_count, neutral_subj_1_count, \\\n",
    "                neutral_subj_2_count] \n",
    "    \n",
    "    pred_errors_count_vals_name = [\"subj_person_male_pred_error\" ,\"subj_person_female_pred_error\", \"emotional_state_pred_error\", \\\n",
    "                \"emotional_situation_pred_error\",\"verb_feel_pred_error\", \"verb1_pred_error\", \"neutral_subj_1_pred_error\", \\\n",
    "                \"neutral_subj_2_pred_error\"] \n",
    "\n",
    "    pred_errors_count_vals = [subj_person_male_pred_error ,subj_person_female_pred_error, emotional_state_pred_error, \\\n",
    "                emotional_situation_pred_error,verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, \\\n",
    "                neutral_subj_2_pred_error] \n",
    "    \n",
    "    fairness_error_count_vals_name = [\"subj_person_male_fairness_error\" ,\"subj_person_female_fairness_error\", \"emotional_state_fairness_error\", \\\n",
    "                \"emotional_situation_fairness_error\",\"verb_feel_fairness_error\", \"verb1_fairness_error\", \"neutral_subj_1_fairness_error\", \\\n",
    "                \"neutral_subj_2_fairness_error\"] \n",
    "\n",
    "    fairness_error_count_vals = [subj_person_male_fairness_error ,subj_person_female_fairness_error, emotional_state_fairness_error, \\\n",
    "                emotional_situation_fairness_error,verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, \\\n",
    "                neutral_subj_2_fairness_error] \n",
    "    \n",
    "    bias_count_vals_name = [\"bias_pair_count\" ,\"bias_pair_pred_error\", \"bias_pair_fairness_error\"] \n",
    "\n",
    "    bias_count_vals = [bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error] \n",
    "    \n",
    "    assert len(data_vals_name) == len(data_vals), \"ERROR: bug in variables names for stored inputs\"\n",
    "    for i in range(0, len(data_vals)):\n",
    "        store = data_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/exploitation/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(data_vals[i], handle)\n",
    "    \n",
    "    \n",
    "    assert len(token_count_vals_name) == len(token_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs\".format(token_count_vals_name)\n",
    "    for i in range(0, len(token_count_vals)):\n",
    "        store = token_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/exploitation/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(token_count_vals[i], handle)\n",
    "            \n",
    "    assert len(pred_errors_count_vals_name) == len(pred_errors_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs\".format(fairness_error_count_vals_name)\n",
    "    for i in range(0, len(pred_errors_count_vals)):\n",
    "        store = pred_errors_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/exploitation/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(pred_errors_count_vals[i], handle)\n",
    "            \n",
    "            \n",
    "    assert len(fairness_error_count_vals_name) == len(fairness_error_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs {}, {}\".format(fairness_error_count_vals_name)\n",
    "    for i in range(0, len(fairness_error_count_vals)):\n",
    "        store = fairness_error_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/exploitation/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(fairness_error_count_vals[i], handle)\n",
    "    \n",
    "    \n",
    "    assert len(bias_count_vals_name) == len(bias_count_vals), \\\n",
    "        \"ERROR: bug in variables names for stored inputs {}, {}\".format(bias_count_vals_name)\n",
    "    for i in range(0, len(bias_count_vals)):\n",
    "        store = bias_count_vals_name[i] + \".pickle\"\n",
    "        with open('saved_pickles/exploitation/' + mode + '/' + store, 'wb') as handle:\n",
    "            pickle.dump(bias_count_vals[i], handle)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Classification RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mmy_model_txt_classifier_RNN\u001b[m\u001b[m\n",
      "\u001b[34mmy_model_txt_classifier_RNN_with_multiple_LSTM_layers\u001b[m\u001b[m\n",
      "\u001b[34mmy_model_txt_classifier_hub\u001b[m\u001b[m\n",
      "\u001b[34massets\u001b[m\u001b[m         saved_model.pb \u001b[34mvariables\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# my_model directory\n",
    "!ls ../saved_model\n",
    "\n",
    "# Contains an assets folder, saved_model.pb, and variables folder.\n",
    "!ls ../saved_model/my_model_txt_classifier_RNN_with_multiple_LSTM_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          523840    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         66048     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 635,329\n",
      "Trainable params: 635,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn2_model = tf.keras.models.load_model('../saved_model/my_model_txt_classifier_RNN_with_multiple_LSTM_layers')\n",
    "\n",
    "# Check its architecture\n",
    "rnn2_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model Prediction Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string is [4025, 222, 6307, 2327, 4043, 2120, 7975]\n",
      "The original string: \"Hello TensorFlow.\"\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Hello TensorFlow.'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string: \"{}\"'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_size(vec, size):\n",
    "  zeros = [0] * (size - len(vec))\n",
    "  vec.extend(zeros)\n",
    "  return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predict(model, sample_pred_text, pad):\n",
    "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "  if pad:\n",
    "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "  return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_text = \"Alex is sad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = sample_predict(rnn2_model, sample_pred_text, pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34363434]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sign(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = sample_predict(rnn2_model, sample_pred_text, pad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.9473848]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sign(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = sample_predict(rnn2_model, sample_pred_text, pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84336615]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2):\n",
    "\n",
    "    unique_input1_set = set()\n",
    "    unique_input2_set = set()\n",
    "    unique_input_pair_set = set()\n",
    "\n",
    "    unique_input1_error_set = set()\n",
    "    unique_input2_error_set = set()\n",
    "\n",
    "    pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "    unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "    retrain_dict = dict()\n",
    "\n",
    "    subj_person_male_count, subj_person_female_count, emotional_state_count, emotional_situation_count = {}, {}, {}, {}\n",
    "    verb_feel_count, verb1_count, neutral_subj_1_count, neutral_subj_2_count= {}, {}, {}, {}\n",
    "\n",
    "    subj_person_male_pred_error, subj_person_female_pred_error, emotional_state_pred_error, emotional_situation_pred_error = {}, {}, {}, {}\n",
    "    verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, neutral_subj_2_pred_error= {}, {}, {}, {}\n",
    "\n",
    "    subj_person_male_fairness_error, subj_person_female_fairness_error, emotional_state_fairness_error, emotional_situation_fairness_error = {}, {}, {}, {}\n",
    "    verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, neutral_subj_2_fairness_error= {}, {}, {}, {}\n",
    "\n",
    "    tmp1, tmp2 = 0, 0\n",
    "\n",
    "    count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "    pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "    fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "    #bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error = {}, {}, {}\n",
    "\n",
    "    bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "    bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "    bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "    \n",
    "#     noun_dict1, noun_dict2, noun_error1, noun_error2, = {}, {}, {}, {}\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    \n",
    "    noun1_probability = get_probability_dict(noun_error1, noun_dict1)\n",
    "    noun2_probability = get_probability_dict(noun_error2, noun_dict2)\n",
    "    \n",
    "#     print(\"noun1_probability: {}\".format(noun1_probability))\n",
    "#     print(\"noun2_probability: {}\".format(noun2_probability))\n",
    "    \n",
    "#     for noun_choice in [0, 1, 2, 3, 4, 5]:\n",
    "    for i in range(ITERS):\n",
    "        \n",
    "#         print(\"noun_error1: {}\".format(noun_error1))\n",
    "#         print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "        \n",
    "\n",
    "#         print(\"noun_error2: {}\".format(noun_error2))\n",
    "#         print(\"noun_dict2: {}\".format(noun_dict2))\n",
    "       \n",
    "\n",
    "        tokens = select_tokens_probabilistically(noun_choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability)\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 0)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)  \n",
    "        noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "        validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                            unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "\n",
    "        count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "        bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "        input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 0)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "        validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                            unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "\n",
    "        count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "        bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 1)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "        validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                            unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "        \n",
    "        count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "        bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 2)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "        validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                            unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "\n",
    "        count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "        bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 3)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "        validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                            unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "\n",
    "        count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "        bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "        input1, input2 = make_gender_specific_subject_sentence(tokens, verb_feel_list, 4)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "        validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                            unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "        \n",
    "        count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "        bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "        input1, input2 = make_neutral_subject_sentence(tokens, verb_feel_list, 1)\n",
    "        unique_input1_set.add(input1)\n",
    "        unique_input2_set.add(input2)\n",
    "        unique_input_pair_set.add(input1 + \"; \" + input2)\n",
    "        noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "        validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                            unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "        \n",
    "        count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "        bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "        # or (len(unique_input_pair_set) >= max_input_gen_threshold)\n",
    "        if (len(unique_input_pair_set) == tmp1 == tmp2) or (len(unique_input_pair_set) >= max_input_gen_threshold):\n",
    "            print(\"Maximum input generation threshold reached, {} unique inputs generated\".format(len(unique_input_pair_set)))\n",
    "            break\n",
    "\n",
    "        if ITERS%2 == 0:\n",
    "            tmp1 = len(unique_input_pair_set)\n",
    "        else:\n",
    "            tmp2 = len(unique_input_pair_set)\n",
    "\n",
    "        if (i > 0) and  (i % num_iter == 0):\n",
    "            print(\"ITERATION: \", str(i))\n",
    "            print(\"Unique PREDICTION errors: \" + str(len(unique_pred_input1_error_set)))\n",
    "            print(\"Unique FAIRNESS errors: \" + str(len(unique_fairness_input1_error_set)))\n",
    "            print(\"Unique length of retrain_dict: \" + str(len(retrain_dict)))  \n",
    "\n",
    "            print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "            print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "            print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "\n",
    "#                 print(\"count_dict : \", count_dict)\n",
    "#                 print(\"pred_error_dict : \", pred_error_dict)\n",
    "#                 print(\"fairness_error_dict: \", fairness_error_dict)\n",
    "\n",
    "#                 print(\"bias_pair_count : \", bias_pair_count)\n",
    "#                 print(\"bias_pair_pred_error : \", bias_pair_pred_error)\n",
    "#                 print(\"bias_pair_fairness_error: \", bias_pair_fairness_error)\n",
    "\n",
    "            print(\"-\"*20)\n",
    "\n",
    "    print(\"pred_err_count: \", pred_err_count)\n",
    "    print(\"pred_error_rate: \", pred_err_count/(7 * ITERS))\n",
    "    print(\"*\"*20)\n",
    "\n",
    "    print(\"fairness_err_count: \", fairness_err_count)\n",
    "    print(\"fairness_error_rate: \", fairness_err_count/(7 * ITERS))\n",
    "    print(\"*\"*20)\n",
    "\n",
    "    print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "    print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "    print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "    print(\"*\"*20)\n",
    "\n",
    "    print(\"Final Unique PREDICTION errors: \" + str(len(unique_pred_input1_error_set)))\n",
    "    print(\"Final Unique FAIRNESS violations: \" + str(len(unique_fairness_input1_error_set)))\n",
    "    print(\"Final length of retrain_dict: \" + str(len(retrain_dict)))\n",
    "    print(\"*\"*20)\n",
    "\n",
    "#     print(\"Final count_dict : \", count_dict)\n",
    "#     print(\"Final pred_error_dict : \", pred_error_dict)\n",
    "#     print(\"Final fairness_error_dict: \", fairness_error_dict)\n",
    "#     print(\"*\"*20)\n",
    "\n",
    "#     print(\"Final bias_pair_count : \", bias_pair_count)\n",
    "#     print(\"Final bias_pair_pred_error : \", bias_pair_pred_error)\n",
    "#     print(\"Final bias_pair_fairness_error: \", bias_pair_fairness_error)\n",
    "#     print(\"noun_dict: \", list(noun_dict)[0:4])\n",
    "#     print(\"noun_error: \", list(noun_error)[0:4])\n",
    "    \n",
    "    save_data(mode, noun_dict1, noun_dict2, noun_error1, noun_error2, unique_input1_set, unique_input2_set, unique_input_pair_set, unique_input1_error_set,\\\n",
    "            unique_input2_error_set, pred_err_count, fairness_err_count, unique_pred_input1_error_set, \\\n",
    "            unique_fairness_input1_error_set, retrain_dict, subj_person_male_count ,subj_person_female_count, emotional_state_count, \\\n",
    "                emotional_situation_count,verb_feel_count, verb1_count, neutral_subj_1_count, \\\n",
    "                neutral_subj_2_count, subj_person_male_pred_error ,subj_person_female_pred_error, emotional_state_pred_error, \\\n",
    "                emotional_situation_pred_error,verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, \\\n",
    "                neutral_subj_2_pred_error, subj_person_male_fairness_error ,subj_person_female_fairness_error, emotional_state_fairness_error, \\\n",
    "                emotional_situation_fairness_error,verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, \\\n",
    "                neutral_subj_2_fairness_error, bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neutral_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2):\n",
    "    \n",
    "    unique_input1_set = set()\n",
    "    unique_input2_set = set()\n",
    "    unique_input_pair_set = set()\n",
    "\n",
    "    unique_input1_error_set = set()\n",
    "    unique_input2_error_set = set()\n",
    "\n",
    "    pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "    unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "    retrain_dict = dict()\n",
    "\n",
    "    subj_person_male_count, subj_person_female_count, emotional_state_count, emotional_situation_count = {}, {}, {}, {}\n",
    "    verb_feel_count, verb1_count, neutral_subj_1_count, neutral_subj_2_count= {}, {}, {}, {}\n",
    "\n",
    "    subj_person_male_pred_error, subj_person_female_pred_error, emotional_state_pred_error, emotional_situation_pred_error = {}, {}, {}, {}\n",
    "    verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, neutral_subj_2_pred_error= {}, {}, {}, {}\n",
    "\n",
    "    subj_person_male_fairness_error, subj_person_female_fairness_error, emotional_state_fairness_error, emotional_situation_fairness_error = {}, {}, {}, {}\n",
    "    verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, neutral_subj_2_fairness_error= {}, {}, {}, {}\n",
    "\n",
    "    tmp1, tmp2 = 0, 0\n",
    "\n",
    "    count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "    pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "    fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "    #bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error = {}, {}, {}\n",
    "\n",
    "    bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "    bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "    bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "    \n",
    "#     noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    noun1_probability = get_probability_dict(noun_error1, noun_dict1)\n",
    "    noun2_probability = get_probability_dict(noun_error2, noun_dict2)\n",
    "    \n",
    "    for noun_choice in [0, 1, 2, 3, 4, 5]:\n",
    "        for i in range(ITERS):\n",
    "            \n",
    "            tokens = select_tokens_probabilistically(noun_choice, noun_error1, noun_error2, noun_dict1, noun_dict2, noun1_probability, noun2_probability)\n",
    "\n",
    "            input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 0)\n",
    "            unique_input1_set.add(input1)\n",
    "            unique_input2_set.add(input2)\n",
    "            unique_input_pair_set.add(input1 + \"; \" + input2)  \n",
    "            noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "            validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                                unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "\n",
    "            count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "            bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "\n",
    "            input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 1)\n",
    "            unique_input1_set.add(input1)\n",
    "            unique_input2_set.add(input2)\n",
    "            unique_input_pair_set.add(input1 + \"; \" + input2)  \n",
    "            noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "            validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                                unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "\n",
    "            count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "            bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "\n",
    "            input1, input2 = make_sentiment_neutral_sentences(tokens, verb_feel_list, 2)\n",
    "            unique_input1_set.add(input1)\n",
    "            unique_input2_set.add(input2)\n",
    "            unique_input_pair_set.add(input1 + \"; \" + input2)  \n",
    "            noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error  = \\\n",
    "            validate_prediction(tokens, noun_dict1, noun_dict2, noun_error1, noun_error2, input1, input2, unique_pred_input1_error_set, pred_err_count, \\\n",
    "                                unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error)\n",
    "\n",
    "            count_dict = update_counts(input1, input2, tokens, count_dict)\n",
    "            bias_pair_count = update_bias_pairs(input1, input2, tokens, bias_pair_count)\n",
    "\n",
    "            # or (len(unique_input_pair_set) >= max_input_gen_threshold)\n",
    "            if (len(unique_input_pair_set) == tmp1 == tmp2) or (len(unique_input_pair_set) >= max_input_gen_threshold):\n",
    "                print(\"Maximum input generation threshold reached, {} unique inputs generated\".format(len(unique_input_pair_set)))\n",
    "                break\n",
    "\n",
    "            if ITERS%2 == 0:\n",
    "                tmp1 = len(unique_input_pair_set)\n",
    "            else:\n",
    "                tmp2 = len(unique_input_pair_set)\n",
    "\n",
    "            if (i > 0) and  (i % num_iter == 0):\n",
    "                print(\"ITERATION: \", str(i))\n",
    "                print(\"Unique PREDICTION errors: \" + str(len(unique_pred_input1_error_set)))\n",
    "                print(\"Unique FAIRNESS errors: \" + str(len(unique_fairness_input1_error_set)))\n",
    "                print(\"Unique length of retrain_dict: \" + str(len(retrain_dict)))  \n",
    "\n",
    "                print(\"Unique input1s: \" + str(len(unique_input1_set)))\n",
    "                print(\"Unique input2s: \" + str(len(unique_input2_set)))\n",
    "                print(\"Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "\n",
    "#                 print(\"count_dict : \", count_dict)\n",
    "#                 print(\"pred_error_dict : \", pred_error_dict)\n",
    "#                 print(\"fairness_error_dict: \", fairness_error_dict)\n",
    "\n",
    "#                 print(\"bias_pair_count : \", bias_pair_count)\n",
    "#                 print(\"bias_pair_pred_error : \", bias_pair_pred_error)\n",
    "#                 print(\"bias_pair_fairness_error: \", bias_pair_fairness_error)\n",
    "\n",
    "                print(\"-\"*20)\n",
    "    \n",
    "        if (len(unique_input_pair_set) == tmp1 == tmp2) or (len(unique_input_pair_set) >= max_input_gen_threshold):\n",
    "            print(\"Maximum input generation threshold reached, {} unique inputs generated\".format(len(unique_input_pair_set)))\n",
    "            break\n",
    "\n",
    "    print(\"pred_err_count: \", pred_err_count)\n",
    "    print(\"pred_error_rate: \", pred_err_count/(7 * ITERS))\n",
    "    print(\"*\"*20)\n",
    "\n",
    "    print(\"fairness_err_count: \", fairness_err_count)\n",
    "    print(\"fairness_error_rate: \", fairness_err_count/(7 * ITERS))\n",
    "    print(\"*\"*20)\n",
    "\n",
    "    print(\"Final Unique input1s: \" + str(len(unique_input1_set)))\n",
    "    print(\"Final Unique input2s: \" + str(len(unique_input2_set)))\n",
    "    print(\"Final Unique input pairs: \" + str(len(unique_input_pair_set)))\n",
    "    print(\"*\"*20)\n",
    "\n",
    "    print(\"Final Unique PREDICTION errors: \" + str(len(unique_pred_input1_error_set)))\n",
    "    print(\"Final Unique FAIRNESS violations: \" + str(len(unique_fairness_input1_error_set)))\n",
    "    print(\"Final length of retrain_dict: \" + str(len(retrain_dict)))\n",
    "    print(\"*\"*20)\n",
    "\n",
    "#     print(\"Final count_dict : \", count_dict)\n",
    "#     print(\"Final pred_error_dict : \", pred_error_dict)\n",
    "#     print(\"Final fairness_error_dict: \", fairness_error_dict)\n",
    "#     print(\"*\"*20)\n",
    "\n",
    "#     print(\"Final bias_pair_count : \", bias_pair_count)\n",
    "#     print(\"Final bias_pair_pred_error : \", bias_pair_pred_error)\n",
    "#     print(\"Final bias_pair_fairness_error: \", bias_pair_fairness_error)\n",
    "\n",
    "    save_data(mode, noun_dict1, noun_dict2, noun_error1, noun_error2, unique_input1_set, unique_input2_set, unique_input_pair_set, unique_input1_error_set,\\\n",
    "            unique_input2_error_set, pred_err_count, fairness_err_count, unique_pred_input1_error_set, \\\n",
    "            unique_fairness_input1_error_set, retrain_dict, subj_person_male_count ,subj_person_female_count, emotional_state_count, \\\n",
    "                emotional_situation_count,verb_feel_count, verb1_count, neutral_subj_1_count, \\\n",
    "                neutral_subj_2_count, subj_person_male_pred_error ,subj_person_female_pred_error, emotional_state_pred_error, \\\n",
    "                emotional_situation_pred_error,verb_feel_pred_error, verb1_pred_error, neutral_subj_1_pred_error, \\\n",
    "                neutral_subj_2_pred_error, subj_person_male_fairness_error ,subj_person_female_fairness_error, emotional_state_fairness_error, \\\n",
    "                emotional_situation_fairness_error,verb_feel_fairness_error, verb1_fairness_error, neutral_subj_1_fairness_error, \\\n",
    "                neutral_subj_2_fairness_error, bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### A. Run Probabilistic Test Generation for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  0 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000\n",
    "num_iter = 5000 #00 \n",
    "max_input_gen_threshold =  2700 #3000\n",
    "mode = \"rnn2-unpadding/direct-gender-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'this girl': 2888, 'this woman': 2777, 'my aunt': 2683, 'her': 6817, 'my mother': 2615, 'my wife': 2814, 'She': 2800, 'my sister': 2726, 'my daughter': 2812, 'my girlfriend': 2894}\n",
      "noun_dict2 :  {'this boy': 2865, 'this man': 2771, 'him': 4213, 'my uncle': 2702, 'my father': 2634, 'my husband': 2805, 'He': 2800, 'my brother': 2707, 'my son': 2814, 'my boyfriend': 2900}\n",
      "noun_error1 :  {'this girl': 355, 'this woman': 265, 'my aunt': 785, 'her': 2229, 'my daughter': 592, 'my mother': 896, 'my sister': 470, 'She': 264, 'my girlfriend': 246, 'my wife': 40}\n",
      "noun_error2 :  {'this boy': 375, 'him': 922, 'my uncle': 911, 'my father': 952, 'my son': 681, 'this man': 260, 'my brother': 489, 'He': 283, 'my boyfriend': 248, 'my husband': 125}\n"
     ]
    }
   ],
   "source": [
    "#load pickles\n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = '../Exploration/saved_pickles/exploration/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('../Exploration/saved_pickles/exploration/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'this girl': 355, 'this woman': 265, 'my aunt': 785, 'my daughter': 592, 'my mother': 896, 'my sister': 470, 'She': 264, 'my girlfriend': 246, 'my wife': 40}\n",
      "noun_dict1: {'this girl': 2888, 'this woman': 2777, 'my aunt': 2683, 'my mother': 2615, 'my wife': 2814, 'She': 2800, 'my sister': 2726, 'my daughter': 2812, 'my girlfriend': 2894}\n",
      "noun_error2: {'this boy': 375, 'my uncle': 911, 'my father': 952, 'my son': 681, 'this man': 260, 'my brother': 489, 'He': 283, 'my boyfriend': 248, 'my husband': 125}\n",
      "noun_dict2: {'this boy': 2865, 'this man': 2771, 'my uncle': 2702, 'my father': 2634, 'my husband': 2805, 'He': 2800, 'my brother': 2707, 'my son': 2814, 'my boyfriend': 2900}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 2700 unique inputs generated\n",
      "pred_err_count:  2954\n",
      "pred_error_rate:  0.014066666666666667\n",
      "********************\n",
      "fairness_err_count:  930\n",
      "fairness_error_rate:  0.004428571428571428\n",
      "********************\n",
      "Final Unique input1s: 1154\n",
      "Final Unique input2s: 1166\n",
      "Final Unique input pairs: 2700\n",
      "********************\n",
      "Final Unique PREDICTION errors: 1668\n",
      "Final Unique FAIRNESS violations: 504\n",
      "Final length of retrain_dict: 302\n",
      "********************\n",
      "noun_dict1:  {'this girl': 3201, 'this woman': 3085, 'my aunt': 3560, 'my mother': 3556, 'my wife': 2883, 'She': 3010, 'my sister': 3294, 'my daughter': 3388, 'my girlfriend': 3106, 'her': 1613}\n",
      "noun_dict2:  {'this boy': 3265, 'this man': 3014, 'my uncle': 3560, 'my father': 3484, 'my husband': 2915, 'He': 3024, 'my brother': 3245, 'my son': 3406, 'my boyfriend': 3131, 'him': 700}\n",
      "noun_error1:  {'this girl': 401, 'this woman': 323, 'my aunt': 922, 'my daughter': 683, 'my mother': 1113, 'my sister': 548, 'She': 287, 'my girlfriend': 293, 'my wife': 44, 'her': 446}\n",
      "noun_error2:  {'this boy': 402, 'my uncle': 1168, 'my father': 1175, 'my son': 786, 'this man': 286, 'my brother': 565, 'He': 303, 'my boyfriend': 267, 'my husband': 139, 'him': 163}\n"
     ]
    }
   ],
   "source": [
    "generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Random gender noun comparisons (e.g. My boyfriend/My mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  1 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"rnn2-unpadding/random-gender-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'this girl': 388, 'her': 1096, 'my daughter': 467, 'my girlfriend': 361, 'my sister': 406, 'my wife': 374, 'this woman': 359, 'my mother': 444, 'She': 355, 'my aunt': 356, 'my mom': 388}\n",
      "noun_dict2 :  {'my brother': 414, 'him': 652, 'my father': 386, 'my boyfriend': 512, 'my dad': 444, 'He': 361, 'this man': 315, 'my husband': 381, 'my son': 327, 'this boy': 381, 'my uncle': 377}\n",
      "noun_error1 :  {'this girl': 37, 'my daughter': 57, 'her': 291, 'my sister': 71, 'this woman': 71, 'my mother': 81, 'my girlfriend': 42, 'my wife': 37, 'She': 50, 'my aunt': 47, 'my mom': 36}\n",
      "noun_error2 :  {'him': 151, 'my father': 108, 'this man': 38, 'my son': 81, 'my husband': 52, 'He': 39, 'my brother': 47, 'this boy': 38, 'my uncle': 101, 'my boyfriend': 54, 'my dad': 30}\n"
     ]
    }
   ],
   "source": [
    "#load pickles \n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "#bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error = {}, {}, {}\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = '../Exploration/saved_pickles/exploration/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('../Exploration/saved_pickles/exploration/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'this girl': 37, 'my daughter': 57, 'my sister': 71, 'this woman': 71, 'my mother': 81, 'my girlfriend': 42, 'my wife': 37, 'She': 50, 'my aunt': 47, 'my mom': 36}\n",
      "noun_dict1: {'this girl': 388, 'my daughter': 467, 'my girlfriend': 361, 'my sister': 406, 'my wife': 374, 'this woman': 359, 'my mother': 444, 'She': 355, 'my aunt': 356, 'my mom': 388}\n",
      "noun_error2: {'my father': 108, 'this man': 38, 'my son': 81, 'my husband': 52, 'He': 39, 'my brother': 47, 'this boy': 38, 'my uncle': 101, 'my boyfriend': 54, 'my dad': 30}\n",
      "noun_dict2: {'my brother': 414, 'my father': 386, 'my boyfriend': 512, 'my dad': 444, 'He': 361, 'this man': 315, 'my husband': 381, 'my son': 327, 'this boy': 381, 'my uncle': 377}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3002 unique inputs generated\n",
      "pred_err_count:  2918\n",
      "pred_error_rate:  0.013895238095238096\n",
      "********************\n",
      "fairness_err_count:  881\n",
      "fairness_error_rate:  0.004195238095238096\n",
      "********************\n",
      "Final Unique input1s: 1324\n",
      "Final Unique input2s: 1302\n",
      "Final Unique input pairs: 3002\n",
      "********************\n",
      "Final Unique PREDICTION errors: 1859\n",
      "Final Unique FAIRNESS violations: 523\n",
      "Final length of retrain_dict: 318\n",
      "********************\n",
      "noun_dict1:  {'this girl': 674, 'my daughter': 953, 'my girlfriend': 626, 'my sister': 916, 'my wife': 730, 'this woman': 987, 'my mother': 900, 'She': 737, 'my aunt': 670, 'my mom': 769, 'her': 1138}\n",
      "noun_dict2:  {'my brother': 632, 'my father': 1085, 'my boyfriend': 832, 'my dad': 646, 'He': 656, 'this man': 672, 'my husband': 764, 'my son': 1007, 'this boy': 629, 'my uncle': 1045, 'him': 676}\n",
      "noun_error1:  {'this girl': 68, 'my daughter': 140, 'my sister': 149, 'this woman': 212, 'my mother': 168, 'my girlfriend': 90, 'my wife': 87, 'She': 108, 'my aunt': 85, 'my mom': 73, 'her': 317}\n",
      "noun_error2:  {'my father': 285, 'this man': 77, 'my son': 200, 'my husband': 103, 'He': 78, 'my brother': 68, 'this boy': 66, 'my uncle': 333, 'my boyfriend': 76, 'my dad': 45, 'him': 138}\n"
     ]
    }
   ],
   "source": [
    "generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Test for Indirect Gender Bias, i.e. Occupational Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  2 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"rnn2-unpadding/gender-occupation-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'the designer': 127, 'her': 855, 'the secretary': 167, 'the assistant': 121, 'the baker': 124, 'the hairdresser': 128, 'the the librarian': 125, 'the cashier': 172, 'the librarian': 160, 'the housekeeper': 129, 'the counselor': 201, 'the editor': 135, 'the auditor': 168, 'the writer': 134, 'the clerk': 151, 'the cleaner': 182, 'the nurse': 130, 'the the paralegal': 164, 'the the nurse': 143, 'the receptionist': 175, 'the teacher': 308, 'the attendant': 152, 'the tailor': 161}\n",
      "noun_dict2 :  {'the manager': 106, 'him': 564, 'the janitor': 185, 'the accountant': 167, 'the salesperson': 132, 'the mover': 138, 'the developer': 187, 'the construction worker': 109, 'the farmer': 138, 'the guard': 142, 'the technician': 177, 'the physician': 117, 'the CEO': 182, 'the carpenter': 172, 'the laborer': 109, 'the driver': 118, 'the cook': 133, 'the supervisor': 124, 'the sheriff': 155, 'the engineer': 267, 'the lawyer': 137, 'the analyst': 169, 'the chief': 118, 'the mechanic': 158}\n",
      "noun_error1 :  {'the designer': 21, 'her': 193, 'the hairdresser': 22, 'the the librarian': 12, 'the secretary': 18, 'the librarian': 13, 'the housekeeper': 17, 'the cashier': 16, 'the writer': 19, 'the cleaner': 14, 'the auditor': 44, 'the counselor': 33, 'the nurse': 28, 'the the paralegal': 41, 'the receptionist': 21, 'the the nurse': 28, 'the assistant': 9, 'the editor': 15, 'the clerk': 43, 'the teacher': 43, 'the attendant': 16, 'the baker': 20, 'the tailor': 19}\n",
      "noun_error2 :  {'the manager': 15, 'him': 136, 'the mover': 29, 'the developer': 18, 'the construction worker': 23, 'the farmer': 22, 'the technician': 44, 'the physician': 22, 'the driver': 28, 'the cook': 31, 'the CEO': 25, 'the supervisor': 35, 'the salesperson': 25, 'the engineer': 37, 'the sheriff': 22, 'the janitor': 16, 'the guard': 12, 'the lawyer': 21, 'the chief': 19, 'the mechanic': 29, 'the accountant': 5, 'the laborer': 17, 'the carpenter': 12, 'the analyst': 19}\n"
     ]
    }
   ],
   "source": [
    "#load pickles \n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "#bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error = {}, {}, {}\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = '../Exploration/saved_pickles/exploration/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('../Exploration/saved_pickles/exploration/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'the designer': 21, 'the hairdresser': 22, 'the the librarian': 12, 'the secretary': 18, 'the librarian': 13, 'the housekeeper': 17, 'the cashier': 16, 'the writer': 19, 'the cleaner': 14, 'the auditor': 44, 'the counselor': 33, 'the nurse': 28, 'the the paralegal': 41, 'the receptionist': 21, 'the the nurse': 28, 'the assistant': 9, 'the editor': 15, 'the clerk': 43, 'the teacher': 43, 'the attendant': 16, 'the baker': 20, 'the tailor': 19}\n",
      "noun_dict1: {'the designer': 127, 'the secretary': 167, 'the assistant': 121, 'the baker': 124, 'the hairdresser': 128, 'the the librarian': 125, 'the cashier': 172, 'the librarian': 160, 'the housekeeper': 129, 'the counselor': 201, 'the editor': 135, 'the auditor': 168, 'the writer': 134, 'the clerk': 151, 'the cleaner': 182, 'the nurse': 130, 'the the paralegal': 164, 'the the nurse': 143, 'the receptionist': 175, 'the teacher': 308, 'the attendant': 152, 'the tailor': 161}\n",
      "noun_error2: {'the manager': 15, 'the mover': 29, 'the developer': 18, 'the construction worker': 23, 'the farmer': 22, 'the technician': 44, 'the physician': 22, 'the driver': 28, 'the cook': 31, 'the CEO': 25, 'the supervisor': 35, 'the salesperson': 25, 'the engineer': 37, 'the sheriff': 22, 'the janitor': 16, 'the guard': 12, 'the lawyer': 21, 'the chief': 19, 'the mechanic': 29, 'the accountant': 5, 'the laborer': 17, 'the carpenter': 12, 'the analyst': 19}\n",
      "noun_dict2: {'the manager': 106, 'the janitor': 185, 'the accountant': 167, 'the salesperson': 132, 'the mover': 138, 'the developer': 187, 'the construction worker': 109, 'the farmer': 138, 'the guard': 142, 'the technician': 177, 'the physician': 117, 'the CEO': 182, 'the carpenter': 172, 'the laborer': 109, 'the driver': 118, 'the cook': 133, 'the supervisor': 124, 'the sheriff': 155, 'the engineer': 267, 'the lawyer': 137, 'the analyst': 169, 'the chief': 118, 'the mechanic': 158}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3000 unique inputs generated\n",
      "pred_err_count:  2329\n",
      "pred_error_rate:  0.01109047619047619\n",
      "********************\n",
      "fairness_err_count:  770\n",
      "fairness_error_rate:  0.0036666666666666666\n",
      "********************\n",
      "Final Unique input1s: 1792\n",
      "Final Unique input2s: 1839\n",
      "Final Unique input pairs: 3000\n",
      "********************\n",
      "Final Unique PREDICTION errors: 1683\n",
      "Final Unique FAIRNESS violations: 552\n",
      "Final length of retrain_dict: 398\n",
      "********************\n",
      "noun_dict1:  {'the designer': 305, 'the secretary': 242, 'the assistant': 239, 'the baker': 279, 'the hairdresser': 321, 'the the librarian': 201, 'the cashier': 308, 'the librarian': 250, 'the housekeeper': 248, 'the counselor': 397, 'the editor': 298, 'the auditor': 530, 'the writer': 282, 'the clerk': 483, 'the cleaner': 240, 'the nurse': 382, 'the the paralegal': 434, 'the the nurse': 307, 'the receptionist': 332, 'the teacher': 400, 'the attendant': 271, 'the tailor': 280, 'her': 678}\n",
      "noun_dict2:  {'the manager': 225, 'the janitor': 257, 'the accountant': 212, 'the salesperson': 309, 'the mover': 299, 'the developer': 282, 'the construction worker': 370, 'the farmer': 239, 'the guard': 203, 'the technician': 452, 'the physician': 296, 'the CEO': 349, 'the carpenter': 240, 'the laborer': 237, 'the driver': 351, 'the cook': 364, 'the supervisor': 361, 'the sheriff': 268, 'the engineer': 480, 'the lawyer': 335, 'the analyst': 269, 'the chief': 296, 'the mechanic': 314, 'him': 590}\n",
      "noun_error1:  {'the designer': 50, 'the hairdresser': 64, 'the the librarian': 20, 'the secretary': 24, 'the librarian': 30, 'the housekeeper': 29, 'the cashier': 32, 'the writer': 37, 'the cleaner': 21, 'the auditor': 108, 'the counselor': 57, 'the nurse': 75, 'the the paralegal': 87, 'the receptionist': 54, 'the the nurse': 57, 'the assistant': 31, 'the editor': 28, 'the clerk': 155, 'the teacher': 60, 'the attendant': 33, 'the baker': 43, 'the tailor': 36, 'her': 168}\n",
      "noun_error2:  {'the manager': 37, 'the mover': 68, 'the developer': 28, 'the construction worker': 88, 'the farmer': 39, 'the technician': 88, 'the physician': 53, 'the driver': 56, 'the cook': 84, 'the CEO': 49, 'the supervisor': 77, 'the salesperson': 68, 'the engineer': 69, 'the sheriff': 40, 'the janitor': 28, 'the guard': 16, 'the lawyer': 53, 'the chief': 46, 'the mechanic': 48, 'the accountant': 12, 'the laborer': 49, 'the carpenter': 17, 'the analyst': 40, 'him': 143}\n"
     ]
    }
   ],
   "source": [
    "generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Test for   for Indirect Gender Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  3 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"rnn2-unpadding/gender-name-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'Laura': 181, 'her': 577, 'Lisa': 135, 'Patricia': 100, 'Ashley': 151, 'Linda': 126, 'Rebecca': 113, 'Karen': 64, 'Emily': 88, 'Deborah': 141, 'Margaret': 100, 'Sharon': 87, 'Barbara': 92, 'Susan': 78, 'Cynthia': 130, 'Jessica': 174, 'Sandra': 96, 'Betty': 150, 'Amanda': 145, 'Carol': 105, 'Donna': 113, 'Elizabeth': 107, 'Mary': 130, 'Kimberly': 101, 'Dorothy ': 104, 'Jennifer': 124, 'Sarah': 131, 'Michelle': 128, 'Melissa': 100, 'Nancy': 85, 'Stephanie': 83}\n",
      "noun_dict2 :  {'John ': 117, 'him': 605, 'Kenneth': 184, 'Andrew': 89, 'Anthony': 104, 'James': 94, 'Christopher': 126, 'Brian': 84, 'Thomas': 144, 'Joshua': 78, 'Mark': 158, 'Timothy': 81, 'Matthew': 140, 'Michael ': 99, 'George': 116, 'Jeffrey': 120, 'Richard': 114, 'Edward': 120, 'Robert ': 95, 'Ryan': 116, 'Kevin': 116, 'Daniel': 125, 'Steven': 150, 'Charles': 139, 'William ': 134, 'Joseph': 116, 'Paul': 66, 'Donald': 160, 'Jason': 101, 'Ronald': 99, 'David ': 49}\n",
      "noun_error1 :  {'Laura': 31, 'her': 137, 'Patricia': 16, 'Ashley': 17, 'Rebecca': 30, 'Linda': 16, 'Emily': 12, 'Deborah': 11, 'Cynthia': 12, 'Sandra': 12, 'Betty': 24, 'Lisa': 9, 'Donna': 37, 'Elizabeth': 20, 'Jessica': 17, 'Amanda': 20, 'Mary': 15, 'Dorothy ': 14, 'Margaret': 12, 'Sarah': 19, 'Michelle': 40, 'Melissa': 13, 'Jennifer': 22, 'Nancy': 33, 'Carol': 14, 'Stephanie': 32, 'Sharon': 6, 'Karen': 14, 'Barbara': 11, 'Susan': 10, 'Kimberly': 14}\n",
      "noun_error2 :  {'John ': 23, 'him': 144, 'Anthony': 22, 'James': 12, 'Thomas': 18, 'Brian': 5, 'Joshua': 19, 'Mark': 26, 'George': 21, 'Kevin': 13, 'Matthew': 17, 'Daniel': 22, 'Charles': 29, 'William ': 21, 'Paul': 14, 'Edward': 19, 'Jason': 27, 'Kenneth': 33, 'Robert ': 16, 'Christopher': 23, 'Donald': 30, 'Richard': 20, 'Ronald': 14, 'Michael ': 13, 'Ryan': 11, 'Steven': 13, 'Joseph': 27, 'Timothy': 9, 'Jeffrey': 15, 'David ': 3, 'Andrew': 11}\n"
     ]
    }
   ],
   "source": [
    "#load pickles \n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "#bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error = {}, {}, {}\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = '../Exploration/saved_pickles/exploration/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('../Exploration/saved_pickles/exploration/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'Laura': 31, 'Patricia': 16, 'Ashley': 17, 'Rebecca': 30, 'Linda': 16, 'Emily': 12, 'Deborah': 11, 'Cynthia': 12, 'Sandra': 12, 'Betty': 24, 'Lisa': 9, 'Donna': 37, 'Elizabeth': 20, 'Jessica': 17, 'Amanda': 20, 'Mary': 15, 'Dorothy ': 14, 'Margaret': 12, 'Sarah': 19, 'Michelle': 40, 'Melissa': 13, 'Jennifer': 22, 'Nancy': 33, 'Carol': 14, 'Stephanie': 32, 'Sharon': 6, 'Karen': 14, 'Barbara': 11, 'Susan': 10, 'Kimberly': 14}\n",
      "noun_dict1: {'Laura': 181, 'Lisa': 135, 'Patricia': 100, 'Ashley': 151, 'Linda': 126, 'Rebecca': 113, 'Karen': 64, 'Emily': 88, 'Deborah': 141, 'Margaret': 100, 'Sharon': 87, 'Barbara': 92, 'Susan': 78, 'Cynthia': 130, 'Jessica': 174, 'Sandra': 96, 'Betty': 150, 'Amanda': 145, 'Carol': 105, 'Donna': 113, 'Elizabeth': 107, 'Mary': 130, 'Kimberly': 101, 'Dorothy ': 104, 'Jennifer': 124, 'Sarah': 131, 'Michelle': 128, 'Melissa': 100, 'Nancy': 85, 'Stephanie': 83}\n",
      "noun_error2: {'John ': 23, 'Anthony': 22, 'James': 12, 'Thomas': 18, 'Brian': 5, 'Joshua': 19, 'Mark': 26, 'George': 21, 'Kevin': 13, 'Matthew': 17, 'Daniel': 22, 'Charles': 29, 'William ': 21, 'Paul': 14, 'Edward': 19, 'Jason': 27, 'Kenneth': 33, 'Robert ': 16, 'Christopher': 23, 'Donald': 30, 'Richard': 20, 'Ronald': 14, 'Michael ': 13, 'Ryan': 11, 'Steven': 13, 'Joseph': 27, 'Timothy': 9, 'Jeffrey': 15, 'David ': 3, 'Andrew': 11}\n",
      "noun_dict2: {'John ': 117, 'Kenneth': 184, 'Andrew': 89, 'Anthony': 104, 'James': 94, 'Christopher': 126, 'Brian': 84, 'Thomas': 144, 'Joshua': 78, 'Mark': 158, 'Timothy': 81, 'Matthew': 140, 'Michael ': 99, 'George': 116, 'Jeffrey': 120, 'Richard': 114, 'Edward': 120, 'Robert ': 95, 'Ryan': 116, 'Kevin': 116, 'Daniel': 125, 'Steven': 150, 'Charles': 139, 'William ': 134, 'Joseph': 116, 'Paul': 66, 'Donald': 160, 'Jason': 101, 'Ronald': 99, 'David ': 49}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3002 unique inputs generated\n",
      "pred_err_count:  2189\n",
      "pred_error_rate:  0.010423809523809524\n",
      "********************\n",
      "fairness_err_count:  722\n",
      "fairness_error_rate:  0.003438095238095238\n",
      "********************\n",
      "Final Unique input1s: 2002\n",
      "Final Unique input2s: 2005\n",
      "Final Unique input pairs: 3002\n",
      "********************\n",
      "Final Unique PREDICTION errors: 1630\n",
      "Final Unique FAIRNESS violations: 545\n",
      "Final length of retrain_dict: 400\n",
      "********************\n",
      "noun_dict1:  {'Laura': 300, 'Lisa': 189, 'Patricia': 221, 'Ashley': 235, 'Linda': 205, 'Rebecca': 319, 'Karen': 180, 'Emily': 184, 'Deborah': 196, 'Margaret': 155, 'Sharon': 139, 'Barbara': 198, 'Susan': 145, 'Cynthia': 234, 'Jessica': 308, 'Sandra': 202, 'Betty': 271, 'Amanda': 230, 'Carol': 204, 'Donna': 312, 'Elizabeth': 253, 'Mary': 220, 'Kimberly': 166, 'Dorothy ': 248, 'Jennifer': 275, 'Sarah': 198, 'Michelle': 322, 'Melissa': 194, 'Nancy': 330, 'Stephanie': 289, 'her': 572}\n",
      "noun_dict2:  {'John ': 309, 'Kenneth': 308, 'Andrew': 184, 'Anthony': 232, 'James': 171, 'Christopher': 232, 'Brian': 161, 'Thomas': 215, 'Joshua': 226, 'Mark': 268, 'Timothy': 168, 'Matthew': 200, 'Michael ': 203, 'George': 285, 'Jeffrey': 203, 'Richard': 227, 'Edward': 296, 'Robert ': 187, 'Ryan': 209, 'Kevin': 185, 'Daniel': 231, 'Steven': 150, 'Charles': 301, 'William ': 261, 'Joseph': 288, 'Paul': 258, 'Donald': 318, 'Jason': 333, 'Ronald': 214, 'David ': 80, 'him': 563}\n",
      "noun_error1:  {'Laura': 45, 'Patricia': 33, 'Ashley': 29, 'Rebecca': 60, 'Linda': 29, 'Emily': 24, 'Deborah': 17, 'Cynthia': 22, 'Sandra': 31, 'Betty': 43, 'Lisa': 18, 'Donna': 77, 'Elizabeth': 40, 'Jessica': 32, 'Amanda': 36, 'Mary': 44, 'Dorothy ': 35, 'Margaret': 18, 'Sarah': 30, 'Michelle': 89, 'Melissa': 29, 'Jennifer': 40, 'Nancy': 97, 'Carol': 22, 'Stephanie': 92, 'Sharon': 11, 'Karen': 25, 'Barbara': 20, 'Susan': 17, 'Kimberly': 17, 'her': 153}\n",
      "noun_error2:  {'John ': 58, 'Anthony': 47, 'James': 25, 'Thomas': 30, 'Brian': 15, 'Joshua': 50, 'Mark': 44, 'George': 54, 'Kevin': 22, 'Matthew': 27, 'Daniel': 47, 'Charles': 47, 'William ': 37, 'Paul': 46, 'Edward': 39, 'Jason': 75, 'Kenneth': 58, 'Robert ': 39, 'Christopher': 37, 'Donald': 50, 'Richard': 42, 'Ronald': 36, 'Michael ': 30, 'Ryan': 22, 'Steven': 13, 'Joseph': 54, 'Timothy': 16, 'Jeffrey': 29, 'David ': 10, 'Andrew': 22, 'him': 147}\n"
     ]
    }
   ],
   "source": [
    "generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Test for   for Indirect Racial Bias, i.e. Name Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_choice =  5 #Noun /Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"rnn2-unpadding/racial-name-noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'Heather': 175, 'her': 761, 'Roger': 213, 'Betsy': 202, 'Harry': 213, 'Justin': 216, 'Jack': 220, 'Amanda': 175, 'Josh': 254, 'Andrew': 147, 'Ryan': 161, 'Courtney': 187, 'Melanie': 127, 'Kristin': 171, 'Katie': 95, 'Frank': 166, 'Alan': 112, 'Stephanie': 201, 'Adam': 192, 'Ellen': 136, 'Nancy': 160}\n",
      "noun_dict2 :  {'Terrence': 235, 'him': 553, 'Lamar': 154, 'Torrance': 200, 'Darnell': 197, 'Alphonse': 195, 'Jasmine': 144, 'Lakisha': 170, 'Shaniqua': 184, 'Latisha': 160, 'Alonzo': 165, 'Jamel': 179, 'Tanisha': 193, 'Malik': 164, 'Latoya': 191, 'Jerome': 159, 'Ebony': 207, 'Tia': 171, 'Leroy': 193, 'Shereen': 175, 'Nichelle': 120}\n",
      "noun_error1 :  {'Heather': 21, 'her': 163, 'Roger': 27, 'Betsy': 35, 'Harry': 44, 'Jack': 41, 'Andrew': 41, 'Amanda': 30, 'Courtney': 48, 'Melanie': 30, 'Kristin': 39, 'Katie': 17, 'Frank': 24, 'Justin': 24, 'Stephanie': 62, 'Adam': 36, 'Ellen': 33, 'Nancy': 44, 'Ryan': 29, 'Josh': 47, 'Alan': 17}\n",
      "noun_error2 :  {'Terrence': 41, 'Lamar': 58, 'him': 147, 'Torrance': 58, 'Darnell': 54, 'Lakisha': 29, 'Shaniqua': 21, 'Jamel': 46, 'Malik': 32, 'Latisha': 15, 'Alphonse': 50, 'Jerome': 24, 'Tia': 34, 'Jasmine': 27, 'Tanisha': 32, 'Alonzo': 25, 'Leroy': 29, 'Shereen': 23, 'Latoya': 27, 'Nichelle': 28, 'Ebony': 31}\n"
     ]
    }
   ],
   "source": [
    "#load pickles \n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "#bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error = {}, {}, {}\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = '../Exploration/saved_pickles/exploration/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('../Exploration/saved_pickles/exploration/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'Heather': 21, 'Roger': 27, 'Betsy': 35, 'Harry': 44, 'Jack': 41, 'Andrew': 41, 'Amanda': 30, 'Courtney': 48, 'Melanie': 30, 'Kristin': 39, 'Katie': 17, 'Frank': 24, 'Justin': 24, 'Stephanie': 62, 'Adam': 36, 'Ellen': 33, 'Nancy': 44, 'Ryan': 29, 'Josh': 47, 'Alan': 17}\n",
      "noun_dict1: {'Heather': 175, 'Roger': 213, 'Betsy': 202, 'Harry': 213, 'Justin': 216, 'Jack': 220, 'Amanda': 175, 'Josh': 254, 'Andrew': 147, 'Ryan': 161, 'Courtney': 187, 'Melanie': 127, 'Kristin': 171, 'Katie': 95, 'Frank': 166, 'Alan': 112, 'Stephanie': 201, 'Adam': 192, 'Ellen': 136, 'Nancy': 160}\n",
      "noun_error2: {'Terrence': 41, 'Lamar': 58, 'Torrance': 58, 'Darnell': 54, 'Lakisha': 29, 'Shaniqua': 21, 'Jamel': 46, 'Malik': 32, 'Latisha': 15, 'Alphonse': 50, 'Jerome': 24, 'Tia': 34, 'Jasmine': 27, 'Tanisha': 32, 'Alonzo': 25, 'Leroy': 29, 'Shereen': 23, 'Latoya': 27, 'Nichelle': 28, 'Ebony': 31}\n",
      "noun_dict2: {'Terrence': 235, 'Lamar': 154, 'Torrance': 200, 'Darnell': 197, 'Alphonse': 195, 'Jasmine': 144, 'Lakisha': 170, 'Shaniqua': 184, 'Latisha': 160, 'Alonzo': 165, 'Jamel': 179, 'Tanisha': 193, 'Malik': 164, 'Latoya': 191, 'Jerome': 159, 'Ebony': 207, 'Tia': 171, 'Leroy': 193, 'Shereen': 175, 'Nichelle': 120}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3000 unique inputs generated\n",
      "pred_err_count:  2302\n",
      "pred_error_rate:  0.010961904761904761\n",
      "********************\n",
      "fairness_err_count:  938\n",
      "fairness_error_rate:  0.0044666666666666665\n",
      "********************\n",
      "Final Unique input1s: 1826\n",
      "Final Unique input2s: 1778\n",
      "Final Unique input pairs: 3000\n",
      "********************\n",
      "Final Unique PREDICTION errors: 1703\n",
      "Final Unique FAIRNESS violations: 677\n",
      "Final length of retrain_dict: 487\n",
      "********************\n",
      "noun_dict1:  {'Heather': 290, 'Roger': 352, 'Betsy': 416, 'Harry': 359, 'Justin': 289, 'Jack': 395, 'Amanda': 314, 'Josh': 441, 'Andrew': 455, 'Ryan': 329, 'Courtney': 393, 'Melanie': 376, 'Kristin': 366, 'Katie': 281, 'Frank': 316, 'Alan': 232, 'Stephanie': 452, 'Adam': 374, 'Ellen': 259, 'Nancy': 385, 'her': 680}\n",
      "noun_dict2:  {'Terrence': 389, 'Lamar': 485, 'Torrance': 378, 'Darnell': 444, 'Alphonse': 447, 'Jasmine': 286, 'Lakisha': 322, 'Shaniqua': 328, 'Latisha': 256, 'Alonzo': 308, 'Jamel': 451, 'Tanisha': 383, 'Malik': 308, 'Latoya': 301, 'Jerome': 302, 'Ebony': 335, 'Tia': 398, 'Leroy': 335, 'Shereen': 297, 'Nichelle': 320, 'him': 599}\n",
      "noun_error1:  {'Heather': 37, 'Roger': 56, 'Betsy': 69, 'Harry': 72, 'Jack': 77, 'Andrew': 108, 'Amanda': 68, 'Courtney': 124, 'Melanie': 79, 'Kristin': 92, 'Katie': 63, 'Frank': 49, 'Justin': 35, 'Stephanie': 163, 'Adam': 58, 'Ellen': 55, 'Nancy': 107, 'Ryan': 67, 'Josh': 80, 'Alan': 38, 'her': 146}\n",
      "noun_error2:  {'Terrence': 80, 'Lamar': 211, 'Torrance': 99, 'Darnell': 109, 'Lakisha': 48, 'Shaniqua': 41, 'Jamel': 107, 'Malik': 51, 'Latisha': 22, 'Alphonse': 110, 'Jerome': 55, 'Tia': 84, 'Jasmine': 55, 'Tanisha': 65, 'Alonzo': 59, 'Leroy': 48, 'Shereen': 53, 'Latoya': 42, 'Nichelle': 100, 'Ebony': 44, 'him': 139}\n"
     ]
    }
   ],
   "source": [
    "generate_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Test for Neutral (Sentiment) Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000\n",
    "num_iter = 5000 \n",
    "max_input_gen_threshold =  3000\n",
    "mode = \"rnn2-unpadding/neutral-sentiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_dict1 :  {'this girl': 18804, 'my mother': 18903, 'her': 19125, 'She': 19386, 'this woman': 18615, 'my sister': 18825, 'my daughter': 19158, 'my aunt': 19212, 'my wife': 19185, 'my girlfriend': 19110, 'my mom': 8802, 'the cashier': 111, 'the librarian': 111, 'the attendant': 90, 'the the librarian': 138, 'the clerk': 162, 'the secretary': 114, 'the cleaner': 117, 'the tailor': 141, 'the baker': 99, 'the editor': 135, 'the designer': 120, 'the teacher': 222, 'the writer': 117, 'the the paralegal': 120, 'the housekeeper': 120, 'the assistant': 84, 'the the nurse': 117, 'the auditor': 111, 'the hairdresser': 99, 'the nurse': 111, 'the receptionist': 117, 'the counselor': 102}\n",
      "noun_dict2 :  {'this boy': 18951, 'my father': 19113, 'He': 19485, 'this man': 18570, 'my brother': 18846, 'my son': 19056, 'my uncle': 19161, 'my husband': 18876, 'my boyfriend': 18855, 'my dad': 9087, 'the cook': 126, 'the guard': 129, 'the sheriff': 105, 'the accountant': 105, 'the technician': 138, 'the mover': 99, 'the salesperson': 144, 'the construction worker': 108, 'the mechanic': 114, 'the lawyer': 117, 'the farmer': 144, 'the engineer': 108, 'the chief': 84, 'the CEO': 123, 'the developer': 114, 'the manager': 135, 'the driver': 111, 'the laborer': 111, 'the janitor': 108, 'the analyst': 96, 'the physician': 96, 'the supervisor': 141, 'the carpenter': 102}\n",
      "noun_error1 :  {'this girl': 6188, 'my mother': 12507, 'her': 12564, 'She': 7680, 'my sister': 5478, 'my aunt': 10560, 'this woman': 5767, 'my daughter': 6356, 'my girlfriend': 6161, 'my wife': 2199, 'my mom': 2084, 'the cashier': 40, 'the librarian': 28, 'the attendant': 25, 'the the librarian': 53, 'the clerk': 105, 'the cleaner': 44, 'the tailor': 44, 'the baker': 41, 'the editor': 45, 'the teacher': 57, 'the writer': 38, 'the the paralegal': 50, 'the hairdresser': 38, 'the secretary': 30, 'the nurse': 37, 'the housekeeper': 31, 'the the nurse': 39, 'the designer': 34, 'the counselor': 29, 'the receptionist': 31, 'the auditor': 44, 'the assistant': 20}\n",
      "noun_error2 :  {'this boy': 6681, 'my father': 12751, 'He': 7788, 'my brother': 4427, 'my uncle': 12751, 'this man': 4335, 'my son': 7826, 'my boyfriend': 4496, 'my dad': 1974, 'my husband': 1951, 'the cook': 54, 'the guard': 35, 'the sheriff': 41, 'the accountant': 25, 'the technician': 47, 'the salesperson': 56, 'the mover': 34, 'the construction worker': 39, 'the mechanic': 48, 'the lawyer': 28, 'the farmer': 30, 'the engineer': 30, 'the chief': 26, 'the CEO': 32, 'the developer': 35, 'the manager': 67, 'the laborer': 41, 'the janitor': 42, 'the analyst': 40, 'the supervisor': 62, 'the driver': 43, 'the carpenter': 27, 'the physician': 21}\n"
     ]
    }
   ],
   "source": [
    "#load pickles \n",
    "\n",
    "pred_err_count, fairness_err_count = 0, 0\n",
    "\n",
    "unique_pred_input1_error_set, unique_fairness_input1_error_set = set(), set() \n",
    "retrain_dict = dict()\n",
    "\n",
    "count_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "pred_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "fairness_error_dict = [{}, {}, {}, {}, {}, {}, {}, {}]\n",
    "\n",
    "#bias_pair_count, bias_pair_pred_error, bias_pair_fairness_error = {}, {}, {}\n",
    "\n",
    "bias_pair_count = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_pred_error = [{}, {}, {}, {}, {}, {}]\n",
    "bias_pair_fairness_error = [{}, {}, {}, {}, {}, {}]\n",
    "noun_dict1, noun_dict2, noun_error1, noun_error2 = {}, {}, {}, {}\n",
    "\n",
    "c_vals = [noun_dict1, noun_dict2, noun_error1, noun_error2, unique_pred_input1_error_set, pred_err_count, unique_fairness_input1_error_set, fairness_err_count, retrain_dict, pred_error_dict, fairness_error_dict, bias_pair_pred_error, bias_pair_fairness_error]\n",
    "c_names = [\"noun_dict1\", \"noun_dict2\", \"noun_error1\", \"noun_error2\", \"unique_pred_input1_error_set\", \"pred_err_count\", \"unique_fairness_input1_error_set\", \"fairness_err_count\", \"retrain_dict\", \"pred_error_dict\", \"fairness_error_dict\", \"bias_pair_pred_error\", \"bias_pair_fairness_error\"]\n",
    "\n",
    "assert len(c_names) == len(c_vals), \\\n",
    "    \"ERROR: bug in variables names for stored inputs {}, {}\".format(c_names)\n",
    "\n",
    "for i in range(0, len(c_vals)):\n",
    "    store = c_names[i] + \".pickle\"\n",
    "    target = '../Exploration/saved_pickles/exploration/' + mode + '/' + store\n",
    "    if os.path.exists(target):\n",
    "        if os.path.getsize(target) > 0:\n",
    "#             print(os.path.getsize(target))\n",
    "#             print(os.path.exists(target))\n",
    "            with open('../Exploration/saved_pickles/exploration/' + mode + '/' + store, 'rb') as handle:\n",
    "                c_vals[i] = pickle.load(handle)\n",
    "                if i <4:\n",
    "                    print(c_names[i], \": \", c_vals[i])\n",
    "#         else:\n",
    "#             print(\"ERROR {} is empty\".format(target))\n",
    "#     else:\n",
    "#         print(\"ERROR {} does not exist\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noun_dict1 = c_vals[0]\n",
    "noun_dict2 = c_vals[1]\n",
    "noun_error1 =  c_vals[2]\n",
    "noun_error2 =  c_vals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_dict1.pop('her', None)\n",
    "noun_error1.pop('her', None)\n",
    "noun_dict2.pop('him', None)\n",
    "noun_error2.pop('him', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_error1: {'this girl': 6188, 'my mother': 12507, 'She': 7680, 'my sister': 5478, 'my aunt': 10560, 'this woman': 5767, 'my daughter': 6356, 'my girlfriend': 6161, 'my wife': 2199, 'my mom': 2084, 'the cashier': 40, 'the librarian': 28, 'the attendant': 25, 'the the librarian': 53, 'the clerk': 105, 'the cleaner': 44, 'the tailor': 44, 'the baker': 41, 'the editor': 45, 'the teacher': 57, 'the writer': 38, 'the the paralegal': 50, 'the hairdresser': 38, 'the secretary': 30, 'the nurse': 37, 'the housekeeper': 31, 'the the nurse': 39, 'the designer': 34, 'the counselor': 29, 'the receptionist': 31, 'the auditor': 44, 'the assistant': 20}\n",
      "noun_dict1: {'this girl': 18804, 'my mother': 18903, 'She': 19386, 'this woman': 18615, 'my sister': 18825, 'my daughter': 19158, 'my aunt': 19212, 'my wife': 19185, 'my girlfriend': 19110, 'my mom': 8802, 'the cashier': 111, 'the librarian': 111, 'the attendant': 90, 'the the librarian': 138, 'the clerk': 162, 'the secretary': 114, 'the cleaner': 117, 'the tailor': 141, 'the baker': 99, 'the editor': 135, 'the designer': 120, 'the teacher': 222, 'the writer': 117, 'the the paralegal': 120, 'the housekeeper': 120, 'the assistant': 84, 'the the nurse': 117, 'the auditor': 111, 'the hairdresser': 99, 'the nurse': 111, 'the receptionist': 117, 'the counselor': 102}\n",
      "noun_error2: {'this boy': 6681, 'my father': 12751, 'He': 7788, 'my brother': 4427, 'my uncle': 12751, 'this man': 4335, 'my son': 7826, 'my boyfriend': 4496, 'my dad': 1974, 'my husband': 1951, 'the cook': 54, 'the guard': 35, 'the sheriff': 41, 'the accountant': 25, 'the technician': 47, 'the salesperson': 56, 'the mover': 34, 'the construction worker': 39, 'the mechanic': 48, 'the lawyer': 28, 'the farmer': 30, 'the engineer': 30, 'the chief': 26, 'the CEO': 32, 'the developer': 35, 'the manager': 67, 'the laborer': 41, 'the janitor': 42, 'the analyst': 40, 'the supervisor': 62, 'the driver': 43, 'the carpenter': 27, 'the physician': 21}\n",
      "noun_dict2: {'this boy': 18951, 'my father': 19113, 'He': 19485, 'this man': 18570, 'my brother': 18846, 'my son': 19056, 'my uncle': 19161, 'my husband': 18876, 'my boyfriend': 18855, 'my dad': 9087, 'the cook': 126, 'the guard': 129, 'the sheriff': 105, 'the accountant': 105, 'the technician': 138, 'the mover': 99, 'the salesperson': 144, 'the construction worker': 108, 'the mechanic': 114, 'the lawyer': 117, 'the farmer': 144, 'the engineer': 108, 'the chief': 84, 'the CEO': 123, 'the developer': 114, 'the manager': 135, 'the driver': 111, 'the laborer': 111, 'the janitor': 108, 'the analyst': 96, 'the physician': 96, 'the supervisor': 141, 'the carpenter': 102}\n"
     ]
    }
   ],
   "source": [
    "print(\"noun_error1: {}\".format(noun_error1))\n",
    "print(\"noun_dict1: {}\".format(noun_dict1))\n",
    "print(\"noun_error2: {}\".format(noun_error2))\n",
    "print(\"noun_dict2: {}\".format(noun_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input generation threshold reached, 3002 unique inputs generated\n",
      "Maximum input generation threshold reached, 3002 unique inputs generated\n",
      "pred_err_count:  3711\n",
      "pred_error_rate:  0.01767142857142857\n",
      "********************\n",
      "fairness_err_count:  1308\n",
      "fairness_error_rate:  0.006228571428571429\n",
      "********************\n",
      "Final Unique input1s: 320\n",
      "Final Unique input2s: 330\n",
      "Final Unique input pairs: 3002\n",
      "********************\n",
      "Final Unique PREDICTION errors: 3002\n",
      "Final Unique FAIRNESS violations: 1015\n",
      "Final length of retrain_dict: 245\n",
      "********************\n",
      "noun_dict1:  {'this girl': 18939, 'my mother': 19095, 'She': 19509, 'this woman': 18711, 'my sister': 18933, 'my daughter': 19254, 'my aunt': 19389, 'my wife': 19236, 'my girlfriend': 19221, 'my mom': 8853, 'the cashier': 294, 'the librarian': 222, 'the attendant': 189, 'the the librarian': 252, 'the clerk': 402, 'the secretary': 204, 'the cleaner': 225, 'the tailor': 246, 'the baker': 219, 'the editor': 243, 'the designer': 246, 'the teacher': 294, 'the writer': 237, 'the the paralegal': 240, 'the housekeeper': 222, 'the assistant': 186, 'the the nurse': 249, 'the auditor': 267, 'the hairdresser': 234, 'the nurse': 198, 'the receptionist': 192, 'the counselor': 168, 'her': 264}\n",
      "noun_dict2:  {'this boy': 19044, 'my father': 19332, 'He': 19599, 'this man': 18666, 'my brother': 18924, 'my son': 19203, 'my uncle': 19338, 'my husband': 18924, 'my boyfriend': 18903, 'my dad': 9171, 'the cook': 228, 'the guard': 219, 'the sheriff': 228, 'the accountant': 177, 'the technician': 234, 'the mover': 234, 'the salesperson': 279, 'the construction worker': 258, 'the mechanic': 231, 'the lawyer': 180, 'the farmer': 219, 'the engineer': 207, 'the chief': 162, 'the CEO': 204, 'the developer': 240, 'the manager': 333, 'the driver': 225, 'the laborer': 252, 'the janitor': 234, 'the analyst': 246, 'the physician': 186, 'the supervisor': 270, 'the carpenter': 219}\n",
      "noun_error1:  {'this girl': 6228, 'my mother': 12599, 'She': 7707, 'my sister': 5507, 'my aunt': 10627, 'this woman': 5811, 'my daughter': 6387, 'my girlfriend': 6193, 'my wife': 2209, 'my mom': 2099, 'the cashier': 92, 'the librarian': 59, 'the attendant': 51, 'the the librarian': 88, 'the clerk': 226, 'the cleaner': 81, 'the tailor': 75, 'the baker': 85, 'the editor': 78, 'the teacher': 82, 'the writer': 77, 'the the paralegal': 109, 'the hairdresser': 90, 'the secretary': 62, 'the nurse': 60, 'the housekeeper': 53, 'the the nurse': 97, 'the designer': 87, 'the counselor': 57, 'the receptionist': 67, 'the auditor': 102, 'the assistant': 46, 'her': 117}\n",
      "noun_error2:  {'this boy': 6708, 'my father': 12858, 'He': 7825, 'my brother': 4452, 'my uncle': 12851, 'this man': 4365, 'my son': 7903, 'my boyfriend': 4507, 'my dad': 1991, 'my husband': 1968, 'the cook': 98, 'the guard': 66, 'the sheriff': 81, 'the accountant': 49, 'the technician': 72, 'the salesperson': 107, 'the mover': 80, 'the construction worker': 74, 'the mechanic': 85, 'the lawyer': 43, 'the farmer': 48, 'the engineer': 65, 'the chief': 49, 'the CEO': 48, 'the developer': 69, 'the manager': 162, 'the laborer': 88, 'the janitor': 79, 'the analyst': 92, 'the supervisor': 116, 'the driver': 80, 'the carpenter': 56, 'the physician': 56}\n"
     ]
    }
   ],
   "source": [
    "generate_neutral_tests_probabilistically(noun_choice, ITERS, max_input_gen_threshold, mode, noun_error1, noun_error2, noun_dict1, noun_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
